{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "039090e4-5406-4b1c-b3c9-5b663cc2872e",
   "metadata": {},
   "source": [
    "# Model Training and Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d29f30c5-6ec1-4339-a2af-a73bd36f91c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#model imports\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier,XGBRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from statistics import mean\n",
    "\n",
    "#useful functions\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34803428-b671-4537-b669-66a6fad30c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define an instance of the models without hyperparameters\n",
    "\n",
    "DT = DecisionTreeClassifier()\n",
    "RF = RandomForestClassifier(verbose = 1, n_jobs=-1)\n",
    "XGB = XGBClassifier()\n",
    "KNN = KNeighborsClassifier(n_neighbors = 50) # n_neighbors through trial and error\n",
    "MLP = MLPClassifier(activation='relu',\n",
    "    solver='sgd',learning_rate='invscaling',\n",
    "    learning_rate_init=0.001,\n",
    "    batch_size=100,verbose = True) # Based on ML Practical Parameters\n",
    "\n",
    "\n",
    "#Creating an instance of our encoder for the target\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27ac0de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\AppData\\Local\\Temp\\ipykernel_40632\\581552609.py:2: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data= pd.read_csv(\"../../data/train_data_enriched.csv\", index_col=\"Claim Identifier\")\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data= pd.read_csv(\"../../data/train_data_enriched.csv\", index_col=\"Claim Identifier\")\n",
    "data_test = pd.read_csv(\"../../data/test_data_enriched.csv\",index_col=\"Claim Identifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae25eec-6e04-4da5-a564-5fc146a80596",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41080786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(X_train, X_val, target_column, algorithm,validation = True):\n",
    "\n",
    "    # Separating the missing values from the non missing values\n",
    "    available_data = X_train[X_train[target_column].notna()]\n",
    "    missing_X_train = X_train[X_train[target_column].isna()]\n",
    "    missing_X_val = X_val[X_val[target_column].isna()]\n",
    "\n",
    "    # Making sure if there is enough data for inputing, returning it if not\n",
    "    if len(missing_X_train) == 0:\n",
    "        print(f\"no missing values to input on {target_column}\")\n",
    "        return X_train, X_val\n",
    "\n",
    "    # Separating the target column from the rest\n",
    "    X_available = available_data.drop(columns=[target_column])\n",
    "    y_available = available_data[target_column]\n",
    "\n",
    "    # Making sure our columns are consistent\n",
    "    X_available = X_available.select_dtypes(include=[\"number\"])\n",
    "    missing_X_train = missing_X_train.select_dtypes(include=[\"number\"])\n",
    "    missing_X_val = missing_X_val.select_dtypes(include=[\"number\"])\n",
    "\n",
    "    common_columns = X_available.columns.intersection(missing_X_train.columns).intersection(missing_X_val.columns)\n",
    "    X_available = X_available[common_columns]\n",
    "    missing_X_train = missing_X_train[common_columns]\n",
    "    missing_X_val = missing_X_val[common_columns]\n",
    "\n",
    "    # Making sure there is any column after keeping the common columns\n",
    "    if X_available.shape[1] == 0:\n",
    "        print(f\"Without any column to input in {target_column}\")\n",
    "        return X_train, X_val\n",
    "\n",
    "    # Training the model with the available data\n",
    "    model = algorithm\n",
    "    model.fit(X_available, y_available)\n",
    "\n",
    "    # Prediting the missing values\n",
    "    predicted_train = model.predict(missing_X_train)\n",
    "    if validation:\n",
    "        predicted_val = model.predict(missing_X_val)\n",
    "\n",
    "    # Filling the training and validation with predictions. The latter is only filled if the argument is true\n",
    "    X_train = X_train.copy()\n",
    "    X_train.loc[X_train[target_column].isna(), target_column] = predicted_train\n",
    "\n",
    "    if validation:\n",
    "        X_val = X_val.copy()\n",
    "        X_val.loc[X_val[target_column].isna(), target_column] = predicted_val\n",
    "\n",
    "    return X_train, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5e1910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This helper fuction was used for finding problems with later functions\n",
    "def check_missing_values(data):\n",
    "    print(data.isnull().sum()[data.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f20a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers(data, column):\n",
    "    # Handles outliers in a numerical column by replacing values outside the interquartile range (IQR) with missing values\n",
    "\n",
    "    # Makes sure we only treat outliers in columns that have any data\n",
    "    if data[column].notnull().sum() > 0: \n",
    "\n",
    "        # Calculating inter quantile range limits\n",
    "        Q1 = data[column].quantile(0.25)\n",
    "        Q3 = data[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Changing the ouliers to missing values\n",
    "        data[column] = np.where(data[column] < lower_bound, np.nan, data[column])\n",
    "        data[column] = np.where(data[column] > upper_bound, np.nan, data[column])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9e310a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_numerical(column, X_train, X_val, scaler):\n",
    "    # Scales the given numerical value based on the training data with the given scaler\n",
    "    \n",
    "    # Make sure the column is numerical\n",
    "    if not pd.api.types.is_numeric_dtype(X_train[column]):\n",
    "        print(f\"Columm '{column}' is not numerical and will be ignored\")\n",
    "        return\n",
    "\n",
    "    # Scaling the data\n",
    "    try:\n",
    "        X_train[column] = scaler.fit_transform(X_train[[column]])\n",
    "        X_val[column] = scaler.transform(X_val[[column]])\n",
    "    except ValueError as e:\n",
    "        print(f\"Mistake scaling the column '{column}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92397747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  claim_carrier_categories(X_train, X_val):\n",
    "    if 'Carrier Claim Category' not in X_train.columns:\n",
    "        # Function to categorize each carrier based on its claim count for dimensionality reduction\n",
    "        \n",
    "        count = X_train['Carrier Name'].value_counts() # Count individual Carriers' counts only on train data\n",
    "        def categorize_claims(count): # Map carrier size based on fixed thresholds decided by us\n",
    "            if count >= 40000:\n",
    "                return 2\n",
    "            elif 4000 <= count < 40000:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "    \n",
    "        # Apply the categorization to create a mapping dictionary\n",
    "        carrier_category_map = count.apply(categorize_claims)\n",
    "    \n",
    "        # Map the `Carrier Name` to the new `Carrier Claim Category`\n",
    "        X_train['Carrier Claim Category'] = X_train['Carrier Name'].map(carrier_category_map)\n",
    "        X_val['Carrier Claim Category'] = X_val['Carrier Name'].map(carrier_category_map)\n",
    "\n",
    "        # If there is a missing value on x_val, it means that that carrier name didn't exist on x_train and is unlikely to have many claim counts\n",
    "        # X_train cannot have NaN in this new feature as Carrier Name has no Missing values\n",
    "        X_val['Carrier Claim Category'].fillna(0, inplace = True)\n",
    "    \n",
    "        return X_train.drop([\"Carrier Name\"], axis = 1, inplace = True) , X_val.drop([\"Carrier Name\"], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db2cfcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_prop_encode(X_train, X_val, feature):\n",
    "    # Categorical encoder function for individual feature\n",
    "    proportion = X_train[feature].value_counts(normalize = True)  # Get the porportion of each category\n",
    "    X_train[feature] = X_train[feature].map(proportion)  # Map the porportions in the column\n",
    "    X_val[feature] = X_val[feature].map(proportion) # Do the same for the validation subset\n",
    "    X_val[feature] = X_val[feature].fillna(0)  # Handle categories in X_val not seen in X_train with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4efb3e3-69d9-4968-a39c-9f2cd20c4ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rfe(algorithm,X_train,y_train,X_val,y_val):\n",
    "    # Function to run RFE on specified algorithm with train/val split for it to be run within every fold to avoid data leakage\n",
    "    \n",
    "    #Generating the variables where we will store our results\n",
    "    nof_list = np.arange(1, len(X_train.columns) + 1)            \n",
    "    high_score = 0\n",
    "    opt_n_features = 0\n",
    "    train_score_list = []\n",
    "    val_score_list = []\n",
    "\n",
    "    #Variable where we will store the optimum amount of features\n",
    "    best_rfe = None\n",
    "\n",
    "    model = algorithm\n",
    "\n",
    "    for n in nof_list:\n",
    "        rfe = RFE(estimator=model, n_features_to_select=n)\n",
    "    \n",
    "    # Fitting the model to rfe\n",
    "        X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "        X_val_rfe = rfe.transform(X_val)\n",
    "    \n",
    "    # Training and predicting\n",
    "        model.fit(X_train_rfe, y_train)\n",
    "        pred_train = model.predict(X_train_rfe)\n",
    "        pred_val = model.predict(X_val_rfe)\n",
    "    \n",
    "    # Evaluating using the macro f1_score\n",
    "        train_score = f1_score(y_train, pred_train, average=\"macro\")\n",
    "        val_score = f1_score(y_val, pred_val, average=\"macro\")\n",
    "        train_score_list.append(train_score)\n",
    "        val_score_list.append(val_score)\n",
    "    \n",
    "    # Checking if this is the best combination of features so far\n",
    "        if val_score >= high_score:\n",
    "            high_score = val_score\n",
    "            opt_n_features = n\n",
    "            best_rfe = rfe  # Storing the rfe with the best number of features\n",
    "\n",
    "# Checking what amount of features and which features where the best for the model\n",
    "    selected_features = X_train.columns[best_rfe.support_].tolist()\n",
    "\n",
    "    print(\"Optimal number of features: %d\" % opt_n_features)\n",
    "    print(\"Score with %d features: %f\" % (opt_n_features, high_score))\n",
    "    print(\"Selected Features:\\n\", selected_features)\n",
    "\n",
    "    return selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc04b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_scores(X, y, num_features, cat_features, num_imputing_algorithm=XGBRegressor(), cat_imputing_algorithm=XGBClassifier(), scaling_outlier= True, scaler=MinMaxScaler(), rfe = False):\n",
    "    \"\"\"\n",
    "    Performs stratified cross-validation on a dataset to evaluate multiple classification models, while handling \n",
    "    preprocessing steps like missing value imputation, scaling, outlier removal, feature engineering, and optional \n",
    "    feature selection using Recursive Feature Elimination (RFE).\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    X : Entire data without target variable\n",
    "\n",
    "    y : Entire data with only target variable\n",
    "\n",
    "    num_features : List of numerical features within X\n",
    "\n",
    "    cat_features : List of categorical features within X (non-binary)\n",
    "\n",
    "    num_imputing_algorithm : Algorithm to be used for imputing numerical features' missing values. Defaults to XGBRegressor.\n",
    "\n",
    "    cat_imputing_algorithm : Algorithm to be used for imputing categorical features' missing values. Defaults to XGBClassifier.\n",
    "\n",
    "    scaling_outlier : Boolean indicating if scaling and outlier handling should be applied. Defaults to True.\n",
    "\n",
    "    scaler : Scaling algorithm to be used for scaling. Defaults to True.\n",
    "\n",
    "    rfe : Boolean indicating if RFE should be used within each fold to determine current most valuable features to use. Defaults to False.\n",
    "\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # Generating the lists to store our results\n",
    "    precision_scores_train = [[],[],[],[],[]]\n",
    "    precision_scores_val = [[],[],[],[],[]]  \n",
    "    recall_scores_train = [[],[],[],[],[]]\n",
    "    recall_scores_val = [[],[],[],[],[]]\n",
    "    f1_scores_train =  [[],[],[],[],[]]\n",
    "    f1_scores_val =  [[],[],[],[],[]]\n",
    "\n",
    "    precision_scores_train_mean = []\n",
    "    precision_scores_val_mean = [] \n",
    "    recall_scores_train_mean = []\n",
    "    recall_scores_val_mean = []\n",
    "    f1_scores_train_mean =  []\n",
    "    f1_scores_val_mean =  []\n",
    "    \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        # Dividing our data in validation and train\n",
    "        X_train, X_val = X.iloc[train_index].copy(), X.iloc[test_index].copy()\n",
    "        y_train, y_val = y.iloc[train_index].copy(), y.iloc[test_index].copy()\n",
    "\n",
    "        # Filling missing values\n",
    "        for column in num_features:\n",
    "            X_train, X_val = impute_missing_values(X_train,X_val, column, num_imputing_algorithm)\n",
    "        \n",
    "        # Removing inconsistencies on the train\n",
    "        inconsistent = X_train[(X_train['Age at Injury'] > 80) | (X_train[\"Age at Injury\"] < 16)].index\n",
    "        X_train = X_train.loc[~X_train.index.isin(inconsistent)]\n",
    "        y_train = y_train.loc[~y_train.index.isin(inconsistent)]\n",
    "\n",
    "        # Performing scaling and outlier treatment dependent on the boolean\n",
    "        if scaling_outlier:\n",
    "            for column in num_features:\n",
    "                handle_outliers(X_train, column)\n",
    "                X_train, X_val = impute_missing_values(X_train,X_val, column, num_imputing_algorithm,validation= False)\n",
    "\n",
    "            for column in num_features:\n",
    "                scale_numerical(column, X_train, X_val, scaler)\n",
    "                \n",
    "        # Creating an ordinal variable\n",
    "        claim_carrier_categories(X_train, X_val)\n",
    "\n",
    "        #Filling missing values in the ordinal variable that might appear on X_val\n",
    "        X_val, X_train = impute_missing_values(X_val,X_train, \"Carrier Claim Category\", cat_imputing_algorithm, validation=False)\n",
    "\n",
    "        # Scaling special Carrier Claim Category feature\n",
    "        if scaling_outlier:\n",
    "            scale_numerical(\"Carrier Claim Category\", X_train, X_val, scaler)\n",
    "\n",
    "        # Categorical Prop Encoding\n",
    "        for cat_feature in cat_features:\n",
    "            categorical_prop_encode(X_train, X_val, cat_feature)\n",
    "            if scaling_outlier:\n",
    "                scale_numerical(\"Carrier Claim Category\", X_train, X_val, scaler)\n",
    "\n",
    "        # Selecting features with Rfe\n",
    "        if rfe:\n",
    "            Selected_features = Rfe(XGBClassifier(), X_train, y_train, X_val, y_val)\n",
    "            X_train = X_train[Selected_features]\n",
    "            X_val = X_val[Selected_features]\n",
    "\n",
    "        \n",
    "        # Training the classification models\n",
    "        DT.fit(X_train, y_train)\n",
    "        print(\"Done DT\")\n",
    "        RF.fit(X_train, y_train)\n",
    "        print(\"Done RF\")\n",
    "        XGB.fit(X_train, y_train)\n",
    "        print(\"Done XGB\")\n",
    "        KNN.fit(X_train, y_train)\n",
    "        print(\"Done KNN\")\n",
    "        MLP.fit(X_train, y_train)\n",
    "        print(\"Done MLP\")\n",
    "\n",
    "        # Making the predictions for the training and validation data\n",
    "        pred_train_DT = DT.predict(X_train)\n",
    "        pred_train_RF = RF.predict(X_train)\n",
    "        pred_train_XGB = XGB.predict(X_train)\n",
    "        pred_train_KNN = KNN.predict(X_train)\n",
    "        pred_train_MLP = MLP.predict(X_train)\n",
    "        print(\"Done training predictions\")\n",
    "        \n",
    "        pred_val_DT = DT.predict(X_val)\n",
    "        pred_val_RF = RF.predict(X_val)\n",
    "        pred_val_XGB = XGB.predict(X_val)\n",
    "        pred_val_KNN = KNN.predict(X_val)\n",
    "        pred_val_MLP = MLP.predict(X_val)\n",
    "        print(\"Done validation predictions\")\n",
    "\n",
    "        # Calculating and storing the scores\n",
    "        i = 0\n",
    "        for predictions in [pred_train_DT,pred_train_RF,pred_train_XGB,pred_train_KNN,pred_train_MLP]:\n",
    "            precision_scores_train[i].append(precision_score(y_train, predictions, average='macro'))\n",
    "            recall_scores_train[i].append(recall_score(y_train, predictions, average='macro'))\n",
    "            f1_scores_train[i].append(f1_score(y_train, predictions, average='macro'))\n",
    "            i+=1\n",
    "        j=0\n",
    "        for predictions in [pred_val_DT,pred_val_RF,pred_val_XGB,pred_val_KNN,pred_val_MLP]:\n",
    "            precision_scores_val[j].append(precision_score(y_val, predictions, average='macro'))\n",
    "            recall_scores_val[j].append(recall_score(y_val, predictions, average='macro'))\n",
    "            f1_scores_val[j].append(f1_score(y_val, predictions, average='macro'))\n",
    "            j+=1\n",
    "\n",
    "        # Check the confusion matrixes of our predictions\n",
    "        print(confusion_matrix(y_val, pred_val_DT))\n",
    "        print(confusion_matrix(y_val, pred_val_RF))\n",
    "        print(confusion_matrix(y_val, pred_val_XGB))\n",
    "        print(confusion_matrix(y_val, pred_val_KNN))\n",
    "        print(confusion_matrix(y_val, pred_val_MLP))\n",
    "\n",
    "    # Aggregating the average results across the folds\n",
    "    for l in range(0,5): \n",
    "        precision_scores_train_mean.append(mean(precision_scores_train[l]))\n",
    "        precision_scores_val_mean.append(mean(precision_scores_val[l]))\n",
    "        recall_scores_train_mean.append(mean(recall_scores_train[l]))\n",
    "        recall_scores_val_mean.append(mean(recall_scores_val[l]))\n",
    "        f1_scores_train_mean.append(mean(f1_scores_train[l]))\n",
    "        f1_scores_val_mean.append(mean(f1_scores_val[l]))\n",
    "\n",
    "    # Storing the results in a dataframe\n",
    "    model_results = pd.DataFrame(data={\n",
    "        'Train_precision': precision_scores_train_mean,\n",
    "        'Test_precision': precision_scores_val_mean,\n",
    "        'Train_recall': recall_scores_train_mean,\n",
    "        'Test_recall': recall_scores_val_mean,\n",
    "        'Train_f1_score': f1_scores_train_mean,\n",
    "        'Test_f1_score': f1_scores_val_mean,\n",
    "    }, index=[\"Decision Tree\",\"Random Forest\",\"XGBoost\", \"KNearestNeighbors\",\"Multi Layer Perceptron\"])\n",
    "\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb306304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(model, X_train, y_train, num_features, cat_features, X_test, \n",
    "                    num_imputing_algorithm=XGBRegressor(), \n",
    "                    cat_imputing_algorithm=XGBClassifier(), scaling_outlier = False , \n",
    "                    scaler=MinMaxScaler(), missing = True,\n",
    "                    secondary_model = None, y_train_secondary=None, secondary_missing = True):\n",
    "\n",
    "    \"\"\"\n",
    "    Performs stratified cross-validation on a dataset to evaluate multiple classification models, while handling \n",
    "    preprocessing steps like missing value imputation, scaling, outlier removal, feature engineering, and optional \n",
    "    feature selection using Recursive Feature Elimination (RFE).\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    model: algorithm to use to predict primary target variable.\n",
    "    \n",
    "    X_train : Entire data without target variable.\n",
    "\n",
    "    y_train : Entire data with only target variable.\n",
    "\n",
    "    num_features : List of numerical features within X\n",
    "\n",
    "    cat_features : List of categorical features within X (non-binary)\n",
    "\n",
    "    num_imputing_algorithm : Algorithm to be used for imputing numerical features' missing values. Defaults to XGBRegressor.\n",
    "\n",
    "    cat_imputing_algorithm : Algorithm to be used for imputing categorical features' missing values. Defaults to XGBClassifier.\n",
    "\n",
    "    scaling_outlier : Boolean indicating if scaling and outlier handling should be applied. Defaults to True.\n",
    "\n",
    "    scaler : Scaling algorithm to be used for scaling. Defaults to True.\n",
    "\n",
    "    rfe : Boolean indicating if RFE should be used within each fold to determine current most valuable features to use. Defaults to False.\n",
    "\n",
    "    secondary_model: algorithm to use to predict secondary target variable. Defaults to None if secondary should not be predicted.\n",
    "\n",
    "    y_train_secondary: Entire data with only secondary target variable. Defaults to None if secondary should not be predicted.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Impute missing values\n",
    "    if missing:\n",
    "        for column in num_features:\n",
    "            X_train, X_test = impute_missing_values(X_train,X_test,column, num_imputing_algorithm)\n",
    "\n",
    "    # Remove inconsistencies\n",
    "    inconsistent = X_train[(X_train['Age at Injury'] > 80) | (X_train[\"Age at Injury\"] < 16)].index\n",
    "    X_train.drop(inconsistent, inplace=True)\n",
    "    y_train.drop(inconsistent, inplace=True)\n",
    "    if secondary_model is not None and y_train_secondary is not None:\n",
    "        y_train_secondary.drop(inconsistent, inplace=True)\n",
    "\n",
    "    # Scale and remove outliers if specified\n",
    "    if scaling_outlier:\n",
    "        for column in num_features:\n",
    "            handle_outliers(X_train, column) # Handle outliers only for training partition\n",
    "            if missing:\n",
    "                X_train, X_test = impute_missing_values(X_train,X_test, column, num_imputing_algorithm,validation = False)\n",
    "\n",
    "        for column in num_features:\n",
    "            scale_numerical(column, X_train, X_test, scaler)\n",
    "        \n",
    "    # Creating an ordinal variable\n",
    "    claim_carrier_categories(X_train, X_test)\n",
    "\n",
    "    # Scaling special Carrier Claim Category feature\n",
    "    if scaling_outlier:\n",
    "        scale_numerical(\"Carrier Claim Category\", X_train, X_test, scaler)\n",
    "\n",
    "    # Categorical Prop Encoding\n",
    "    for cat_feature in cat_features:\n",
    "        categorical_prop_encode(X_train, X_test, cat_feature)\n",
    "        if scaling_outlier:\n",
    "            scale_numerical(\"Carrier Claim Category\", X_train, X_test, scaler)\n",
    "\n",
    "    # Predict secondary target variable if secondary model and variable is given\n",
    "    if secondary_model is not None and y_train_secondary is not None:\n",
    "        if secondary_missing: # Imputation of missing values for secondary model\n",
    "            X_train_secondary = X_train.copy()\n",
    "            X_test_secondary = X_test.copy()\n",
    "            for column in num_features:\n",
    "                X_train_secondary, X_test_secondary = impute_missing_values(X_train_secondary,X_test_secondary, column, num_imputing_algorithm)\n",
    "\n",
    "        secondary_model.fit(X_train_secondary, y_train_secondary)\n",
    "        pred_secondary_test = secondary_model.predict(X_test_secondary)\n",
    "        X_test[\"Agreement Reached\"] = pred_secondary_test\n",
    "        X_train[\"Agreement Reached\"] = y_train_secondary\n",
    "        \n",
    "    # Fitting the model, making the predictions and reverting the claim injury types back to their string form\n",
    "    model.fit(X_train, y_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    pred_test = le.inverse_transform(pred_test)\n",
    "\n",
    "    # Saving the final submission dataframe with indexes of X_test\n",
    "    submission_df = pd.DataFrame({\n",
    "        \"Claim Injury Type\": pred_test\n",
    "    }, index=X_test.index)\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fb4280-214e-47bf-8c0d-dee36a318c80",
   "metadata": {},
   "source": [
    "# Dataset preparation\n",
    "Label encoding and dropping variables based on multivariate exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fa916c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label enconding our target variable \n",
    "data[\"Claim Injury Type\"] = le.fit_transform(data[\"Claim Injury Type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f92e4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Dropping redundant variables that carry almost the same information (are extremely correlated (|0.8|))\n",
    "We believe it was better to keep Age at Injury than birth year since it should be more related to the injury claim type (it will be tested later)\n",
    "The same logic was applied to dropping the other two dates and two DSA variables since we believe Accident date to be more important'''\n",
    "\n",
    "data = data.loc[:, ~data.columns.isin(['Birth Year', 'Assembly Date', 'C-2 Date', 'Assembly Date DSA', 'First Hearing Date DSA'])]\n",
    "data_test = data_test.loc[:, ~data_test.columns.isin(['Birth Year', 'Assembly Date', 'C-2 Date', 'Assembly Date DSA', 'First Hearing Date DSA'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61c9780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Since the codes always seem to provide the same or more information than the descriptions (have more categories),\n",
    "and the codes are consistent (always only having 1 description for code, while descriptions may have multiple codes)\n",
    "And Crámer's V says they have a very high association\n",
    "we will drop the description columns.'''\n",
    "data.drop(['Industry Code Description','WCIO Cause of Injury Description','WCIO Nature of Injury Description','WCIO Part Of Body Description'], axis = 1,inplace = True)\n",
    "data_test.drop(['Industry Code Description','WCIO Cause of Injury Description','WCIO Nature of Injury Description','WCIO Part Of Body Description'], axis = 1,inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0b67b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Dropping redundant variables that carry almost the same information (have an association above or equal to 0.8)\n",
    "We chose to keep County of Injury above Zip Code and District Name (these 3 have a high association) because it is the easist to interpret and the one we looked more in detail in the exploratory analysis\n",
    "We kept the new variable we made called body section because it keeps most of the same information of the body part code but with a much lower cardinality\n",
    "Lastly we only remove the variable Carrier Name in the function where we create the new variable with lower cardinality because it is need to create that new variable'''\n",
    "data.drop(['Zip Code',\"WCIO Part Of Body Code\",\"District Name\"], axis=1, inplace = True)\n",
    "data_test.drop([\"Zip Code\",\"WCIO Part Of Body Code\",\"District Name\"], axis=1 , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de5fd5-35de-402a-9ec9-e8be53f4a545",
   "metadata": {},
   "source": [
    "## Definition of used numerical and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0cad736",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = ['Age at Injury', 'Average Weekly Wage', 'IME-4 Count', 'Number of Dependents',\n",
    "                \"Accident Year\",\"Accident Month\",\"Accident Day\",\"Accident DayOfWeek\",\n",
    "                \"C-2 Date DSA\",\"C-3 Date DSA\",\"Accident Date\",\"C-3 Date\",\"First Hearing Date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5a3fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\n",
    "    \"Alternative Dispute Resolution\",\n",
    "    \"Carrier Type\",\n",
    "    \"County of Injury\",\n",
    "    \"Gender\",\n",
    "    \"Industry Code\",\n",
    "    \"Medical Fee Region\",\n",
    "    \"WCIO Cause of Injury Code\",\n",
    "    \"WCIO Nature of Injury Code\",\n",
    "    \"Age at Injury Category\",\n",
    "    \"Body Section\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9f7bf2-8050-4a26-91f6-f76141bf5bff",
   "metadata": {},
   "source": [
    "### Quality Checks\n",
    "The only not included variables are the target variables and all the binary variables (non-numeric and non-categorical) as seen below. As binary variables are not scaled nor encoded they don't appear in the definitions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b06d9ea3-cc03-4fe9-9066-43ca1b55243c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(num_features) + len(cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5560997e-b2f3-4465-a89c-e8b32a20aaa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3648707-ed6c-4948-96a9-88783e9cc70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns not in num_features or cat_features: {'First Hearing Date_missing', 'C-3 Date_missing', 'Carrier Name', 'COVID-19 Indicator', 'Assembly Date_missing', 'Agreement Reached', 'Attorney/Representative', 'Claim Injury Type', 'Accident Date_missing', 'C-2 Date_missing'}\n",
      "Count of difference: 10\n"
     ]
    }
   ],
   "source": [
    "# Combine num_features and cat_features into a set\n",
    "defined_features = set(num_features) | set(cat_features)\n",
    "\n",
    "# Get all column names in the DataFrame\n",
    "all_columns = set(data.columns)\n",
    "\n",
    "# Find columns that are not in the defined features\n",
    "undefined_columns = all_columns - defined_features\n",
    "\n",
    "print(\"Columns not in num_features or cat_features:\", undefined_columns)\n",
    "print(\"Count of difference:\", len(undefined_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3299b0-e243-418c-a263-54b209a9033f",
   "metadata": {},
   "source": [
    "## Isolation of target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f0380ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"Claim Injury Type\",\"Agreement Reached\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "130bc8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"Claim Injury Type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca4cdcba-054b-43fc-8d41-919cf8f0b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = data[\"Agreement Reached\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292e2dad-9b48-4cb7-be2c-9f585e3e3b57",
   "metadata": {},
   "source": [
    "# Run CV Score Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7a8d980-33d2-4014-9aa8-8bac0ec786f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Age at Injury\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Accident Month\n",
      "no missing values to input on Accident DayOfWeek\n",
      "Done DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   31.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done RF\n",
      "Done XGB\n",
      "Done KNN\n",
      "Iteration 1, loss = 1.05945966\n",
      "Iteration 2, loss = 1.00383479\n",
      "Iteration 3, loss = 1.00373393\n",
      "Iteration 4, loss = 1.00367385\n",
      "Iteration 5, loss = 1.00362802\n",
      "Iteration 6, loss = 1.00358934\n",
      "Iteration 7, loss = 1.00355530\n",
      "Iteration 8, loss = 1.00352444\n",
      "Iteration 9, loss = 1.00349600\n",
      "Iteration 10, loss = 1.00346950\n",
      "Iteration 11, loss = 1.00344459\n",
      "Iteration 12, loss = 1.00342100\n",
      "Iteration 13, loss = 1.00339851\n",
      "Iteration 14, loss = 1.00337703\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Done MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    4.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validation predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  743  1093   163   300   137    34    12    13]\n",
      " [ 3967 31700  6768 12646  2775   190    22   147]\n",
      " [  653  5334  1755  3884  1497   494    40   125]\n",
      " [ 1178  7955  3215  8850  4325  3297   365   516]\n",
      " [  322  1044   954  1720  3633  1791   153    39]\n",
      " [   34   162   124   245    86   143    25    23]\n",
      " [    0     5     1    10     2     0     1     1]\n",
      " [    5    26     8    27     7     2     4    15]]\n",
      "[[  886  1338     4   148   109    10     0     0]\n",
      " [  448 54257    25  2372  1048    62     0     3]\n",
      " [   45  8971   100  2753  1690   222     0     1]\n",
      " [   26 11347    69  9501  6475  2260     0    23]\n",
      " [    7   746     8  1255  7190   449     0     1]\n",
      " [    0    56     4   375   202   205     0     0]\n",
      " [    0     1     0    13     1     5     0     0]\n",
      " [    0    26     0    59     5     2     0     2]]\n",
      "[[ 1077  1170     3    98   122    24     0     1]\n",
      " [  647 52731    32  2278  2199   282     2    44]\n",
      " [   76  8746    89  2389  1985   449    13    35]\n",
      " [   66 10387   152  8799  7330  2677   153   137]\n",
      " [   14   712    29   908  6919  1039    31     4]\n",
      " [    3    82    12   339   209   182    12     3]\n",
      " [    0     3     0    12     2     3     0     0]\n",
      " [    0    28     1    36     3     4     0    22]]\n",
      "[[  721  1432    12   193   137     0     0     0]\n",
      " [  472 53877   112  2651  1102     1     0     0]\n",
      " [   23  8793   224  3177  1564     1     0     0]\n",
      " [   21 12368   492 11162  5645    13     0     0]\n",
      " [    6   831    82  3744  4991     2     0     0]\n",
      " [    0    43    15   487   296     1     0     0]\n",
      " [    0     0     3    15     2     0     0     0]\n",
      " [    0     9     0    67    18     0     0     0]]\n",
      "[[   77  1882     0   278   258     0     0     0]\n",
      " [   44 54169     1  2716  1285     0     0     0]\n",
      " [    1  9118     0  3308  1355     0     0     0]\n",
      " [    0 12765     1 12972  3963     0     0     0]\n",
      " [    0  1426     0  3789  4441     0     0     0]\n",
      " [    0    29     0   642   171     0     0     0]\n",
      " [    0     1     0    17     2     0     0     0]\n",
      " [    0    17     0    41    36     0     0     0]]\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Age at Injury\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Accident Month\n",
      "no missing values to input on Accident Day\n",
      "no missing values to input on Accident DayOfWeek\n",
      "Done DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   31.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done RF\n",
      "Done XGB\n",
      "Done KNN\n",
      "Iteration 1, loss = 1.05831738\n",
      "Iteration 2, loss = 1.00064623\n",
      "Iteration 3, loss = 1.00043468\n",
      "Iteration 4, loss = 1.00032688\n",
      "Iteration 5, loss = 1.00025335\n",
      "Iteration 6, loss = 1.00019698\n",
      "Iteration 7, loss = 1.00015084\n",
      "Iteration 8, loss = 1.00011153\n",
      "Iteration 9, loss = 1.00007682\n",
      "Iteration 10, loss = 1.00004553\n",
      "Iteration 11, loss = 1.00001692\n",
      "Iteration 12, loss = 0.99999045\n",
      "Iteration 13, loss = 0.99996566\n",
      "Iteration 14, loss = 0.99994231\n",
      "Iteration 15, loss = 0.99992013\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Done MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    3.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validation predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  889  1010   216   270    94    12     0     4]\n",
      " [ 1802 38152  8524  7813  1764   101    11    48]\n",
      " [  272  5441  2499  3359  1731   410    10    59]\n",
      " [  591  5577  6693  8625  5584  2261    69   301]\n",
      " [  117   673  2136  2146  3521   984    34    45]\n",
      " [   20    84   339   219   132    23     0    26]\n",
      " [    1     2     5     8     2     0     0     1]\n",
      " [    3    27    24    29     5     0     0     6]]\n",
      "[[ 1077  1097    50   179    90     2     0     0]\n",
      " [  439 52477  1214  3017  1052    16     0     0]\n",
      " [   13  7375   645  3689  1960    99     0     0]\n",
      " [   14  6680  1551 14611  5989   855     0     1]\n",
      " [    3   416   402  3569  5151   115     0     0]\n",
      " [    0    32    62   684    63     2     0     0]\n",
      " [    0     1     1    17     0     0     0     0]\n",
      " [    0    22     3    69     0     0     0     0]]\n",
      "[[ 1242   925   121   106    84    13     0     4]\n",
      " [  567 47351  6128  2640  1390    89     0    50]\n",
      " [   36  7290   817  3033  2216   356     0    33]\n",
      " [   50  6154  5058  8213  7395  2704     0   127]\n",
      " [    8   422  1564  2051  4956   653     1     1]\n",
      " [    0    38   205   509    56    35     0     0]\n",
      " [    0     1     1    16     1     0     0     0]\n",
      " [    1    20     9    52     0     0     0    12]]\n",
      "[[ 1040  1137    34   197    87     0     0     0]\n",
      " [  465 53977   307  2302  1164     0     0     0]\n",
      " [   14  8389   450  3559  1369     0     0     0]\n",
      " [   24 11654  1053 13685  3285     0     0     0]\n",
      " [    9   778   234  4888  3747     0     0     0]\n",
      " [    0    59    47   594   143     0     0     0]\n",
      " [    0     1     2    13     3     0     0     0]\n",
      " [    0    12     3    75     4     0     0     0]]\n",
      "[[  329  1793     0   360    13     0     0     0]\n",
      " [  204 54802     0  3059   150     0     0     0]\n",
      " [    2  9153     1  4450   175     0     0     0]\n",
      " [   10 13260    74 15903   454     0     0     0]\n",
      " [    0  2301     1  6695   659     0     0     0]\n",
      " [    0    62     3   763    15     0     0     0]\n",
      " [    0     7     0    12     0     0     0     0]\n",
      " [    0    29     0    64     1     0     0     0]]\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Age at Injury\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Accident Month\n",
      "no missing values to input on Accident DayOfWeek\n",
      "Done DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   35.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done RF\n",
      "Done XGB\n",
      "Done KNN\n",
      "Iteration 1, loss = 1.06573210\n",
      "Iteration 2, loss = 1.00717049\n",
      "Iteration 3, loss = 1.00700699\n",
      "Iteration 4, loss = 1.00691632\n",
      "Iteration 5, loss = 1.00685183\n",
      "Iteration 6, loss = 1.00680128\n",
      "Iteration 7, loss = 1.00675908\n",
      "Iteration 8, loss = 1.00672237\n",
      "Iteration 9, loss = 1.00668958\n",
      "Iteration 10, loss = 1.00665979\n",
      "Iteration 11, loss = 1.00663237\n",
      "Iteration 12, loss = 1.00660681\n",
      "Iteration 13, loss = 1.00658287\n",
      "Iteration 14, loss = 1.00656017\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Done MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    3.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validation predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  957   903   287   201   121    21     1     5]\n",
      " [ 1738 36617 10519  7011  2152   156     3    19]\n",
      " [  298  5551  2301  2848  2390   357    11    25]\n",
      " [  635  5617  6416  6257  7432  3082    52   210]\n",
      " [  119  1024  3097  2729  2073   593    10    11]\n",
      " [   27    64   183   421   125    18     0     4]\n",
      " [    2     2     4     4     6     1     0     0]\n",
      " [    5    25    23    32     5     2     0     2]]\n",
      "[[ 1078  1071    90   114   139     4     0     0]\n",
      " [  380 50961  2816  2533  1506    18     0     1]\n",
      " [   12  7647   653  2937  2471    61     0     0]\n",
      " [   20  7018  3904 10155  7791   811     0     2]\n",
      " [    4   539  1483  4075  3516    39     0     0]\n",
      " [    0    21    11   747    63     0     0     0]\n",
      " [    0     2     0    13     4     0     0     0]\n",
      " [    0    14     5    72     3     0     0     0]]\n",
      "[[ 1153   945   190    60   138     7     0     3]\n",
      " [  436 42907 10106  2667  2034    38     0    27]\n",
      " [   35  7235  1151  2310  2899   140     0    11]\n",
      " [   74  6362  7753  4719  9545  1143     0   105]\n",
      " [    5   449  2534  3539  3050    79     0     0]\n",
      " [    0    21    29   729    63     0     0     0]\n",
      " [    0     1     1    13     4     0     0     0]\n",
      " [    0    17     7    67     0     0     0     3]]\n",
      "[[ 1059  1113    35   209    80     0     0     0]\n",
      " [  455 53672   439  2313  1336     0     0     0]\n",
      " [   13  8609   450  3508  1201     0     0     0]\n",
      " [   31 11524  1087 14049  3010     0     0     0]\n",
      " [    6   862   274  4924  3590     0     0     0]\n",
      " [    0    38    22   656   126     0     0     0]\n",
      " [    0     4     1     9     5     0     0     0]\n",
      " [    0    16     4    63    11     0     0     0]]\n",
      "[[   29  1911     0   536    20     0     0     0]\n",
      " [   51 53944     0  4032   188     0     0     0]\n",
      " [   10  8913     0  4631   227     0     0     0]\n",
      " [   83 12083     0 17164   371     0     0     0]\n",
      " [    4  1067     0  7724   861     0     0     0]\n",
      " [    0    10     0   809    23     0     0     0]\n",
      " [    0     1     0    15     3     0     0     0]\n",
      " [    0     7     0    86     1     0     0     0]]\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Age at Injury\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Accident Month\n",
      "no missing values to input on Accident Day\n",
      "no missing values to input on Accident DayOfWeek\n",
      "Done DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   29.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done RF\n",
      "Done XGB\n",
      "Done KNN\n",
      "Iteration 1, loss = 1.05629337\n",
      "Iteration 2, loss = 1.00533859\n",
      "Iteration 3, loss = 1.00478610\n",
      "Iteration 4, loss = 1.00454685\n",
      "Iteration 5, loss = 1.00440601\n",
      "Iteration 6, loss = 1.00431067\n",
      "Iteration 7, loss = 1.00424018\n",
      "Iteration 8, loss = 1.00418497\n",
      "Iteration 9, loss = 1.00413973\n",
      "Iteration 10, loss = 1.00410122\n",
      "Iteration 11, loss = 1.00406758\n",
      "Iteration 12, loss = 1.00403758\n",
      "Iteration 13, loss = 1.00401041\n",
      "Iteration 14, loss = 1.00398549\n",
      "Iteration 15, loss = 1.00396243\n",
      "Iteration 16, loss = 1.00394087\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Done MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    3.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validation predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  968   963   222   180   130    30     0     3]\n",
      " [ 1423 38188  8804  6512  3112   146     0    29]\n",
      " [  200  5179  2177  2978  2715   496     0    36]\n",
      " [  426  5192  5931  7483  7835  2669    12   154]\n",
      " [   76  1035  3113  3056  1923   439     5     9]\n",
      " [    4    99   209   402   110    14     0     4]\n",
      " [    1     3     4     5     5     0     0     1]\n",
      " [    1    27    26    27     3     5     0     5]]\n",
      "[[ 1160  1019    76    98   141     2     0     0]\n",
      " [  355 52964  1298  1813  1777     7     0     0]\n",
      " [   12  7397   522  2619  3208    23     0     0]\n",
      " [   18  7220  3068  9735  9427   234     0     0]\n",
      " [    4   570  1780  5094  2201     7     0     0]\n",
      " [    0    17    21   747    57     0     0     0]\n",
      " [    0     0     0    17     2     0     0     0]\n",
      " [    0    13     8    70     3     0     0     0]]\n",
      "[[ 1297   846   140    71   128    11     0     3]\n",
      " [  458 46728  6635  1305  3025    42     0    21]\n",
      " [   20  6962   876  2083  3671   153     0    16]\n",
      " [   34  5947  6645  5439 10569   981     0    87]\n",
      " [    6   449  2462  4329  2369    39     0     2]\n",
      " [    0    12    44   736    50     0     0     0]\n",
      " [    0     1     1    15     2     0     0     0]\n",
      " [    0    19    12    57     1     0     0     5]]\n",
      "[[ 1160   964    39   194   139     0     0     0]\n",
      " [  448 53771   249  2145  1601     0     0     0]\n",
      " [   11  8154   276  3240  2100     0     0     0]\n",
      " [   57 11561   873 12177  5034     0     0     0]\n",
      " [   11   973   372  5574  2726     0     0     0]\n",
      " [    0    37    34   671   100     0     0     0]\n",
      " [    0     0     0    16     3     0     0     0]\n",
      " [    0    10     2    71    11     0     0     0]]\n",
      "[[   80  1890     0   496    30     0     0     0]\n",
      " [   30 54377     1  3516   290     0     0     0]\n",
      " [    0  8553     0  4943   285     0     0     0]\n",
      " [    6 12123     0 17233   340     0     0     0]\n",
      " [    0  1938     0  7083   635     0     0     0]\n",
      " [    0    23     0   800    19     0     0     0]\n",
      " [    0     0     0    18     1     0     0     0]\n",
      " [    0    23     0    69     2     0     0     0]]\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Age at Injury\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Accident Month\n",
      "no missing values to input on Accident Day\n",
      "no missing values to input on Accident DayOfWeek\n",
      "Done DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   33.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done RF\n",
      "Done XGB\n",
      "Done KNN\n",
      "Iteration 1, loss = 1.05566805\n",
      "Iteration 2, loss = 1.00197468\n",
      "Iteration 3, loss = 1.00161870\n",
      "Iteration 4, loss = 1.00147727\n",
      "Iteration 5, loss = 1.00139541\n",
      "Iteration 6, loss = 1.00133789\n",
      "Iteration 7, loss = 1.00129329\n",
      "Iteration 8, loss = 1.00125606\n",
      "Iteration 9, loss = 1.00122366\n",
      "Iteration 10, loss = 1.00119429\n",
      "Iteration 11, loss = 1.00116741\n",
      "Iteration 12, loss = 1.00114242\n",
      "Iteration 13, loss = 1.00111887\n",
      "Iteration 14, loss = 1.00109661\n",
      "Iteration 15, loss = 1.00107541\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Done MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    5.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validation predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  891   642   721   187    50     0     0     4]\n",
      " [  666 12123 41119  3709   585     4     0     8]\n",
      " [   87  1979  9702  1360   643     4     0     6]\n",
      " [  213  3856 19171  3913  2524    18     0     7]\n",
      " [   67  1337  4129  2989  1123    10     0     1]\n",
      " [   17   109   313   325    74     4     0     0]\n",
      " [    1     0     7    10     2     0     0     0]\n",
      " [    2    13    29    42     6     0     0     2]]\n",
      "[[ 1093   567   759    67     9     0     0     0]\n",
      " [  177  7758 49798   428    52     0     0     1]\n",
      " [    3   446 13067   238    27     0     0     0]\n",
      " [    8  1177 27217  1274    26     0     0     0]\n",
      " [    1   395  5206  3585   469     0     0     0]\n",
      " [    0    18   222   571    31     0     0     0]\n",
      " [    0     1     3    15     1     0     0     0]\n",
      " [    0    11    41    42     0     0     0     0]]\n",
      "[[ 1087   429   932    37    10     0     0     0]\n",
      " [  218 11613 46052   264    66     0     0     1]\n",
      " [    2   836 12721   171    51     0     0     0]\n",
      " [    8  1416 27235   945    98     0     0     0]\n",
      " [    3   375  5435  3257   586     0     0     0]\n",
      " [    0    12   249   541    40     0     0     0]\n",
      " [    0     0     6    13     1     0     0     0]\n",
      " [    1    12    38    43     0     0     0     0]]\n",
      "[[  990  1101    80   291    33     0     0     0]\n",
      " [  219 53057   831  3375   732     0     0     0]\n",
      " [    5  8260   829  4087   600     0     0     0]\n",
      " [   16 11516  2124 15393   653     0     0     0]\n",
      " [    5  1134   645  5992  1880     0     0     0]\n",
      " [    0    33    61   708    40     0     0     0]\n",
      " [    0     1     1    14     4     0     0     0]\n",
      " [    0     5     5    79     5     0     0     0]]\n",
      "[[   73  1636     0   779     7     0     0     0]\n",
      " [  151 52845     0  5186    32     0     0     0]\n",
      " [   10  8372     0  5392     7     0     0     0]\n",
      " [  134 11595     0 17895    78     0     0     0]\n",
      " [    8  1348     0  8276    24     0     0     0]\n",
      " [    1    18     0   822     1     0     0     0]\n",
      " [    0     3     0    17     0     0     0     0]\n",
      " [    0    12     0    81     1     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "results = cv_scores(X, y, num_features, cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4ffce66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train_precision</th>\n",
       "      <th>Test_precision</th>\n",
       "      <th>Train_recall</th>\n",
       "      <th>Test_recall</th>\n",
       "      <th>Train_f1_score</th>\n",
       "      <th>Test_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.212526</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.221899</td>\n",
       "      <td>0.999970</td>\n",
       "      <td>0.198285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.317133</td>\n",
       "      <td>0.999953</td>\n",
       "      <td>0.266778</td>\n",
       "      <td>0.999964</td>\n",
       "      <td>0.251668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.796238</td>\n",
       "      <td>0.292125</td>\n",
       "      <td>0.580005</td>\n",
       "      <td>0.261095</td>\n",
       "      <td>0.622694</td>\n",
       "      <td>0.240626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNearestNeighbors</th>\n",
       "      <td>0.382109</td>\n",
       "      <td>0.311089</td>\n",
       "      <td>0.302306</td>\n",
       "      <td>0.268896</td>\n",
       "      <td>0.314234</td>\n",
       "      <td>0.272701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi Layer Perceptron</th>\n",
       "      <td>0.288944</td>\n",
       "      <td>0.253547</td>\n",
       "      <td>0.207521</td>\n",
       "      <td>0.207361</td>\n",
       "      <td>0.196247</td>\n",
       "      <td>0.194124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Train_precision  Test_precision  Train_recall  \\\n",
       "Decision Tree                  0.999950        0.212526      0.999990   \n",
       "Random Forest                  0.999975        0.317133      0.999953   \n",
       "XGBoost                        0.796238        0.292125      0.580005   \n",
       "KNearestNeighbors              0.382109        0.311089      0.302306   \n",
       "Multi Layer Perceptron         0.288944        0.253547      0.207521   \n",
       "\n",
       "                        Test_recall  Train_f1_score  Test_f1_score  \n",
       "Decision Tree              0.221899        0.999970       0.198285  \n",
       "Random Forest              0.266778        0.999964       0.251668  \n",
       "XGBoost                    0.261095        0.622694       0.240626  \n",
       "KNearestNeighbors          0.268896        0.314234       0.272701  \n",
       "Multi Layer Perceptron     0.207361        0.196247       0.194124  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the results\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfbe2252-c3ff-4d5d-b2f9-29ea2c79d61c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Age at Injury\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Accident Month\n",
      "no missing values to input on Accident Day\n",
      "no missing values to input on Accident DayOfWeek\n",
      "Done DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   26.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done RF\n",
      "Done XGB\n",
      "Done KNN\n",
      "Iteration 1, loss = 0.15569410\n",
      "Iteration 2, loss = 0.14132858\n",
      "Iteration 3, loss = 0.14131259\n",
      "Iteration 4, loss = 0.14130066\n",
      "Iteration 5, loss = 0.14129066\n",
      "Iteration 6, loss = 0.14128188\n",
      "Iteration 7, loss = 0.14127397\n",
      "Iteration 8, loss = 0.14126671\n",
      "Iteration 9, loss = 0.14125996\n",
      "Iteration 10, loss = 0.14125363\n",
      "Iteration 11, loss = 0.14124765\n",
      "Iteration 12, loss = 0.14124197\n",
      "Iteration 13, loss = 0.14123655\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Done MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validation predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[54552 54895]\n",
      " [ 2066  3292]]\n",
      "[[93677 15770]\n",
      " [ 2631  2727]]\n",
      "[[77894 31553]\n",
      " [ 2154  3204]]\n",
      "[[107491   1956]\n",
      " [  4704    654]]\n",
      "[[109366     81]\n",
      " [  5259     99]]\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Age at Injury\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Accident Month\n",
      "no missing values to input on Accident Day\n",
      "no missing values to input on Accident DayOfWeek\n",
      "Done DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   25.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done RF\n",
      "Done XGB\n",
      "Done KNN\n",
      "Iteration 1, loss = 0.15622841\n",
      "Iteration 2, loss = 0.14291577\n",
      "Iteration 3, loss = 0.14290111\n",
      "Iteration 4, loss = 0.14289014\n",
      "Iteration 5, loss = 0.14288094\n",
      "Iteration 6, loss = 0.14287287\n",
      "Iteration 7, loss = 0.14286558\n",
      "Iteration 8, loss = 0.14285889\n",
      "Iteration 9, loss = 0.14285268\n",
      "Iteration 10, loss = 0.14284684\n",
      "Iteration 11, loss = 0.14284133\n",
      "Iteration 12, loss = 0.14283609\n",
      "Iteration 13, loss = 0.14283109\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Done MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validation predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[73964 35483]\n",
      " [ 3736  1621]]\n",
      "[[97726 11721]\n",
      " [ 4331  1026]]\n",
      "[[73199 36248]\n",
      " [ 4057  1300]]\n",
      "[[109289    158]\n",
      " [  5275     82]]\n",
      "[[109436     11]\n",
      " [  5330     27]]\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Age at Injury\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Accident Month\n",
      "no missing values to input on Accident Day\n",
      "no missing values to input on Accident DayOfWeek\n",
      "Done DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   27.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done RF\n",
      "Done XGB\n",
      "Done KNN\n",
      "Iteration 1, loss = 0.15784769\n",
      "Iteration 2, loss = 0.14388127\n",
      "Iteration 3, loss = 0.14386678\n",
      "Iteration 4, loss = 0.14385595\n",
      "Iteration 5, loss = 0.14384688\n",
      "Iteration 6, loss = 0.14383891\n",
      "Iteration 7, loss = 0.14383173\n",
      "Iteration 8, loss = 0.14382513\n",
      "Iteration 9, loss = 0.14381899\n",
      "Iteration 10, loss = 0.14381324\n",
      "Iteration 11, loss = 0.14380780\n",
      "Iteration 12, loss = 0.14380264\n",
      "Iteration 13, loss = 0.14379770\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Done MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validation predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[70546 38901]\n",
      " [ 4283  1074]]\n",
      "[[95434 14013]\n",
      " [ 4747   610]]\n",
      "[[60942 48505]\n",
      " [ 4669   688]]\n",
      "[[109334    113]\n",
      " [  5257    100]]\n",
      "[[109418     29]\n",
      " [  5272     85]]\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Age at Injury\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Accident Month\n",
      "no missing values to input on Accident Day\n",
      "no missing values to input on Accident DayOfWeek\n",
      "Done DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   29.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done RF\n",
      "Done XGB\n",
      "Done KNN\n",
      "Iteration 1, loss = 0.15926771\n",
      "Iteration 2, loss = 0.14278852\n",
      "Iteration 3, loss = 0.14277780\n",
      "Iteration 4, loss = 0.14276966\n",
      "Iteration 5, loss = 0.14276285\n",
      "Iteration 6, loss = 0.14275686\n",
      "Iteration 7, loss = 0.14275146\n",
      "Iteration 8, loss = 0.14274650\n",
      "Iteration 9, loss = 0.14274190\n",
      "Iteration 10, loss = 0.14273758\n",
      "Iteration 11, loss = 0.14273350\n",
      "Iteration 12, loss = 0.14272962\n",
      "Iteration 13, loss = 0.14272592\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Done MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validation predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[71475 37972]\n",
      " [ 4471   886]]\n",
      "[[90220 19227]\n",
      " [ 4864   493]]\n",
      "[[62847 46600]\n",
      " [ 4462   895]]\n",
      "[[109095    352]\n",
      " [  5228    129]]\n",
      "[[109420     27]\n",
      " [  5336     21]]\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Age at Injury\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Accident Month\n",
      "no missing values to input on Accident Day\n",
      "no missing values to input on Accident DayOfWeek\n",
      "Done DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   28.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done RF\n",
      "Done XGB\n",
      "Done KNN\n",
      "Iteration 1, loss = 0.15744894\n",
      "Iteration 2, loss = 0.14112732\n",
      "Iteration 3, loss = 0.14111371\n",
      "Iteration 4, loss = 0.14110361\n",
      "Iteration 5, loss = 0.14109517\n",
      "Iteration 6, loss = 0.14108777\n",
      "Iteration 7, loss = 0.14108110\n",
      "Iteration 8, loss = 0.14107498\n",
      "Iteration 9, loss = 0.14106931\n",
      "Iteration 10, loss = 0.14106398\n",
      "Iteration 11, loss = 0.14105896\n",
      "Iteration 12, loss = 0.14105418\n",
      "Iteration 13, loss = 0.14104963\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Done MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validation predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\miniconda3\\envs\\ML2425\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[109076    370]\n",
      " [  4881    477]]\n",
      "[[109437      9]\n",
      " [  5094    264]]\n",
      "[[109426     20]\n",
      " [  4994    364]]\n",
      "[[109429     17]\n",
      " [  5211    147]]\n",
      "[[109442      4]\n",
      " [  5315     43]]\n"
     ]
    }
   ],
   "source": [
    "results_secondary_target = cv_scores(X, y2, num_features, cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f8d3cfc-dff8-42ef-aea3-8bea0a2d53ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train_precision</th>\n",
       "      <th>Test_precision</th>\n",
       "      <th>Train_recall</th>\n",
       "      <th>Test_recall</th>\n",
       "      <th>Train_f1_score</th>\n",
       "      <th>Test_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.546958</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.484038</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.438182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.604890</td>\n",
       "      <td>0.999906</td>\n",
       "      <td>0.540068</td>\n",
       "      <td>0.999951</td>\n",
       "      <td>0.514073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.907491</td>\n",
       "      <td>0.584703</td>\n",
       "      <td>0.637352</td>\n",
       "      <td>0.471547</td>\n",
       "      <td>0.698709</td>\n",
       "      <td>0.439743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNearestNeighbors</th>\n",
       "      <td>0.895985</td>\n",
       "      <td>0.700122</td>\n",
       "      <td>0.536993</td>\n",
       "      <td>0.518383</td>\n",
       "      <td>0.557173</td>\n",
       "      <td>0.520026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi Layer Perceptron</th>\n",
       "      <td>0.476674</td>\n",
       "      <td>0.812743</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.504994</td>\n",
       "      <td>0.488058</td>\n",
       "      <td>0.498157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Train_precision  Test_precision  Train_recall  \\\n",
       "Decision Tree                  1.000000        0.546958      1.000000   \n",
       "Random Forest                  0.999995        0.604890      0.999906   \n",
       "XGBoost                        0.907491        0.584703      0.637352   \n",
       "KNearestNeighbors              0.895985        0.700122      0.536993   \n",
       "Multi Layer Perceptron         0.476674        0.812743      0.500000   \n",
       "\n",
       "                        Test_recall  Train_f1_score  Test_f1_score  \n",
       "Decision Tree              0.484038        1.000000       0.438182  \n",
       "Random Forest              0.540068        0.999951       0.514073  \n",
       "XGBoost                    0.471547        0.698709       0.439743  \n",
       "KNearestNeighbors          0.518383        0.557173       0.520026  \n",
       "Multi Layer Perceptron     0.504994        0.488058       0.498157  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the results\n",
    "results_secondary_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479c6461-4660-46f6-9a61-481d5c652a40",
   "metadata": {},
   "source": [
    "# Run Test Predictions with best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247e2b02-b4e7-4d25-a5f8-2c4ed317e641",
   "metadata": {},
   "source": [
    "## 1. Main Target Variable Only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1106b3d2-3154-43f4-81f8-d49a6cfe172c",
   "metadata": {},
   "source": [
    "### Best model for Claim Injury Type target test prediction: **XGBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77969de7-0da9-4ffd-81c7-18e64394dcd1",
   "metadata": {},
   "source": [
    "#### V1: XGBoost with imputation of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923c21bd-9847-47e1-91ff-70b453b3b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"submission_without_missing = test_prediction(\n",
    "    XGBClassifier(),\n",
    "    X,y,\n",
    "    num_features,cat_features,\n",
    "    data_test,\n",
    "    scaling_outlier= True)\n",
    "submission_without_missing\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe86d483",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to csv\n",
    "\"\"\"submission_without_missing.to_csv(\"submission_without_missing.csv\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a63471c-235f-4bb5-9e2d-d43ac111766a",
   "metadata": {},
   "source": [
    "#### V2: XGBoost without imputation of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "192cf314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\AppData\\Local\\Temp\\ipykernel_16740\\3071127135.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_val['Carrier Claim Category'].fillna(0, inplace = True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claim Injury Type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claim Identifier</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6165911</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166141</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6165907</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166047</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166102</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553137</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553119</th>\n",
       "      <td>1. CANCELLED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553542</th>\n",
       "      <td>1. CANCELLED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553455</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553594</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>387975 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Claim Injury Type\n",
       "Claim Identifier                  \n",
       "6165911                2. NON-COMP\n",
       "6166141                2. NON-COMP\n",
       "6165907                2. NON-COMP\n",
       "6166047                2. NON-COMP\n",
       "6166102                2. NON-COMP\n",
       "...                            ...\n",
       "6553137                2. NON-COMP\n",
       "6553119               1. CANCELLED\n",
       "6553542               1. CANCELLED\n",
       "6553455                2. NON-COMP\n",
       "6553594                2. NON-COMP\n",
       "\n",
       "[387975 rows x 1 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using xgboost´s way of dealing with missing values (better performance)\n",
    "submission_with_missing = test_prediction(\n",
    "    XGBClassifier(),\n",
    "    X,y,\n",
    "    num_features,cat_features,\n",
    "    data_test, \n",
    "    missing= False)\n",
    "submission_with_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7985054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to csv\n",
    "submission_with_missing.to_csv(\"../../data/submission_with_missing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f67a4c-851d-46cc-98ca-71415f23befd",
   "metadata": {},
   "source": [
    "#### RESULT KAGGLE: 0.44011"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c35717-7aa8-4a0b-b19f-2fe66051de49",
   "metadata": {},
   "source": [
    "## 2. Secondary Target Variable AND Main Target Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8720825-7090-4cd9-b652-a62daeba82f5",
   "metadata": {},
   "source": [
    "### Best model prediction of secondary target var: KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fcd304ef-8872-47f2-a878-94a4b28c9ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\AppData\\Local\\Temp\\ipykernel_16740\\1763526717.py:2: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data= pd.read_csv(\"../../data/train_data_enriched.csv\", index_col=\"Claim Identifier\")\n"
     ]
    }
   ],
   "source": [
    "# Reload data\n",
    "data= pd.read_csv(\"../../data/train_data_enriched.csv\", index_col=\"Claim Identifier\")\n",
    "data_test = pd.read_csv(\"../../data/test_data_enriched.csv\",index_col=\"Claim Identifier\")\n",
    "X = data.drop([\"Claim Injury Type\",\"Agreement Reached\"], axis = 1)\n",
    "y = data[\"Claim Injury Type\"]\n",
    "y2 = data[\"Agreement Reached\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a58fbfa-0a3a-458d-ae43-8a1ccd7fd8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Louis\\AppData\\Local\\Temp\\ipykernel_40632\\3071127135.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_val['Carrier Claim Category'].fillna(0, inplace = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no missing values to input on Number of Dependents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claim Injury Type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claim Identifier</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6165911</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166141</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6165907</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166047</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166102</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553137</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553119</th>\n",
       "      <td>1. CANCELLED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553542</th>\n",
       "      <td>1. CANCELLED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553455</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553594</th>\n",
       "      <td>1. CANCELLED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>387975 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Claim Injury Type\n",
       "Claim Identifier                  \n",
       "6165911                2. NON-COMP\n",
       "6166141                2. NON-COMP\n",
       "6165907                2. NON-COMP\n",
       "6166047                2. NON-COMP\n",
       "6166102                2. NON-COMP\n",
       "...                            ...\n",
       "6553137                2. NON-COMP\n",
       "6553119               1. CANCELLED\n",
       "6553542               1. CANCELLED\n",
       "6553455                2. NON-COMP\n",
       "6553594               1. CANCELLED\n",
       "\n",
       "[387975 rows x 1 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_with_missing_with_v2 = test_prediction(\n",
    "    XGBClassifier(), # y classifier\n",
    "    X, y,# Dataset without target, target var 1\n",
    "    num_features,\n",
    "    cat_features,\n",
    "    data_test,\n",
    "    missing= False,\n",
    "    secondary_model = KNeighborsClassifier(n_neighbors = 50), # y2 classifier\n",
    "    y_train_secondary = y2, # target var 2\n",
    "    secondary_missing = True\n",
    "    )\n",
    "submission_with_missing_with_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d050c-d39c-4fbd-aa16-27cc9bb3cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_with_missing_with_v2.to_csv(\"../../data/submission_with_missing_with_v2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e898c472-a39d-49a8-9e94-bc085872dd42",
   "metadata": {},
   "source": [
    "#### RESULT KAGGLE: 0.43776"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58323e4f-eafc-4ba7-bad8-30f64945dfaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f84d732-8a31-4afd-8235-3086955a802b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27aa9a5-a747-4019-8d57-0e733564a351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6825fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def cv_scores_hyperparameter_tuning(X, y, num_features, cat_features, num_imputing_algorithm=XGBRegressor(), cat_imputing_algorithm=XGBClassifier(), scaling_outlier= False, scaler=MinMaxScaler(), rfe = False):\n",
    "    \n",
    "    Takes as argument the predictors and the target, the models used for imputing numerical and categorical \n",
    "    features, if any scaling and outlier removal should be performed,what scaling method should be used and if feature selection with rfe should be used.\n",
    "    Then it returns the results obtained from the stratified cross-validation for the given models.\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # Generating the lists to store our results\n",
    "    precision_scores_train = []\n",
    "    precision_scores_val = []\n",
    "    recall_scores_train = []\n",
    "    recall_scores_val = []\n",
    "    f1_scores_train =  []\n",
    "    f1_scores_val =  []\n",
    "\n",
    "    precision_scores_train_mean = []\n",
    "    precision_scores_val_mean = [] \n",
    "    recall_scores_train_mean = []\n",
    "    recall_scores_val_mean = []\n",
    "    f1_scores_train_mean =  []\n",
    "    f1_scores_val_mean =  []\n",
    "    \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        # Dividing our data in validation and train\n",
    "        X_train, X_val = X.iloc[train_index].copy(), X.iloc[test_index].copy()\n",
    "        y_train, y_val = y.iloc[train_index].copy(), y.iloc[test_index].copy()\n",
    "\n",
    "        # Filling missing values\n",
    "        for column in num_features:\n",
    "            X_train, X_val = impute_missing_values(X_train,X_val, column, num_imputing_algorithm)\n",
    "        \n",
    "        # Removing inconsistencies on the train\n",
    "        inconsistent = X_train[(X_train['Age at Injury'] > 80) | (X_train[\"Age at Injury\"] < 16)].index\n",
    "        X_train = X_train.loc[~X_train.index.isin(inconsistent)]\n",
    "        y_train = y_train.loc[~y_train.index.isin(inconsistent)]\n",
    "\n",
    "        # Performing scaling and outlier treatment dependent on the boolean\n",
    "        if scaling_outlier:\n",
    "            for column in num_features:\n",
    "                handle_outliers(X_train, column)\n",
    "                X_train, X_val = impute_missing_values(X_train,X_val, column, num_imputing_algorithm,validation= False)\n",
    "\n",
    "            for column in num_features:\n",
    "                scale_numerical(column, X_train, X_val, scaler)\n",
    "                \n",
    "        # Creating an ordinal variable\n",
    "        claim_carrier_categories(X_train, X_val)\n",
    "\n",
    "        #Filling missing values in the ordinal variable that might appear on X_val\n",
    "        X_val, X_train = impute_missing_values(X_val,X_train, \"Carrier Claim Category\", cat_imputing_algorithm, validation=False)\n",
    "\n",
    "        # Categorical Prop Encoding\n",
    "        for cat_feature in cat_features:\n",
    "            categorical_prop_encode(X_train, X_val, cat_feature)\n",
    "            if scaling_outlier:\n",
    "                scale_numerical(\"Carrier Claim Category\", X_train, X_val, scaler)\n",
    "\n",
    "        # Selecting features with Rfe\n",
    "        if rfe:\n",
    "            Selected_features = Rfe(XGBClassifier(), X_train, y_train, X_val, y_val)\n",
    "            X_train = X_train[Selected_features]\n",
    "            X_val = X_val[Selected_features]\n",
    "\n",
    "        # Training the classification models\n",
    "        XGBT.fit(X_train, y_train)\n",
    "        print(\"Done XGBT\")\n",
    "\n",
    "        # Making the predictions for the training and validation data\n",
    "        pred_train_XGBT = XGBT.predict(X_train)\n",
    "        print(\"Done training predictions\")\n",
    "        \n",
    "        pred_val_XGBT = XGBT.predict(X_val)\n",
    "        print(\"Done validation predictions\")\n",
    "\n",
    "        # Calculating and storing the scores\n",
    "        precision_scores_train.append(precision_score(y_train, pred_train_XGBT, average='macro'))\n",
    "        recall_scores_train.append(recall_score(y_train, pred_train_XGBT, average='macro'))\n",
    "        f1_scores_train.append(f1_score(y_train, pred_train_XGBT, average='macro'))\n",
    "        \n",
    "        precision_scores_val.append(precision_score(y_val, pred_val_XGBT, average='macro'))\n",
    "        recall_scores_val.append(recall_score(y_val, pred_val_XGBT, average='macro'))\n",
    "        f1_scores_val.append(f1_score(y_val, pred_val_XGBT, average='macro'))\n",
    "\n",
    "\n",
    "    # Aggregating the average results across the folds\n",
    "    precision_scores_train_mean.append(mean(precision_scores_train))\n",
    "    precision_scores_val_mean.append(mean(precision_scores_val))\n",
    "    recall_scores_train_mean.append(mean(recall_scores_train))\n",
    "    recall_scores_val_mean.append(mean(recall_scores_val))\n",
    "    f1_scores_train_mean.append(mean(f1_scores_train))\n",
    "    f1_scores_val_mean.append(mean(f1_scores_val))\n",
    "\n",
    "    # Storing the results in a dataframe\n",
    "    scores = {'Train_precision': precision_scores_train_mean,\n",
    "    'Test_precision': precision_scores_val_mean,\n",
    "    'Train_recall': recall_scores_train_mean,\n",
    "    'Test_recall': recall_scores_val_mean,\n",
    "    'Train_f1_score': f1_scores_train_mean,\n",
    "    'Test_f1_score': f1_scores_val_mean}\n",
    "\n",
    "    print(scores)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0b6940",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import xgboost as xgb\n",
    "for gama in range(10):\n",
    "    for depth in range(4,8):\n",
    "        XGBT = XGBClassifier(gamma = gama, max_depth = depth)\n",
    "        print(boost, gama, depth)\n",
    "        cv_scores_hyperparameter_tuning(X, y,num_features,cat_features,scaling_outlier = True)\n",
    "        print(\"------------\")\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "039090e4-5406-4b1c-b3c9-5b663cc2872e",
   "metadata": {},
   "source": [
    "# Model Training and Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d29f30c5-6ec1-4339-a2af-a73bd36f91c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#model imports\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier,XGBRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from statistics import mean\n",
    "\n",
    "#useful functions\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be579d5a-846d-4a28-923c-bee09ea5aa38",
   "metadata": {},
   "source": [
    "# Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34803428-b671-4537-b669-66a6fad30c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae25eec-6e04-4da5-a564-5fc146a80596",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41080786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(X_train, X_val, target_column, algorithm,validation = True):\n",
    "\n",
    "    # Separating the missing values from the non missing values\n",
    "    available_data = X_train[X_train[target_column].notna()]\n",
    "    missing_X_train = X_train[X_train[target_column].isna()]\n",
    "    missing_X_val = X_val[X_val[target_column].isna()]\n",
    "\n",
    "    # Making sure if there is enough data for inputing, returning it if not\n",
    "    if len(missing_X_train) == 0:\n",
    "        print(f\"no missing values to input on {target_column}\")\n",
    "        return X_train, X_val\n",
    "\n",
    "    # Separating the target column from the rest\n",
    "    X_available = available_data.drop(columns=[target_column])\n",
    "    y_available = available_data[target_column]\n",
    "\n",
    "    # Making sure our columns are consistent\n",
    "    X_available = X_available.select_dtypes(include=[\"number\"])\n",
    "    missing_X_train = missing_X_train.select_dtypes(include=[\"number\"])\n",
    "    missing_X_val = missing_X_val.select_dtypes(include=[\"number\"])\n",
    "\n",
    "    common_columns = X_available.columns.intersection(missing_X_train.columns).intersection(missing_X_val.columns)\n",
    "    X_available = X_available[common_columns]\n",
    "    missing_X_train = missing_X_train[common_columns]\n",
    "    missing_X_val = missing_X_val[common_columns]\n",
    "\n",
    "    # Making sure there is any column after keeping the common columns\n",
    "    if X_available.shape[1] == 0:\n",
    "        print(f\"Without any column to input in {target_column}\")\n",
    "        return X_train, X_val\n",
    "\n",
    "    # Training the model with the available data\n",
    "    model = algorithm\n",
    "    model.fit(X_available, y_available)\n",
    "\n",
    "    # Prediting the missing values\n",
    "    predicted_train = model.predict(missing_X_train)\n",
    "    if validation:\n",
    "        predicted_val = model.predict(missing_X_val)\n",
    "\n",
    "    # Filling the training and validation with predictions. The latter is only filled if the argument is true\n",
    "    X_train = X_train.copy()\n",
    "    X_train.loc[X_train[target_column].isna(), target_column] = predicted_train\n",
    "\n",
    "    if validation:\n",
    "        X_val = X_val.copy()\n",
    "        X_val.loc[X_val[target_column].isna(), target_column] = predicted_val\n",
    "\n",
    "    return X_train, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e1910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This helper fuction was used for finding problems with later functions\n",
    "def check_missing_values(data):\n",
    "    print(data.isnull().sum()[data.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f20a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers(data, column):\n",
    "    # Handles outliers in a numerical column by replacing values outside the interquartile range (IQR) with missing values\n",
    "\n",
    "    # Makes sure we only treat outliers in columns that have any data\n",
    "    if data[column].notnull().sum() > 0: \n",
    "\n",
    "        # Calculating inter quantile range limits\n",
    "        Q1 = data[column].quantile(0.25)\n",
    "        Q3 = data[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Changing the ouliers to missing values\n",
    "        data[column] = np.where(data[column] < lower_bound, np.nan, data[column])\n",
    "        data[column] = np.where(data[column] > upper_bound, np.nan, data[column])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9e310a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_numerical(column, X_train, X_val, scaler):\n",
    "    # Scales the given numerical value based on the training data with the given scaler\n",
    "    \n",
    "    # Make sure the column is numerical\n",
    "    if not pd.api.types.is_numeric_dtype(X_train[column]):\n",
    "        print(f\"Columm '{column}' is not numerical and will be ignored\")\n",
    "        return\n",
    "\n",
    "    # Scaling the data\n",
    "    try:\n",
    "        X_train[column] = scaler.fit_transform(X_train[[column]])\n",
    "        X_val[column] = scaler.transform(X_val[[column]])\n",
    "    except ValueError as e:\n",
    "        print(f\"Mistake scaling the column '{column}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92397747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  claim_carrier_categories(X_train, X_val):\n",
    "    if 'Carrier Claim Category' not in X_train.columns:\n",
    "        # Function to categorize each carrier based on its claim count for dimensionality reduction\n",
    "        \n",
    "        count = X_train['Carrier Name'].value_counts() # Count individual Carriers' counts only on train data\n",
    "        def categorize_claims(count): # Map carrier size based on fixed thresholds decided by us\n",
    "            if count >= 40000:\n",
    "                return 2\n",
    "            elif 4000 <= count < 40000:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "    \n",
    "        # Apply the categorization to create a mapping dictionary\n",
    "        carrier_category_map = count.apply(categorize_claims)\n",
    "    \n",
    "        # Map the `Carrier Name` to the new `Carrier Claim Category`\n",
    "        X_train['Carrier Claim Category'] = X_train['Carrier Name'].map(carrier_category_map)\n",
    "        X_val['Carrier Claim Category'] = X_val['Carrier Name'].map(carrier_category_map)\n",
    "\n",
    "        # If there is a missing value on x_val, it means that that carrier name didn't exist on x_train and is unlikely to have many claim counts\n",
    "        # X_train cannot have NaN in this new feature as Carrier Name has no Missing values\n",
    "        X_val['Carrier Claim Category'].fillna(0, inplace = True)\n",
    "    \n",
    "        return X_train.drop([\"Carrier Name\"], axis = 1, inplace = True) , X_val.drop([\"Carrier Name\"], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db2cfcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_prop_encode(X_train, X_val, feature):\n",
    "    # Categorical encoder function for individual feature\n",
    "    proportion = X_train[feature].value_counts(normalize = True)  # Get the porportion of each category\n",
    "    X_train[feature] = X_train[feature].map(proportion)  # Map the porportions in the column\n",
    "    X_val[feature] = X_val[feature].map(proportion) # Do the same for the validation subset\n",
    "    X_val[feature] = X_val[feature].fillna(0)  # Handle categories in X_val not seen in X_train with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4efb3e3-69d9-4968-a39c-9f2cd20c4ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rfe(algorithm,X_train,y_train,X_val,y_val):\n",
    "    # Function to run RFE on specified algorithm with train/val split for it to be run within every fold to avoid data leakage\n",
    "    \n",
    "    #Generating the variables where we will store our results\n",
    "    nof_list = np.arange(1, len(X_train.columns) + 1)            \n",
    "    high_score = 0\n",
    "    opt_n_features = 0\n",
    "    train_score_list = []\n",
    "    val_score_list = []\n",
    "\n",
    "    #Variable where we will store the optimum amount of features\n",
    "    best_rfe = None\n",
    "\n",
    "    model = algorithm\n",
    "\n",
    "    for n in nof_list:\n",
    "        rfe = RFE(estimator=model, n_features_to_select=n)\n",
    "    \n",
    "    # Fitting the model to rfe\n",
    "        X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "        X_val_rfe = rfe.transform(X_val)\n",
    "    \n",
    "    # Training and predicting\n",
    "        model.fit(X_train_rfe, y_train)\n",
    "        pred_train = model.predict(X_train_rfe)\n",
    "        pred_val = model.predict(X_val_rfe)\n",
    "    \n",
    "    # Evaluating using the macro f1_score\n",
    "        train_score = f1_score(y_train, pred_train, average=\"macro\")\n",
    "        val_score = f1_score(y_val, pred_val, average=\"macro\")\n",
    "        train_score_list.append(train_score)\n",
    "        val_score_list.append(val_score)\n",
    "    \n",
    "    # Checking if this is the best combination of features so far\n",
    "        if val_score >= high_score:\n",
    "            high_score = val_score\n",
    "            opt_n_features = n\n",
    "            best_rfe = rfe  # Storing the rfe with the best number of features\n",
    "\n",
    "# Checking what amount of features and which features where the best for the model\n",
    "    selected_features = X_train.columns[best_rfe.support_].tolist()\n",
    "\n",
    "    print(\"Optimal number of features: %d\" % opt_n_features)\n",
    "    print(\"Score with %d features: %f\" % (opt_n_features, high_score))\n",
    "    print(\"Selected Features:\\n\", selected_features)\n",
    "\n",
    "    return selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc04b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_scores(X, y, num_features, cat_features, num_imputing_algorithm=XGBRegressor(), cat_imputing_algorithm=XGBClassifier(), scaling_outlier= True, scaler=MinMaxScaler(), rfe = False):\n",
    "    \"\"\"\n",
    "    Performs stratified cross-validation on a dataset to evaluate multiple classification models, while handling \n",
    "    preprocessing steps like missing value imputation, scaling, outlier removal, feature engineering, and optional \n",
    "    feature selection using Recursive Feature Elimination (RFE).\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    X : Entire data without target variable\n",
    "\n",
    "    y : Entire data with only target variable\n",
    "\n",
    "    num_features : List of numerical features within X\n",
    "\n",
    "    cat_features : List of categorical features within X (non-binary)\n",
    "\n",
    "    num_imputing_algorithm : Algorithm to be used for imputing numerical features' missing values. Defaults to XGBRegressor.\n",
    "\n",
    "    cat_imputing_algorithm : Algorithm to be used for imputing categorical features' missing values. Defaults to XGBClassifier.\n",
    "\n",
    "    scaling_outlier : Boolean indicating if scaling and outlier handling should be applied. Defaults to True.\n",
    "\n",
    "    scaler : Scaling algorithm to be used for scaling. Defaults to True.\n",
    "\n",
    "    rfe : Boolean indicating if RFE should be used within each fold to determine current most valuable features to use. Defaults to False.\n",
    "\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # Generating the lists to store our results\n",
    "    precision_scores_train = [[],[],[],[],[]]\n",
    "    precision_scores_val = [[],[],[],[],[]]  \n",
    "    recall_scores_train = [[],[],[],[],[]]\n",
    "    recall_scores_val = [[],[],[],[],[]]\n",
    "    f1_scores_train =  [[],[],[],[],[]]\n",
    "    f1_scores_val =  [[],[],[],[],[]]\n",
    "\n",
    "    precision_scores_train_mean = []\n",
    "    precision_scores_val_mean = [] \n",
    "    recall_scores_train_mean = []\n",
    "    recall_scores_val_mean = []\n",
    "    f1_scores_train_mean =  []\n",
    "    f1_scores_val_mean =  []\n",
    "    \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        # Dividing our data in validation and train\n",
    "        X_train, X_val = X.iloc[train_index].copy(), X.iloc[test_index].copy()\n",
    "        y_train, y_val = y.iloc[train_index].copy(), y.iloc[test_index].copy()\n",
    "\n",
    "        # Filling missing values\n",
    "        for column in num_features:\n",
    "            X_train, X_val = impute_missing_values(X_train,X_val, column, num_imputing_algorithm)\n",
    "        \n",
    "        # Removing inconsistencies on the train\n",
    "        inconsistent = X_train[(X_train['Age at Injury'] > 80) | (X_train[\"Age at Injury\"] < 16)].index\n",
    "        X_train = X_train.loc[~X_train.index.isin(inconsistent)]\n",
    "        y_train = y_train.loc[~y_train.index.isin(inconsistent)]\n",
    "\n",
    "        # Performing scaling and outlier treatment dependent on the boolean\n",
    "        if scaling_outlier:\n",
    "            for column in num_features:\n",
    "                handle_outliers(X_train, column)\n",
    "                X_train, X_val = impute_missing_values(X_train,X_val, column, num_imputing_algorithm,validation= False)\n",
    "\n",
    "            for column in num_features:\n",
    "                scale_numerical(column, X_train, X_val, scaler)\n",
    "                \n",
    "        # Creating an ordinal variable\n",
    "        claim_carrier_categories(X_train, X_val)\n",
    "\n",
    "\n",
    "        # Scaling special Carrier Claim Category feature\n",
    "        if scaling_outlier:\n",
    "            scale_numerical(\"Carrier Claim Category\", X_train, X_val, scaler)\n",
    "\n",
    "        # Categorical Prop Encoding\n",
    "        for cat_feature in cat_features:\n",
    "            categorical_prop_encode(X_train, X_val, cat_feature)\n",
    "            if scaling_outlier:\n",
    "                scale_numerical(\"Carrier Claim Category\", X_train, X_val, scaler)\n",
    "\n",
    "        # Selecting features with Rfe\n",
    "        if rfe:\n",
    "            Selected_features = Rfe(XGBClassifier(), X_train, y_train, X_val, y_val)\n",
    "            X_train = X_train[Selected_features]\n",
    "            X_val = X_val[Selected_features]\n",
    "\n",
    "        \n",
    "        # Training the classification models\n",
    "        DT.fit(X_train, y_train)\n",
    "        print(\"Done DT\")\n",
    "        RF.fit(X_train, y_train)\n",
    "        print(\"Done RF\")\n",
    "        XGB.fit(X_train, y_train)\n",
    "        print(\"Done XGB\")\n",
    "        KNN.fit(X_train, y_train)\n",
    "        print(\"Done KNN\")\n",
    "        MLP.fit(X_train, y_train)\n",
    "        print(\"Done MLP\")\n",
    "\n",
    "        # Making the predictions for the training and validation data\n",
    "        pred_train_DT = DT.predict(X_train)\n",
    "        pred_train_RF = RF.predict(X_train)\n",
    "        pred_train_XGB = XGB.predict(X_train)\n",
    "        pred_train_KNN = KNN.predict(X_train)\n",
    "        pred_train_MLP = MLP.predict(X_train)\n",
    "        print(\"Done training predictions\")\n",
    "        \n",
    "        pred_val_DT = DT.predict(X_val)\n",
    "        pred_val_RF = RF.predict(X_val)\n",
    "        pred_val_XGB = XGB.predict(X_val)\n",
    "        pred_val_KNN = KNN.predict(X_val)\n",
    "        pred_val_MLP = MLP.predict(X_val)\n",
    "        print(\"Done validation predictions\")\n",
    "\n",
    "        # Calculating and storing the scores\n",
    "        i = 0\n",
    "        for predictions in [pred_train_DT,pred_train_RF,pred_train_XGB,pred_train_KNN,pred_train_MLP]:\n",
    "            precision_scores_train[i].append(precision_score(y_train, predictions, average='macro'))\n",
    "            recall_scores_train[i].append(recall_score(y_train, predictions, average='macro'))\n",
    "            f1_scores_train[i].append(f1_score(y_train, predictions, average='macro'))\n",
    "            i+=1\n",
    "        j=0\n",
    "        for predictions in [pred_val_DT,pred_val_RF,pred_val_XGB,pred_val_KNN,pred_val_MLP]:\n",
    "            precision_scores_val[j].append(precision_score(y_val, predictions, average='macro'))\n",
    "            recall_scores_val[j].append(recall_score(y_val, predictions, average='macro'))\n",
    "            f1_scores_val[j].append(f1_score(y_val, predictions, average='macro'))\n",
    "            j+=1\n",
    "\n",
    "        # Check the confusion matrixes of our predictions\n",
    "        print(confusion_matrix(y_val, pred_val_DT))\n",
    "        print(confusion_matrix(y_val, pred_val_RF))\n",
    "        print(confusion_matrix(y_val, pred_val_XGB))\n",
    "        print(confusion_matrix(y_val, pred_val_KNN))\n",
    "        print(confusion_matrix(y_val, pred_val_MLP))\n",
    "\n",
    "    # Aggregating the average results across the folds\n",
    "    for l in range(0,5): \n",
    "        precision_scores_train_mean.append(mean(precision_scores_train[l]))\n",
    "        precision_scores_val_mean.append(mean(precision_scores_val[l]))\n",
    "        recall_scores_train_mean.append(mean(recall_scores_train[l]))\n",
    "        recall_scores_val_mean.append(mean(recall_scores_val[l]))\n",
    "        f1_scores_train_mean.append(mean(f1_scores_train[l]))\n",
    "        f1_scores_val_mean.append(mean(f1_scores_val[l]))\n",
    "\n",
    "    # Storing the results in a dataframe\n",
    "    model_results = pd.DataFrame(data={\n",
    "        'Train_precision': precision_scores_train_mean,\n",
    "        'Test_precision': precision_scores_val_mean,\n",
    "        'Train_recall': recall_scores_train_mean,\n",
    "        'Test_recall': recall_scores_val_mean,\n",
    "        'Train_f1_score': f1_scores_train_mean,\n",
    "        'Test_f1_score': f1_scores_val_mean,\n",
    "    }, index=[\"Decision Tree\",\"Random Forest\",\"XGBoost\", \"KNearestNeighbors\",\"Multi Layer Perceptron\"])\n",
    "\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb306304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(model, X_train, y_train, num_features, cat_features, X_test, \n",
    "                    num_imputing_algorithm=XGBRegressor(), \n",
    "                    cat_imputing_algorithm=XGBClassifier(), scaling_outlier = False , \n",
    "                    scaler=MinMaxScaler(), missing = True,\n",
    "                    secondary_model = None, y_train_secondary=None, secondary_missing = True):\n",
    "\n",
    "    \"\"\"\n",
    "    Performs stratified cross-validation on a dataset to evaluate multiple classification models, while handling \n",
    "    preprocessing steps like missing value imputation, scaling, outlier removal, feature engineering, and optional \n",
    "    feature selection using Recursive Feature Elimination (RFE).\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    model: algorithm to use to predict primary target variable.\n",
    "    \n",
    "    X_train : Entire data without target variable.\n",
    "\n",
    "    y_train : Entire data with only target variable.\n",
    "\n",
    "    num_features : List of numerical features within X\n",
    "\n",
    "    cat_features : List of categorical features within X (non-binary)\n",
    "\n",
    "    num_imputing_algorithm : Algorithm to be used for imputing numerical features' missing values. Defaults to XGBRegressor.\n",
    "\n",
    "    cat_imputing_algorithm : Algorithm to be used for imputing categorical features' missing values. Defaults to XGBClassifier.\n",
    "\n",
    "    scaling_outlier : Boolean indicating if scaling and outlier handling should be applied. Defaults to True.\n",
    "\n",
    "    scaler : Scaling algorithm to be used for scaling. Defaults to True.\n",
    "\n",
    "    rfe : Boolean indicating if RFE should be used within each fold to determine current most valuable features to use. Defaults to False.\n",
    "\n",
    "    secondary_model: algorithm to use to predict secondary target variable. Defaults to None if secondary should not be predicted.\n",
    "\n",
    "    y_train_secondary: Entire data with only secondary target variable. Defaults to None if secondary should not be predicted.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Impute missing values\n",
    "    if missing:\n",
    "        for column in num_features:\n",
    "            X_train, X_test = impute_missing_values(X_train,X_test,column, num_imputing_algorithm)\n",
    "\n",
    "    # Remove inconsistencies\n",
    "    inconsistent = X_train[(X_train['Age at Injury'] > 80) | (X_train[\"Age at Injury\"] < 16)].index\n",
    "    X_train.drop(inconsistent, inplace=True)\n",
    "    y_train.drop(inconsistent, inplace=True)\n",
    "    if secondary_model is not None and y_train_secondary is not None:\n",
    "        y_train_secondary.drop(inconsistent, inplace=True)\n",
    "\n",
    "    # Scale and remove outliers if specified\n",
    "    if scaling_outlier:\n",
    "        for column in num_features:\n",
    "            handle_outliers(X_train, column) # Handle outliers only for training partition\n",
    "            if missing:\n",
    "                X_train, X_test = impute_missing_values(X_train,X_test, column, num_imputing_algorithm,validation = False)\n",
    "\n",
    "        for column in num_features:\n",
    "            scale_numerical(column, X_train, X_test, scaler)\n",
    "        \n",
    "    # Creating an ordinal variable\n",
    "    claim_carrier_categories(X_train, X_test)\n",
    "\n",
    "    # Scaling special Carrier Claim Category feature\n",
    "    if scaling_outlier:\n",
    "        scale_numerical(\"Carrier Claim Category\", X_train, X_test, scaler)\n",
    "\n",
    "    # Categorical Prop Encoding\n",
    "    for cat_feature in cat_features:\n",
    "        categorical_prop_encode(X_train, X_test, cat_feature)\n",
    "        if scaling_outlier:\n",
    "            scale_numerical(\"Carrier Claim Category\", X_train, X_test, scaler)\n",
    "\n",
    "    # Predict secondary target variable if secondary model and variable is given\n",
    "    if secondary_model is not None and y_train_secondary is not None:\n",
    "        if secondary_missing: # Imputation of missing values for secondary model\n",
    "            X_train_secondary = X_train.copy()\n",
    "            X_test_secondary = X_test.copy()\n",
    "            for column in num_features:\n",
    "                X_train_secondary, X_test_secondary = impute_missing_values(X_train_secondary,X_test_secondary, column, num_imputing_algorithm)\n",
    "\n",
    "        secondary_model.fit(X_train_secondary, y_train_secondary)\n",
    "        pred_secondary_test = secondary_model.predict(X_test_secondary)\n",
    "        X_test[\"Agreement Reached\"] = pred_secondary_test\n",
    "        X_train[\"Agreement Reached\"] = y_train_secondary\n",
    "        \n",
    "    # Fitting the model, making the predictions and reverting the claim injury types back to their string form\n",
    "    model.fit(X_train, y_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    pred_test = le.inverse_transform(pred_test)\n",
    "\n",
    "    # Saving the final submission dataframe with indexes of X_test\n",
    "    submission_df = pd.DataFrame({\n",
    "        \"Claim Injury Type\": pred_test\n",
    "    }, index=X_test.index)\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fb4280-214e-47bf-8c0d-dee36a318c80",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63f6b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_load():\n",
    "    #Define an instance of the models without hyperparameters\n",
    "\n",
    "    DT = DecisionTreeClassifier()\n",
    "    RF = RandomForestClassifier(verbose = 1, n_jobs=-1)\n",
    "    XGB = XGBClassifier()\n",
    "    KNN = KNeighborsClassifier(n_neighbors = 50) # n_neighbors through trial and error\n",
    "    MLP = MLPClassifier(activation='relu',\n",
    "        solver='sgd',learning_rate='invscaling',\n",
    "        learning_rate_init=0.001,\n",
    "        batch_size=100,verbose = True) # Based on ML Practical Parameters\n",
    "    return DT , RF , XGB , KNN , MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acfed470-3d79-409b-9f56-0acb694dd3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load():\n",
    "    # Load data\n",
    "    data= pd.read_csv(\"../../data/train_data_enriched.csv\", index_col=\"Claim Identifier\")\n",
    "    data_test = pd.read_csv(\"../../data/test_data_enriched.csv\",index_col=\"Claim Identifier\")\n",
    "\n",
    "    #Creating an instance of our encoder for the target\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    #Label enconding our target variable \n",
    "    data[\"Claim Injury Type\"] = le.fit_transform(data[\"Claim Injury Type\"])\n",
    "\n",
    "    '''Dropping redundant variables that carry almost the same information (are extremely correlated (|0.8|))\n",
    "    We believe it was better to keep Age at Injury than birth year since it should be more related to the injury claim type (it will be tested later)\n",
    "    The same logic was applied to dropping the other two dates and two DSA variables since we believe Accident date to be more important'''\n",
    "    data = data.loc[:, ~data.columns.isin(['Birth Year', 'Assembly Date', 'C-2 Date', 'Assembly Date DSA', 'First Hearing Date DSA'])]\n",
    "    data_test = data_test.loc[:, ~data_test.columns.isin(['Birth Year', 'Assembly Date', 'C-2 Date', 'Assembly Date DSA', 'First Hearing Date DSA'])]\n",
    "\n",
    "\n",
    "    '''Since the codes always seem to provide the same or more information than the descriptions (have more categories),\n",
    "    and the codes are consistent (always only having 1 description for code, while descriptions may have multiple codes)\n",
    "    And CrÃ¡mer's V says they have a very high association\n",
    "    we will drop the description columns.'''\n",
    "    data.drop(['Industry Code Description','WCIO Cause of Injury Description','WCIO Nature of Injury Description','WCIO Part Of Body Description'], axis = 1,inplace = True)\n",
    "    data_test.drop(['Industry Code Description','WCIO Cause of Injury Description','WCIO Nature of Injury Description','WCIO Part Of Body Description'], axis = 1,inplace = True)\n",
    "\n",
    "    '''Dropping redundant variables that carry almost the same information (have an association above or equal to 0.8)\n",
    "    We chose to keep County of Injury above Zip Code and District Name (these 3 have a high association) because it is the easist to interpret and the one we looked more in detail in the exploratory analysis\n",
    "    We kept the new variable we made called body section because it keeps most of the same information of the body part code but with a much lower cardinality\n",
    "    Lastly we only remove the variable Carrier Name in the function where we create the new variable with lower cardinality because it is need to create that new variable'''\n",
    "    data.drop(['Zip Code',\"WCIO Part Of Body Code\",\"District Name\"], axis=1, inplace = True)\n",
    "    data_test.drop([\"Zip Code\",\"WCIO Part Of Body Code\",\"District Name\"], axis=1 , inplace = True)\n",
    "\n",
    "    # Num and Cat features\n",
    "    num_features = ['Age at Injury', 'Average Weekly Wage', 'IME-4 Count', 'Number of Dependents',\n",
    "                \"Accident Year\",\"Accident Month\",\"Accident Day\",\"Accident DayOfWeek\",\n",
    "                \"C-2 Date DSA\",\"C-3 Date DSA\",\"Accident Date\",\"C-3 Date\",\"First Hearing Date\"]\n",
    "\n",
    "    cat_features = [\n",
    "    \"Alternative Dispute Resolution\",\n",
    "    \"Carrier Type\",\n",
    "    \"County of Injury\",\n",
    "    \"Gender\",\n",
    "    \"Industry Code\",\n",
    "    \"Medical Fee Region\",\n",
    "    \"WCIO Cause of Injury Code\",\n",
    "    \"WCIO Nature of Injury Code\",\n",
    "    \"Age at Injury Category\",\n",
    "    \"Body Section\",\n",
    "    ]\n",
    "    \n",
    "    # Isolation of target vars\n",
    "    X = data.drop([\"Claim Injury Type\",\"Agreement Reached\"], axis = 1)\n",
    "    y = data[\"Claim Injury Type\"]\n",
    "    y2 = data[\"Agreement Reached\"]\n",
    "\n",
    "    return data, data_test, le, num_features, cat_features, X, y, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48485ea7-f577-4de5-91d9-dddf50760d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_7256/3389366436.py:3: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data= pd.read_csv(\"../../data/train_data_enriched.csv\", index_col=\"Claim Identifier\")\n"
     ]
    }
   ],
   "source": [
    "# Reset all models, datasets and global Parameters for fresh retraining\n",
    "DT , RF , XGB , KNN , MLP = model_load()\n",
    "data, data_test,le, num_features, cat_features, X, y, y2 = data_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9f7bf2-8050-4a26-91f6-f76141bf5bff",
   "metadata": {},
   "source": [
    "### Quality Checks\n",
    "The only not included variables are the target variables and all the binary variables (non-numeric and non-categorical) as seen below. As binary variables are not scaled nor encoded they don't appear in the definitions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b06d9ea3-cc03-4fe9-9066-43ca1b55243c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(num_features) + len(cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5560997e-b2f3-4465-a89c-e8b32a20aaa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3648707-ed6c-4948-96a9-88783e9cc70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns not in num_features or cat_features: {'Attorney/Representative', 'COVID-19 Indicator', 'Carrier Name', 'First Hearing Date_missing', 'C-3 Date_missing', 'Assembly Date_missing', 'Accident Date_missing', 'Agreement Reached', 'Claim Injury Type', 'C-2 Date_missing'}\n",
      "Count of difference: 10\n"
     ]
    }
   ],
   "source": [
    "# Combine num_features and cat_features into a set\n",
    "defined_features = set(num_features) | set(cat_features)\n",
    "\n",
    "# Get all column names in the DataFrame\n",
    "all_columns = set(data.columns)\n",
    "\n",
    "# Find columns that are not in the defined features\n",
    "undefined_columns = all_columns - defined_features\n",
    "\n",
    "print(\"Columns not in num_features or cat_features:\", undefined_columns)\n",
    "print(\"Count of difference:\", len(undefined_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292e2dad-9b48-4cb7-be2c-9f585e3e3b57",
   "metadata": {},
   "source": [
    "# Run CV Score Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7a8d980-33d2-4014-9aa8-8bac0ec786f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Age at Injury\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Accident Month\n",
      "no missing values to input on Accident DayOfWeek\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_7256/3071127135.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_val['Carrier Claim Category'].fillna(0, inplace = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   16.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done RF\n",
      "Done XGB\n",
      "Done KNN\n",
      "Iteration 1, loss = 1.05985956\n",
      "Iteration 2, loss = 1.00534335\n",
      "Iteration 3, loss = 1.00518388\n",
      "Iteration 4, loss = 1.00510129\n",
      "Iteration 5, loss = 1.00504309\n",
      "Iteration 6, loss = 1.00499643\n",
      "Iteration 7, loss = 1.00495634\n",
      "Iteration 8, loss = 1.00492066\n",
      "Iteration 9, loss = 1.00488808\n",
      "Iteration 10, loss = 1.00485795\n",
      "Iteration 11, loss = 1.00482974\n",
      "Iteration 12, loss = 1.00480315\n",
      "Iteration 13, loss = 1.00477790\n",
      "Iteration 14, loss = 1.00475380\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Done MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    2.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validation predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  755  1119   162   282   119    42     7     9]\n",
      " [ 4099 31644  6721 12503  2950   157    22   119]\n",
      " [  639  5323  1824  3781  1589   484    59    83]\n",
      " [ 1104  8313  3421  8354  4440  3231   434   404]\n",
      " [  336  1234   989  1626  3653  1619   172    27]\n",
      " [   30   182   130   213    78   155    34    20]\n",
      " [    0     5     3     7     2     0     2     1]\n",
      " [    3    30     3    28     4     3     3    20]]\n",
      "[[  947  1279     3   146   109    11     0     0]\n",
      " [  508 54257    18  2244  1121    63     0     4]\n",
      " [   48  9047    90  2652  1684   258     0     3]\n",
      " [   32 11468    92  9059  6518  2504     0    28]\n",
      " [    7   786    18  1256  7108   480     0     1]\n",
      " [    0    70     3   371   180   218     0     0]\n",
      " [    0     1     0    13     2     4     0     0]\n",
      " [    0    24     0    61     5     2     0     2]]\n",
      "[[ 1116  1118     7   112   132     9     0     1]\n",
      " [  656 51993    38  2418  2860   194     2    54]\n",
      " [   65  8589    78  2538  2136   323     9    44]\n",
      " [   66  9988   154  9311  7820  2076   129   157]\n",
      " [   13   706    32  1025  7086   765    24     5]\n",
      " [    2    72     9   379   220   145    12     3]\n",
      " [    0     2     0    10     1     6     0     1]\n",
      " [    1    27     0    36     4     1     0    25]]\n",
      "[[  721  1432    12   193   137     0     0     0]\n",
      " [  472 53877   112  2651  1102     1     0     0]\n",
      " [   23  8793   224  3177  1564     1     0     0]\n",
      " [   21 12369   492 11162  5644    13     0     0]\n",
      " [    6   831    82  3744  4991     2     0     0]\n",
      " [    0    43    15   487   296     1     0     0]\n",
      " [    0     0     3    15     2     0     0     0]\n",
      " [    0     9     0    67    18     0     0     0]]\n",
      "[[  285  1709     0   336   165     0     0     0]\n",
      " [  439 53542     3  3227  1004     0     0     0]\n",
      " [   41  8775     2  3932  1032     0     0     0]\n",
      " [  317 11854    23 15206  2301     0     0     0]\n",
      " [   10   853     1  5290  3502     0     0     0]\n",
      " [    1     9     0   750    82     0     0     0]\n",
      " [    0     0     0    19     1     0     0     0]\n",
      " [    0    12     0    53    29     0     0     0]]\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Age at Injury\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Accident Month\n",
      "no missing values to input on Accident Day\n",
      "no missing values to input on Accident DayOfWeek\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_7256/3071127135.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_val['Carrier Claim Category'].fillna(0, inplace = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   16.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done RF\n",
      "Done XGB\n",
      "Done KNN\n",
      "Iteration 1, loss = 1.05935143\n",
      "Iteration 2, loss = 1.00665848\n",
      "Iteration 3, loss = 1.00621494\n",
      "Iteration 4, loss = 1.00606780\n",
      "Iteration 5, loss = 1.00598919\n",
      "Iteration 6, loss = 1.00593631\n",
      "Iteration 7, loss = 1.00589561\n",
      "Iteration 8, loss = 1.00586175\n",
      "Iteration 9, loss = 1.00583207\n",
      "Iteration 10, loss = 1.00580529\n",
      "Iteration 11, loss = 1.00578066\n",
      "Iteration 12, loss = 1.00575755\n",
      "Iteration 13, loss = 1.00573574\n",
      "Iteration 14, loss = 1.00571503\n",
      "Iteration 15, loss = 1.00569525\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Done MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    2.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validation predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  869  1039   217   258    93    14     0     5]\n",
      " [ 1812 38029  8557  8050  1602   108    12    45]\n",
      " [  286  5488  2394  3436  1698   406     8    65]\n",
      " [  669  5704  5806  9267  5692  2189    68   306]\n",
      " [  119   643  2150  2193  3445  1022    32    52]\n",
      " [   31    89   251   280   145    18     0    29]\n",
      " [    1     2     5     8     3     0     0     0]\n",
      " [    4    31    15    28     5     1     0    10]]\n",
      "[[ 1089  1066    57   179    99     5     0     0]\n",
      " [  463 52313  1571  2728  1123    17     0     0]\n",
      " [   13  7424   635  3615  2012    82     0     0]\n",
      " [   13  6938  1737 14117  6158   737     0     1]\n",
      " [    3   451   481  3425  5195   101     0     0]\n",
      " [    0    29    70   674    69     1     0     0]\n",
      " [    0     1     1    16     1     0     0     0]\n",
      " [    0    15     4    75     0     0     0     0]]\n",
      "[[ 1242   925   121   106    84    13     0     4]\n",
      " [  568 47350  6128  2640  1390    89     0    50]\n",
      " [   36  7290   817  3033  2216   356     0    33]\n",
      " [   50  6154  5058  8213  7395  2704     0   127]\n",
      " [    8   422  1564  2051  4956   653     1     1]\n",
      " [    0    38   205   509    56    35     0     0]\n",
      " [    0     1     1    16     1     0     0     0]\n",
      " [    1    20     9    52     0     0     0    12]]\n",
      "[[ 1040  1137    34   197    87     0     0     0]\n",
      " [  465 53977   307  2303  1163     0     0     0]\n",
      " [   14  8389   450  3559  1369     0     0     0]\n",
      " [   24 11654  1053 13685  3285     0     0     0]\n",
      " [    9   778   234  4888  3747     0     0     0]\n",
      " [    0    59    47   594   143     0     0     0]\n",
      " [    0     1     2    13     3     0     0     0]\n",
      " [    0    12     3    75     4     0     0     0]]\n",
      "[[    0  1959     0   471    65     0     0     0]\n",
      " [    0 54879     0  3044   292     0     0     0]\n",
      " [    0  9106     0  4311   364     0     0     0]\n",
      " [    0 14059     0 15028   614     0     0     0]\n",
      " [    0  2185     0  6096  1375     0     0     0]\n",
      " [    0   127     0   688    28     0     0     0]\n",
      " [    0     6     0    11     2     0     0     0]\n",
      " [    0    25     0    62     7     0     0     0]]\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Age at Injury\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Accident Month\n",
      "no missing values to input on Accident DayOfWeek\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_7256/3071127135.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_val['Carrier Claim Category'].fillna(0, inplace = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   15.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done RF\n",
      "Done XGB\n",
      "Done KNN\n",
      "Iteration 1, loss = 1.06556965\n",
      "Iteration 2, loss = 1.00921755\n",
      "Iteration 3, loss = 1.00880697\n",
      "Iteration 4, loss = 1.00863612\n",
      "Iteration 5, loss = 1.00853304\n",
      "Iteration 6, loss = 1.00845917\n",
      "Iteration 7, loss = 1.00840068\n",
      "Iteration 8, loss = 1.00835176\n",
      "Iteration 9, loss = 1.00830942\n",
      "Iteration 10, loss = 1.00827169\n",
      "Iteration 11, loss = 1.00823751\n",
      "Iteration 12, loss = 1.00820615\n",
      "Iteration 13, loss = 1.00817700\n",
      "Iteration 14, loss = 1.00814965\n",
      "Iteration 15, loss = 1.00812388\n",
      "Iteration 16, loss = 1.00809942\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Done MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    2.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validation predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  965   895   280   210   119    23     0     4]\n",
      " [ 1679 36479 10708  7101  2045   168     7    28]\n",
      " [  297  5522  2328  2889  2346   363    11    25]\n",
      " [  673  5708  6347  6154  7456  3068    67   228]\n",
      " [  110  1053  3078  2709  2105   572    11    18]\n",
      " [   30    61   178   430   122    16     0     5]\n",
      " [    1     2     2     7     7     0     0     0]\n",
      " [    4    17    24    39     6     2     0     2]]\n",
      "[[ 1088  1050    85   127   141     5     0     0]\n",
      " [  411 50763  2981  2670  1375    15     0     0]\n",
      " [   12  7618   612  3110  2364    65     0     0]\n",
      " [   22  6999  3549 11146  7320   664     0     1]\n",
      " [    4   519  1370  4099  3631    33     0     0]\n",
      " [    0    23    11   745    63     0     0     0]\n",
      " [    0     2     0    13     4     0     0     0]\n",
      " [    0    10     5    78     1     0     0     0]]\n",
      "[[ 1153   945   190    60   138     7     0     3]\n",
      " [  436 42907 10106  2667  2034    38     0    27]\n",
      " [   35  7235  1151  2310  2899   140     0    11]\n",
      " [   74  6362  7753  4719  9545  1143     0   105]\n",
      " [    5   449  2534  3539  3050    79     0     0]\n",
      " [    0    21    29   729    63     0     0     0]\n",
      " [    0     1     1    13     4     0     0     0]\n",
      " [    0    17     7    67     0     0     0     3]]\n",
      "[[ 1059  1113    35   209    80     0     0     0]\n",
      " [  455 53672   439  2313  1336     0     0     0]\n",
      " [   13  8609   450  3508  1201     0     0     0]\n",
      " [   31 11524  1087 14049  3010     0     0     0]\n",
      " [    6   862   274  4924  3590     0     0     0]\n",
      " [    0    38    22   656   126     0     0     0]\n",
      " [    0     4     1     9     5     0     0     0]\n",
      " [    0    16     4    63    11     0     0     0]]\n",
      "[[    0  1811     0   629    56     0     0     0]\n",
      " [    0 54375     0  3472   366     2     0     0]\n",
      " [    0  9300     0  4180   301     0     0     0]\n",
      " [    1 13000     0 16285   415     0     0     0]\n",
      " [    0  1822     0  6461  1373     0     0     0]\n",
      " [    0    49     0   762    31     0     0     0]\n",
      " [    0     7     0    11     1     0     0     0]\n",
      " [    0    22     0    67     5     0     0     0]]\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Age at Injury\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Accident Month\n",
      "no missing values to input on Accident Day\n",
      "no missing values to input on Accident DayOfWeek\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_7256/3071127135.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_val['Carrier Claim Category'].fillna(0, inplace = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   16.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done RF\n",
      "Done XGB\n",
      "Done KNN\n",
      "Iteration 1, loss = 1.05383962\n",
      "Iteration 2, loss = 1.00203863\n",
      "Iteration 3, loss = 1.00135433\n",
      "Iteration 4, loss = 1.00110877\n",
      "Iteration 5, loss = 1.00097982\n",
      "Iteration 6, loss = 1.00089564\n",
      "Iteration 7, loss = 1.00083428\n",
      "Iteration 8, loss = 1.00078611\n",
      "Iteration 9, loss = 1.00074590\n",
      "Iteration 10, loss = 1.00071126\n",
      "Iteration 11, loss = 1.00068047\n",
      "Iteration 12, loss = 1.00065252\n",
      "Iteration 13, loss = 1.00062681\n",
      "Iteration 14, loss = 1.00060289\n",
      "Iteration 15, loss = 1.00058044\n",
      "Iteration 16, loss = 1.00055918\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Done MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    2.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validation predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  981   969   205   191   122    27     0     1]\n",
      " [ 1556 38300  8757  6344  3021   202     0    34]\n",
      " [  221  5143  2145  3009  2630   610     1    22]\n",
      " [  434  5169  5869  7536  7498  3075    13   108]\n",
      " [   91  1033  3101  3032  1942   447     3     7]\n",
      " [    7   103   203   401   112    13     0     3]\n",
      " [    1     1     4     8     4     0     0     1]\n",
      " [    0    22    21    34     6     6     0     5]]\n",
      "[[ 1181   991    69   113   137     5     0     0]\n",
      " [  369 52947  1345  1814  1733     6     0     0]\n",
      " [   10  7361   575  2688  3124    23     0     0]\n",
      " [   12  7119  3362  9702  9286   221     0     0]\n",
      " [    4   590  1835  4968  2250     9     0     0]\n",
      " [    0    16    21   750    55     0     0     0]\n",
      " [    0     0     0    18     1     0     0     0]\n",
      " [    0    16     8    68     2     0     0     0]]\n",
      "[[ 1297   846   140    71   128    11     0     3]\n",
      " [  458 46728  6635  1305  3024    43     0    21]\n",
      " [   20  6962   876  2083  3671   153     0    16]\n",
      " [   34  5947  6645  5439 10569   981     0    87]\n",
      " [    6   449  2462  4329  2369    39     0     2]\n",
      " [    0    12    44   736    50     0     0     0]\n",
      " [    0     1     1    15     2     0     0     0]\n",
      " [    0    19    12    57     1     0     0     5]]\n",
      "[[ 1160   964    39   194   139     0     0     0]\n",
      " [  448 53771   249  2146  1600     0     0     0]\n",
      " [   11  8154   276  3240  2100     0     0     0]\n",
      " [   57 11561   873 12178  5033     0     0     0]\n",
      " [   11   973   372  5575  2725     0     0     0]\n",
      " [    0    37    34   671   100     0     0     0]\n",
      " [    0     0     0    16     3     0     0     0]\n",
      " [    0    10     2    71    11     0     0     0]]\n",
      "[[    5  1970    79   429    13     0     0     0]\n",
      " [    6 54131   346  3633    98     0     0     0]\n",
      " [    1  8724   132  4786   138     0     0     0]\n",
      " [    0 12125   410 16899   268     0     0     0]\n",
      " [    0  2354   208  6689   405     0     0     0]\n",
      " [    0    21     4   795    22     0     0     0]\n",
      " [    0     1     2    15     1     0     0     0]\n",
      " [    0    12    12    67     3     0     0     0]]\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Age at Injury\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Accident Month\n",
      "no missing values to input on Accident Day\n",
      "no missing values to input on Accident DayOfWeek\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_7256/3071127135.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_val['Carrier Claim Category'].fillna(0, inplace = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   16.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done RF\n",
      "Done XGB\n",
      "Done KNN\n",
      "Iteration 1, loss = 1.05477512\n",
      "Iteration 2, loss = 1.00191961\n",
      "Iteration 3, loss = 1.00165613\n",
      "Iteration 4, loss = 1.00151744\n",
      "Iteration 5, loss = 1.00142494\n",
      "Iteration 6, loss = 1.00135627\n",
      "Iteration 7, loss = 1.00130165\n",
      "Iteration 8, loss = 1.00125651\n",
      "Iteration 9, loss = 1.00121779\n",
      "Iteration 10, loss = 1.00118369\n",
      "Iteration 11, loss = 1.00115312\n",
      "Iteration 12, loss = 1.00112517\n",
      "Iteration 13, loss = 1.00109951\n",
      "Iteration 14, loss = 1.00107574\n",
      "Iteration 15, loss = 1.00105341\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Done MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    2.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validation predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  916   608   745   200    24     0     0     2]\n",
      " [  678 11266 41990  3868   398     9     0     5]\n",
      " [   98  1724 10215  1407   326     6     0     5]\n",
      " [  173  3291 21353  4041   824    17     0     3]\n",
      " [   64  1173  4549  3089   772     8     0     1]\n",
      " [    7   101   336   333    62     3     0     0]\n",
      " [    0     1     6    10     3     0     0     0]\n",
      " [    3    11    34    40     5     0     0     1]]\n",
      "[[ 1077   595   746    70     7     0     0     0]\n",
      " [  154  8022 49553   434    51     0     0     0]\n",
      " [    3   465 13042   243    28     0     0     0]\n",
      " [    5  1189 27182  1297    29     0     0     0]\n",
      " [    1   388  5207  3603   457     0     0     0]\n",
      " [    0    12   213   588    29     0     0     0]\n",
      " [    0     1     3    14     2     0     0     0]\n",
      " [    0     9    37    48     0     0     0     0]]\n",
      "[[ 1087   429   932    37    10     0     0     0]\n",
      " [  218 11613 46052   264    66     0     0     1]\n",
      " [    2   836 12721   171    51     0     0     0]\n",
      " [    8  1416 27235   945    98     0     0     0]\n",
      " [    3   375  5435  3257   586     0     0     0]\n",
      " [    0    12   249   541    40     0     0     0]\n",
      " [    0     0     6    13     1     0     0     0]\n",
      " [    1    12    38    43     0     0     0     0]]\n",
      "[[  990  1101    80   291    33     0     0     0]\n",
      " [  219 53058   831  3375   731     0     0     0]\n",
      " [    5  8260   829  4087   600     0     0     0]\n",
      " [   16 11516  2124 15393   653     0     0     0]\n",
      " [    5  1134   645  5992  1880     0     0     0]\n",
      " [    0    33    61   708    40     0     0     0]\n",
      " [    0     1     1    14     4     0     0     0]\n",
      " [    0     5     5    79     5     0     0     0]]\n",
      "[[   17  1695     9   707    67     0     0     0]\n",
      " [   17 52752    50  5170   225     0     0     0]\n",
      " [    0  8341    15  5354    71     0     0     0]\n",
      " [   12 11622    37 17759   272     0     0     0]\n",
      " [    0  1359    11  8101   185     0     0     0]\n",
      " [    0    18     2   816     6     0     0     0]\n",
      " [    0     2     0    17     1     0     0     0]\n",
      " [    0     6     0    81     7     0     0     0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "results = cv_scores(X, y, num_features, cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4ffce66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train_precision</th>\n",
       "      <th>Test_precision</th>\n",
       "      <th>Train_recall</th>\n",
       "      <th>Test_recall</th>\n",
       "      <th>Train_f1_score</th>\n",
       "      <th>Test_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.214476</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.225414</td>\n",
       "      <td>0.999970</td>\n",
       "      <td>0.197971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.999964</td>\n",
       "      <td>0.314325</td>\n",
       "      <td>0.999965</td>\n",
       "      <td>0.268240</td>\n",
       "      <td>0.999965</td>\n",
       "      <td>0.252435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.795557</td>\n",
       "      <td>0.290976</td>\n",
       "      <td>0.579948</td>\n",
       "      <td>0.261711</td>\n",
       "      <td>0.622400</td>\n",
       "      <td>0.240904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNearestNeighbors</th>\n",
       "      <td>0.382109</td>\n",
       "      <td>0.311092</td>\n",
       "      <td>0.302306</td>\n",
       "      <td>0.268894</td>\n",
       "      <td>0.314234</td>\n",
       "      <td>0.272702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi Layer Perceptron</th>\n",
       "      <td>0.250839</td>\n",
       "      <td>0.235134</td>\n",
       "      <td>0.203970</td>\n",
       "      <td>0.205196</td>\n",
       "      <td>0.189973</td>\n",
       "      <td>0.192440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Train_precision  Test_precision  Train_recall  \\\n",
       "Decision Tree                  0.999950        0.214476      0.999990   \n",
       "Random Forest                  0.999964        0.314325      0.999965   \n",
       "XGBoost                        0.795557        0.290976      0.579948   \n",
       "KNearestNeighbors              0.382109        0.311092      0.302306   \n",
       "Multi Layer Perceptron         0.250839        0.235134      0.203970   \n",
       "\n",
       "                        Test_recall  Train_f1_score  Test_f1_score  \n",
       "Decision Tree              0.225414        0.999970       0.197971  \n",
       "Random Forest              0.268240        0.999965       0.252435  \n",
       "XGBoost                    0.261711        0.622400       0.240904  \n",
       "KNearestNeighbors          0.268894        0.314234       0.272702  \n",
       "Multi Layer Perceptron     0.205196        0.189973       0.192440  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the results\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fe6f74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_7256/3389366436.py:3: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data= pd.read_csv(\"../../data/train_data_enriched.csv\", index_col=\"Claim Identifier\")\n"
     ]
    }
   ],
   "source": [
    "# Reset all models, datasets and global Parameters for fresh retraining\n",
    "DT , RF , XGB , KNN , MLP = model_load()\n",
    "data, data_test,le, num_features, cat_features, X, y, y2 = data_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfbe2252-c3ff-4d5d-b2f9-29ea2c79d61c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Age at Injury\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Accident Month\n",
      "no missing values to input on Accident Day\n",
      "no missing values to input on Accident DayOfWeek\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_7256/3071127135.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_val['Carrier Claim Category'].fillna(0, inplace = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   12.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done RF\n",
      "Done XGB\n",
      "Done KNN\n",
      "Iteration 1, loss = 0.15437115\n",
      "Iteration 2, loss = 0.14084349\n",
      "Iteration 3, loss = 0.14083148\n",
      "Iteration 4, loss = 0.14082248\n",
      "Iteration 5, loss = 0.14081495\n",
      "Iteration 6, loss = 0.14080834\n",
      "Iteration 7, loss = 0.14080237\n",
      "Iteration 8, loss = 0.14079689\n",
      "Iteration 9, loss = 0.14079178\n",
      "Iteration 10, loss = 0.14078701\n",
      "Iteration 11, loss = 0.14078249\n",
      "Iteration 12, loss = 0.14077819\n",
      "Iteration 13, loss = 0.14077409\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Done MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validation predictions\n",
      "[[61352 48095]\n",
      " [ 2058  3300]]\n",
      "[[95777 13670]\n",
      " [ 2671  2687]]\n",
      "[[77892 31555]\n",
      " [ 2154  3204]]\n",
      "[[107491   1956]\n",
      " [  4704    654]]\n",
      "[[108493    954]\n",
      " [  4852    506]]\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Age at Injury\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Accident Month\n",
      "no missing values to input on Accident Day\n",
      "no missing values to input on Accident DayOfWeek\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_7256/3071127135.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_val['Carrier Claim Category'].fillna(0, inplace = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   12.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done RF\n",
      "Done XGB\n",
      "Done KNN\n",
      "Iteration 1, loss = 0.15906783\n",
      "Iteration 2, loss = 0.14157197\n",
      "Iteration 3, loss = 0.14155674\n",
      "Iteration 4, loss = 0.14154543\n",
      "Iteration 5, loss = 0.14153598\n",
      "Iteration 6, loss = 0.14152772\n",
      "Iteration 7, loss = 0.14152029\n",
      "Iteration 8, loss = 0.14151349\n",
      "Iteration 9, loss = 0.14150717\n",
      "Iteration 10, loss = 0.14150125\n",
      "Iteration 11, loss = 0.14149566\n",
      "Iteration 12, loss = 0.14149036\n",
      "Iteration 13, loss = 0.14148531\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Done MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validation predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[73390 36057]\n",
      " [ 3686  1671]]\n",
      "[[96835 12612]\n",
      " [ 4420   937]]\n",
      "[[73199 36248]\n",
      " [ 4057  1300]]\n",
      "[[109289    158]\n",
      " [  5275     82]]\n",
      "[[109430     17]\n",
      " [  5323     34]]\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Age at Injury\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Accident Month\n",
      "no missing values to input on Accident Day\n",
      "no missing values to input on Accident DayOfWeek\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_7256/3071127135.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_val['Carrier Claim Category'].fillna(0, inplace = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   12.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done RF\n",
      "Done XGB\n",
      "Done KNN\n",
      "Iteration 1, loss = 0.15609047\n",
      "Iteration 2, loss = 0.14276226\n",
      "Iteration 3, loss = 0.14274032\n",
      "Iteration 4, loss = 0.14272499\n",
      "Iteration 5, loss = 0.14271271\n",
      "Iteration 6, loss = 0.14270232\n",
      "Iteration 7, loss = 0.14269323\n",
      "Iteration 8, loss = 0.14268510\n",
      "Iteration 9, loss = 0.14267772\n",
      "Iteration 10, loss = 0.14267094\n",
      "Iteration 11, loss = 0.14266465\n",
      "Iteration 12, loss = 0.14265877\n",
      "Iteration 13, loss = 0.14265325\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Done MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validation predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[70144 39303]\n",
      " [ 4314  1043]]\n",
      "[[92250 17197]\n",
      " [ 4728   629]]\n",
      "[[60942 48505]\n",
      " [ 4668   689]]\n",
      "[[109334    113]\n",
      " [  5257    100]]\n",
      "[[109446      1]\n",
      " [  5355      2]]\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Age at Injury\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Accident Month\n",
      "no missing values to input on Accident Day\n",
      "no missing values to input on Accident DayOfWeek\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_7256/3071127135.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_val['Carrier Claim Category'].fillna(0, inplace = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   12.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done RF\n",
      "Done XGB\n",
      "Done KNN\n",
      "Iteration 1, loss = 0.15919880\n",
      "Iteration 2, loss = 0.14469379\n",
      "Iteration 3, loss = 0.14467638\n",
      "Iteration 4, loss = 0.14466374\n",
      "Iteration 5, loss = 0.14465332\n",
      "Iteration 6, loss = 0.14464429\n",
      "Iteration 7, loss = 0.14463622\n",
      "Iteration 8, loss = 0.14462888\n",
      "Iteration 9, loss = 0.14462209\n",
      "Iteration 10, loss = 0.14461579\n",
      "Iteration 11, loss = 0.14460987\n",
      "Iteration 12, loss = 0.14460427\n",
      "Iteration 13, loss = 0.14459893\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Done MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validation predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[71441 38006]\n",
      " [ 4424   933]]\n",
      "[[84973 24474]\n",
      " [ 4857   500]]\n",
      "[[62846 46601]\n",
      " [ 4462   895]]\n",
      "[[109095    352]\n",
      " [  5228    129]]\n",
      "[[109445      2]\n",
      " [  5355      2]]\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Age at Injury\n",
      "no missing values to input on Number of Dependents\n",
      "no missing values to input on Accident Month\n",
      "no missing values to input on Accident Day\n",
      "no missing values to input on Accident DayOfWeek\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_7256/3071127135.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_val['Carrier Claim Category'].fillna(0, inplace = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   12.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done RF\n",
      "Done XGB\n",
      "Done KNN\n",
      "Iteration 1, loss = 0.15407354\n",
      "Iteration 2, loss = 0.14019931\n",
      "Iteration 3, loss = 0.14018852\n",
      "Iteration 4, loss = 0.14018048\n",
      "Iteration 5, loss = 0.14017375\n",
      "Iteration 6, loss = 0.14016783\n",
      "Iteration 7, loss = 0.14016250\n",
      "Iteration 8, loss = 0.14015761\n",
      "Iteration 9, loss = 0.14015306\n",
      "Iteration 10, loss = 0.14014880\n",
      "Iteration 11, loss = 0.14014477\n",
      "Iteration 12, loss = 0.14014093\n",
      "Iteration 13, loss = 0.14013728\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Done MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done validation predictions\n",
      "[[109033    413]\n",
      " [  4883    475]]\n",
      "[[109434     12]\n",
      " [  5026    332]]\n",
      "[[109426     20]\n",
      " [  4994    364]]\n",
      "[[109429     17]\n",
      " [  5211    147]]\n",
      "[[109394     52]\n",
      " [  5206    152]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "results_secondary_target = cv_scores(X, y2, num_features, cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f8d3cfc-dff8-42ef-aea3-8bea0a2d53ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train_precision</th>\n",
       "      <th>Test_precision</th>\n",
       "      <th>Train_recall</th>\n",
       "      <th>Test_recall</th>\n",
       "      <th>Train_f1_score</th>\n",
       "      <th>Test_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.545383</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490633</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.444067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.603634</td>\n",
       "      <td>0.999897</td>\n",
       "      <td>0.532813</td>\n",
       "      <td>0.999946</td>\n",
       "      <td>0.511495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.907491</td>\n",
       "      <td>0.584706</td>\n",
       "      <td>0.637352</td>\n",
       "      <td>0.471562</td>\n",
       "      <td>0.698709</td>\n",
       "      <td>0.439744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNearestNeighbors</th>\n",
       "      <td>0.895985</td>\n",
       "      <td>0.700122</td>\n",
       "      <td>0.536993</td>\n",
       "      <td>0.518383</td>\n",
       "      <td>0.557173</td>\n",
       "      <td>0.520026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi Layer Perceptron</th>\n",
       "      <td>0.576674</td>\n",
       "      <td>0.769709</td>\n",
       "      <td>0.500005</td>\n",
       "      <td>0.512053</td>\n",
       "      <td>0.488068</td>\n",
       "      <td>0.509603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Train_precision  Test_precision  Train_recall  \\\n",
       "Decision Tree                  1.000000        0.545383      1.000000   \n",
       "Random Forest                  0.999995        0.603634      0.999897   \n",
       "XGBoost                        0.907491        0.584706      0.637352   \n",
       "KNearestNeighbors              0.895985        0.700122      0.536993   \n",
       "Multi Layer Perceptron         0.576674        0.769709      0.500005   \n",
       "\n",
       "                        Test_recall  Train_f1_score  Test_f1_score  \n",
       "Decision Tree              0.490633        1.000000       0.444067  \n",
       "Random Forest              0.532813        0.999946       0.511495  \n",
       "XGBoost                    0.471562        0.698709       0.439744  \n",
       "KNearestNeighbors          0.518383        0.557173       0.520026  \n",
       "Multi Layer Perceptron     0.512053        0.488068       0.509603  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the results\n",
    "results_secondary_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479c6461-4660-46f6-9a61-481d5c652a40",
   "metadata": {},
   "source": [
    "# Run Test Predictions with best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247e2b02-b4e7-4d25-a5f8-2c4ed317e641",
   "metadata": {},
   "source": [
    "## 1. Main Target Variable Only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1106b3d2-3154-43f4-81f8-d49a6cfe172c",
   "metadata": {},
   "source": [
    "### Best model for Claim Injury Type target test prediction: **XGBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77969de7-0da9-4ffd-81c7-18e64394dcd1",
   "metadata": {},
   "source": [
    "#### V1: XGBoost with imputation of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "113e0c18-e051-4ec8-98a9-b038ad0a8d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_7256/3389366436.py:3: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data= pd.read_csv(\"../../data/train_data_enriched.csv\", index_col=\"Claim Identifier\")\n"
     ]
    }
   ],
   "source": [
    "# Reset all models, datasets and global Parameters for fresh retraining\n",
    "DT , RF , XGB , KNN , MLP = model_load()\n",
    "data, data_test,le, num_features, cat_features, X, y, y2 = data_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "923c21bd-9847-47e1-91ff-70b453b3b311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'submission_without_missing = test_prediction(\\n    XGBClassifier(),\\n    X,y,\\n    num_features,cat_features,\\n    data_test,\\n    scaling_outlier= True)\\nsubmission_without_missing'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"submission_without_missing = test_prediction(\n",
    "    XGBClassifier(),\n",
    "    X,y,\n",
    "    num_features,cat_features,\n",
    "    data_test,\n",
    "    scaling_outlier= True)\n",
    "submission_without_missing\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe86d483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'submission_without_missing.to_csv(\"submission_without_missing.csv\")'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export to csv\n",
    "\"\"\"submission_without_missing.to_csv(\"submission_without_missing.csv\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a63471c-235f-4bb5-9e2d-d43ac111766a",
   "metadata": {},
   "source": [
    "#### V2: XGBoost without imputation of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb13dd0f-959b-437c-9793-0169c57ac6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_7256/3389366436.py:3: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data= pd.read_csv(\"../../data/train_data_enriched.csv\", index_col=\"Claim Identifier\")\n"
     ]
    }
   ],
   "source": [
    "# Reset all models, datasets and global Parameters for fresh retraining\n",
    "DT , RF , XGB , KNN , MLP = model_load()\n",
    "data, data_test,le, num_features, cat_features, X, y, y2 = data_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "192cf314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_7256/3071127135.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_val['Carrier Claim Category'].fillna(0, inplace = True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claim Injury Type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claim Identifier</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6165911</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166141</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6165907</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166047</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166102</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553137</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553119</th>\n",
       "      <td>1. CANCELLED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553542</th>\n",
       "      <td>1. CANCELLED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553455</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553594</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>387975 rows Ã 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Claim Injury Type\n",
       "Claim Identifier                  \n",
       "6165911                2. NON-COMP\n",
       "6166141                2. NON-COMP\n",
       "6165907                2. NON-COMP\n",
       "6166047                2. NON-COMP\n",
       "6166102                2. NON-COMP\n",
       "...                            ...\n",
       "6553137                2. NON-COMP\n",
       "6553119               1. CANCELLED\n",
       "6553542               1. CANCELLED\n",
       "6553455                2. NON-COMP\n",
       "6553594                2. NON-COMP\n",
       "\n",
       "[387975 rows x 1 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using xgboostÂ´s way of dealing with missing values (better performance)\n",
    "submission_with_missing = test_prediction(\n",
    "    XGBClassifier(),\n",
    "    X,y,\n",
    "    num_features,cat_features,\n",
    "    data_test, \n",
    "    missing= False)\n",
    "submission_with_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7985054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to csv\n",
    "submission_with_missing.to_csv(\"../../data/submission_with_missing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f67a4c-851d-46cc-98ca-71415f23befd",
   "metadata": {},
   "source": [
    "#### F1 MACRO SCORE KAGGLE: **0.44011**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c35717-7aa8-4a0b-b19f-2fe66051de49",
   "metadata": {},
   "source": [
    "## 2. Secondary Target Variable AND Main Target Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8720825-7090-4cd9-b652-a62daeba82f5",
   "metadata": {},
   "source": [
    "### Best model prediction of secondary target var: KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fcd304ef-8872-47f2-a878-94a4b28c9ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_7256/3389366436.py:3: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data= pd.read_csv(\"../../data/train_data_enriched.csv\", index_col=\"Claim Identifier\")\n"
     ]
    }
   ],
   "source": [
    "# Reset all models, datasets and global Parameters for fresh retraining\n",
    "DT , RF , XGB , KNN , MLP = model_load()\n",
    "data, data_test,le, num_features, cat_features, X, y, y2 = data_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a58fbfa-0a3a-458d-ae43-8a1ccd7fd8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_7256/3071127135.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_val['Carrier Claim Category'].fillna(0, inplace = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no missing values to input on Number of Dependents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claim Injury Type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claim Identifier</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6165911</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166141</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6165907</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166047</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166102</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553137</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553119</th>\n",
       "      <td>1. CANCELLED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553542</th>\n",
       "      <td>1. CANCELLED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553455</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553594</th>\n",
       "      <td>1. CANCELLED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>387975 rows Ã 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Claim Injury Type\n",
       "Claim Identifier                  \n",
       "6165911                2. NON-COMP\n",
       "6166141                2. NON-COMP\n",
       "6165907                2. NON-COMP\n",
       "6166047                2. NON-COMP\n",
       "6166102                2. NON-COMP\n",
       "...                            ...\n",
       "6553137                2. NON-COMP\n",
       "6553119               1. CANCELLED\n",
       "6553542               1. CANCELLED\n",
       "6553455                2. NON-COMP\n",
       "6553594               1. CANCELLED\n",
       "\n",
       "[387975 rows x 1 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_with_missing_with_v2 = test_prediction(\n",
    "    XGBClassifier(), # y classifier\n",
    "    X, y,# Dataset without target, target var 1\n",
    "    num_features,\n",
    "    cat_features,\n",
    "    data_test,\n",
    "    missing= False,\n",
    "    secondary_model = KNeighborsClassifier(n_neighbors = 50), # y2 classifier\n",
    "    y_train_secondary = y2, # target var 2\n",
    "    secondary_missing = True\n",
    "    )\n",
    "submission_with_missing_with_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "309d050c-d39c-4fbd-aa16-27cc9bb3cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_with_missing_with_v2.to_csv(\"../../data/submission_with_missing_with_v2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e898c472-a39d-49a8-9e94-bc085872dd42",
   "metadata": {},
   "source": [
    "#### F1 MACRO SCORE KAGGLE: **0.43776**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58323e4f-eafc-4ba7-bad8-30f64945dfaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f84d732-8a31-4afd-8235-3086955a802b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "637fc1a2-00bf-4f83-9c88-2d07e50fa29a",
   "metadata": {},
   "source": [
    "# New attempt to hyperparam tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6825fdcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def cv_scores_hyperparameter_tuning(X, y, num_features, cat_features, num_imputing_algorithm=XGBRegressor(), cat_imputing_algorithm=XGBClassifier(), scaling_outlier= False, scaler=MinMaxScaler(), rfe = False):\\n    \\n    Takes as argument the predictors and the target, the models used for imputing numerical and categorical \\n    features, if any scaling and outlier removal should be performed,what scaling method should be used and if feature selection with rfe should be used.\\n    Then it returns the results obtained from the stratified cross-validation for the given models.\\n    \\n    skf = StratifiedKFold(n_splits=5)\\n\\n    # Generating the lists to store our results\\n    precision_scores_train = []\\n    precision_scores_val = []\\n    recall_scores_train = []\\n    recall_scores_val = []\\n    f1_scores_train =  []\\n    f1_scores_val =  []\\n\\n    precision_scores_train_mean = []\\n    precision_scores_val_mean = [] \\n    recall_scores_train_mean = []\\n    recall_scores_val_mean = []\\n    f1_scores_train_mean =  []\\n    f1_scores_val_mean =  []\\n    \\n    for train_index, test_index in skf.split(X, y):\\n        # Dividing our data in validation and train\\n        X_train, X_val = X.iloc[train_index].copy(), X.iloc[test_index].copy()\\n        y_train, y_val = y.iloc[train_index].copy(), y.iloc[test_index].copy()\\n\\n        # Filling missing values\\n        for column in num_features:\\n            X_train, X_val = impute_missing_values(X_train,X_val, column, num_imputing_algorithm)\\n        \\n        # Removing inconsistencies on the train\\n        inconsistent = X_train[(X_train[\\'Age at Injury\\'] > 80) | (X_train[\"Age at Injury\"] < 16)].index\\n        X_train = X_train.loc[~X_train.index.isin(inconsistent)]\\n        y_train = y_train.loc[~y_train.index.isin(inconsistent)]\\n\\n        # Performing scaling and outlier treatment dependent on the boolean\\n        if scaling_outlier:\\n            for column in num_features:\\n                handle_outliers(X_train, column)\\n                X_train, X_val = impute_missing_values(X_train,X_val, column, num_imputing_algorithm,validation= False)\\n\\n            for column in num_features:\\n                scale_numerical(column, X_train, X_val, scaler)\\n                \\n        # Creating an ordinal variable\\n        claim_carrier_categories(X_train, X_val)\\n\\n        #Filling missing values in the ordinal variable that might appear on X_val\\n        X_val, X_train = impute_missing_values(X_val,X_train, \"Carrier Claim Category\", cat_imputing_algorithm, validation=False)\\n\\n        # Categorical Prop Encoding\\n        for cat_feature in cat_features:\\n            categorical_prop_encode(X_train, X_val, cat_feature)\\n            if scaling_outlier:\\n                scale_numerical(\"Carrier Claim Category\", X_train, X_val, scaler)\\n\\n        # Selecting features with Rfe\\n        if rfe:\\n            Selected_features = Rfe(XGBClassifier(), X_train, y_train, X_val, y_val)\\n            X_train = X_train[Selected_features]\\n            X_val = X_val[Selected_features]\\n\\n        # Training the classification models\\n        XGBT.fit(X_train, y_train)\\n        print(\"Done XGBT\")\\n\\n        # Making the predictions for the training and validation data\\n        pred_train_XGBT = XGBT.predict(X_train)\\n        print(\"Done training predictions\")\\n        \\n        pred_val_XGBT = XGBT.predict(X_val)\\n        print(\"Done validation predictions\")\\n\\n        # Calculating and storing the scores\\n        precision_scores_train.append(precision_score(y_train, pred_train_XGBT, average=\\'macro\\'))\\n        recall_scores_train.append(recall_score(y_train, pred_train_XGBT, average=\\'macro\\'))\\n        f1_scores_train.append(f1_score(y_train, pred_train_XGBT, average=\\'macro\\'))\\n        \\n        precision_scores_val.append(precision_score(y_val, pred_val_XGBT, average=\\'macro\\'))\\n        recall_scores_val.append(recall_score(y_val, pred_val_XGBT, average=\\'macro\\'))\\n        f1_scores_val.append(f1_score(y_val, pred_val_XGBT, average=\\'macro\\'))\\n\\n\\n    # Aggregating the average results across the folds\\n    precision_scores_train_mean.append(mean(precision_scores_train))\\n    precision_scores_val_mean.append(mean(precision_scores_val))\\n    recall_scores_train_mean.append(mean(recall_scores_train))\\n    recall_scores_val_mean.append(mean(recall_scores_val))\\n    f1_scores_train_mean.append(mean(f1_scores_train))\\n    f1_scores_val_mean.append(mean(f1_scores_val))\\n\\n    # Storing the results in a dataframe\\n    scores = {\\'Train_precision\\': precision_scores_train_mean,\\n    \\'Test_precision\\': precision_scores_val_mean,\\n    \\'Train_recall\\': recall_scores_train_mean,\\n    \\'Test_recall\\': recall_scores_val_mean,\\n    \\'Train_f1_score\\': f1_scores_train_mean,\\n    \\'Test_f1_score\\': f1_scores_val_mean}\\n\\n    print(scores)\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def cv_scores_hyperparameter_tuning(X, y, num_features, cat_features, num_imputing_algorithm=XGBRegressor(), cat_imputing_algorithm=XGBClassifier(), scaling_outlier= False, scaler=MinMaxScaler(), rfe = False):\n",
    "    \n",
    "    Takes as argument the predictors and the target, the models used for imputing numerical and categorical \n",
    "    features, if any scaling and outlier removal should be performed,what scaling method should be used and if feature selection with rfe should be used.\n",
    "    Then it returns the results obtained from the stratified cross-validation for the given models.\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # Generating the lists to store our results\n",
    "    precision_scores_train = []\n",
    "    precision_scores_val = []\n",
    "    recall_scores_train = []\n",
    "    recall_scores_val = []\n",
    "    f1_scores_train =  []\n",
    "    f1_scores_val =  []\n",
    "\n",
    "    precision_scores_train_mean = []\n",
    "    precision_scores_val_mean = [] \n",
    "    recall_scores_train_mean = []\n",
    "    recall_scores_val_mean = []\n",
    "    f1_scores_train_mean =  []\n",
    "    f1_scores_val_mean =  []\n",
    "    \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        # Dividing our data in validation and train\n",
    "        X_train, X_val = X.iloc[train_index].copy(), X.iloc[test_index].copy()\n",
    "        y_train, y_val = y.iloc[train_index].copy(), y.iloc[test_index].copy()\n",
    "\n",
    "        # Filling missing values\n",
    "        for column in num_features:\n",
    "            X_train, X_val = impute_missing_values(X_train,X_val, column, num_imputing_algorithm)\n",
    "        \n",
    "        # Removing inconsistencies on the train\n",
    "        inconsistent = X_train[(X_train['Age at Injury'] > 80) | (X_train[\"Age at Injury\"] < 16)].index\n",
    "        X_train = X_train.loc[~X_train.index.isin(inconsistent)]\n",
    "        y_train = y_train.loc[~y_train.index.isin(inconsistent)]\n",
    "\n",
    "        # Performing scaling and outlier treatment dependent on the boolean\n",
    "        if scaling_outlier:\n",
    "            for column in num_features:\n",
    "                handle_outliers(X_train, column)\n",
    "                X_train, X_val = impute_missing_values(X_train,X_val, column, num_imputing_algorithm,validation= False)\n",
    "\n",
    "            for column in num_features:\n",
    "                scale_numerical(column, X_train, X_val, scaler)\n",
    "                \n",
    "        # Creating an ordinal variable\n",
    "        claim_carrier_categories(X_train, X_val)\n",
    "\n",
    "        #Filling missing values in the ordinal variable that might appear on X_val\n",
    "        X_val, X_train = impute_missing_values(X_val,X_train, \"Carrier Claim Category\", cat_imputing_algorithm, validation=False)\n",
    "\n",
    "        # Categorical Prop Encoding\n",
    "        for cat_feature in cat_features:\n",
    "            categorical_prop_encode(X_train, X_val, cat_feature)\n",
    "            if scaling_outlier:\n",
    "                scale_numerical(\"Carrier Claim Category\", X_train, X_val, scaler)\n",
    "\n",
    "        # Selecting features with Rfe\n",
    "        if rfe:\n",
    "            Selected_features = Rfe(XGBClassifier(), X_train, y_train, X_val, y_val)\n",
    "            X_train = X_train[Selected_features]\n",
    "            X_val = X_val[Selected_features]\n",
    "\n",
    "        # Training the classification models\n",
    "        XGBT.fit(X_train, y_train)\n",
    "        print(\"Done XGBT\")\n",
    "\n",
    "        # Making the predictions for the training and validation data\n",
    "        pred_train_XGBT = XGBT.predict(X_train)\n",
    "        print(\"Done training predictions\")\n",
    "        \n",
    "        pred_val_XGBT = XGBT.predict(X_val)\n",
    "        print(\"Done validation predictions\")\n",
    "\n",
    "        # Calculating and storing the scores\n",
    "        precision_scores_train.append(precision_score(y_train, pred_train_XGBT, average='macro'))\n",
    "        recall_scores_train.append(recall_score(y_train, pred_train_XGBT, average='macro'))\n",
    "        f1_scores_train.append(f1_score(y_train, pred_train_XGBT, average='macro'))\n",
    "        \n",
    "        precision_scores_val.append(precision_score(y_val, pred_val_XGBT, average='macro'))\n",
    "        recall_scores_val.append(recall_score(y_val, pred_val_XGBT, average='macro'))\n",
    "        f1_scores_val.append(f1_score(y_val, pred_val_XGBT, average='macro'))\n",
    "\n",
    "\n",
    "    # Aggregating the average results across the folds\n",
    "    precision_scores_train_mean.append(mean(precision_scores_train))\n",
    "    precision_scores_val_mean.append(mean(precision_scores_val))\n",
    "    recall_scores_train_mean.append(mean(recall_scores_train))\n",
    "    recall_scores_val_mean.append(mean(recall_scores_val))\n",
    "    f1_scores_train_mean.append(mean(f1_scores_train))\n",
    "    f1_scores_val_mean.append(mean(f1_scores_val))\n",
    "\n",
    "    # Storing the results in a dataframe\n",
    "    scores = {'Train_precision': precision_scores_train_mean,\n",
    "    'Test_precision': precision_scores_val_mean,\n",
    "    'Train_recall': recall_scores_train_mean,\n",
    "    'Test_recall': recall_scores_val_mean,\n",
    "    'Train_f1_score': f1_scores_train_mean,\n",
    "    'Test_f1_score': f1_scores_val_mean}\n",
    "\n",
    "    print(scores)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b0b6940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import xgboost as xgb\\nfor gama in range(10):\\n    for depth in range(4,8):\\n        XGBT = XGBClassifier(gamma = gama, max_depth = depth)\\n        print(boost, gama, depth)\\n        cv_scores_hyperparameter_tuning(X, y,num_features,cat_features,scaling_outlier = True)\\n        print(\"------------\")'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import xgboost as xgb\n",
    "for gama in range(10):\n",
    "    for depth in range(4,8):\n",
    "        XGBT = XGBClassifier(gamma = gama, max_depth = depth)\n",
    "        print(boost, gama, depth)\n",
    "        cv_scores_hyperparameter_tuning(X, y,num_features,cat_features,scaling_outlier = True)\n",
    "        print(\"------------\")\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

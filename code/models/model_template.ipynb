{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "039090e4-5406-4b1c-b3c9-5b663cc2872e",
   "metadata": {},
   "source": [
    "Template notebook for one type of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "d29f30c5-6ec1-4339-a2af-a73bd36f91c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier,XGBRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from statistics import mean\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Other imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "27ac0de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_5244/1043285265.py:2: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data= pd.read_csv(\"train_data_enriched.csv\", index_col=\"Claim Identifier\")\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data= pd.read_csv(\"train_data_enriched.csv\", index_col=\"Claim Identifier\")\n",
    "data_test = pd.read_csv(\"test_data_enriched.csv\",index_col=\"Claim Identifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "60061974",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3fa916c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label inconding our target variable \n",
    "data[\"Claim Injury Type\"] = le.fit_transform(data[\"Claim Injury Type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "41080786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(data, target_column, algorithm):\n",
    "    \n",
    "    # Separating the missing values from the non missing values\n",
    "    available_data = data[data[target_column].notna()]\n",
    "    missing_data = data[data[target_column].isna()]\n",
    "\n",
    "    # Making sure there is enough data to input \n",
    "    if len(available_data) == 0 or len(missing_data) == 0:\n",
    "        return data\n",
    "\n",
    "    # Separating the target column from the rest \n",
    "    X_available = available_data.drop(columns=[target_column])\n",
    "    y_available = available_data[target_column]\n",
    "\n",
    "    # Training the model with the available data\n",
    "    model = algorithm\n",
    "    model.fit(X_available, y_available)\n",
    "\n",
    "    # Prediting the missing values\n",
    "    X_missing = missing_data.drop(columns=[target_column])\n",
    "    predicted_values = model.predict(X_missing)\n",
    "\n",
    "    # Inputing the missing values with the predictions\n",
    "    data.loc[data[target_column].isna(), target_column] = predicted_values\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "3f20a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers(data, target_column):\n",
    "    lower_quantile = 0.25\n",
    "    upper_quantile = 0.75\n",
    "    multiplier = 1.5\n",
    "    # Calculate Q1, Q3, and IQR\n",
    "    Q1 = data[target_column].quantile(lower_quantile)\n",
    "    Q3 = data[target_column].quantile(upper_quantile)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define outlier bounds\n",
    "    lower_bound = Q1 - multiplier * IQR\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "    \n",
    "    # Replace outliers with NaN\n",
    "    data[target_column] = data[target_column].apply(\n",
    "        lambda x: np.nan if x < lower_bound or x > upper_bound else x\n",
    "    )\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "c9e310a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling function\n",
    "def scale_numerical(column, X_train, X_val, scaler):\n",
    "    # Ajusta a escala da coluna numÃ©rica convertendo-a para 2D\n",
    "    X_train[column] = pd.DataFrame(scaler.fit_transform(X_train[[column]]))\n",
    "    X_val[column] = pd.DataFrame(scaler.transform(X_val[[column]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "92397747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_ordinal_encode(X_train, X_val,column):\n",
    "    # Define a function to categorize each carrier based on its claim count\n",
    "    count = X_train['Carrier Name'].value_counts()\n",
    "    def categorize_claims(count):\n",
    "        if count >= 40000:\n",
    "            return 2\n",
    "        elif 4000 <= count < 40000:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # Apply the categorization to create a mapping dictionary\n",
    "    carrier_category_map = count.apply(categorize_claims)\n",
    "\n",
    "    # Map the `Carrier Name` to the new `Carrier Claim Category`\n",
    "    X_train['Carrier Claim Category'] = X_train['Carrier Name'].map(carrier_category_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "db2cfcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical encoder function\n",
    "def categorical_prop_encode(X_train, X_val, feature):\n",
    "    proportion = X_train[feature].value_counts(normalize = True)  # Get the porportion of each category\n",
    "    X_train[feature] = X_train[feature].map(proportion)  # Map the porportions in the column\n",
    "    X_val[feature] = X_val[feature].map(proportion) # Do the same for the valid dataset\n",
    "    X_val[feature] = X_val[feature].fillna(0)  # Handle categories in X_val not seen in X_train with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc04b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_scores(model, X, y, num_features, cat_features, num_imputing_algorithm= XGBRegressor() , cat_imputing_algorithm = XGBClassifier(), scalling_outlier = True, scaler = MinMaxScaler()):\n",
    "    ''' Takes as argument the model used, the predictors and the target, the models used for imputing numerical and categorical \n",
    "      features, if any scaling and outlier removed should be performed, and what scaling method should be used.\n",
    "     Then it returns the results obtained from the stratified cross validation for the given model'''\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    # Generating the lists to store our results\n",
    "    precision_scores_train = []\n",
    "    precision_scores_val = []   \n",
    "    recall_scores_train = []  \n",
    "    recall_scores_val = []\n",
    "    f1_scores_train = []    \n",
    "    f1_scores_val = []\n",
    "    index = [f'Fold {i}' for i in range(1,6)]\n",
    "    index.append(\"Average\")\n",
    "    \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        # Dividing our data in validation and train\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    " \n",
    "        #Filling num missing values\n",
    "        for column in num_features:\n",
    "            impute_missing_values(X_train, column, num_imputing_algorithm)\n",
    "            impute_missing_values(X_val, column, num_imputing_algorithm)\n",
    "\n",
    "        #Filling cat missing values\n",
    "        impute_missing_values(X_train, \"Alternative Dispute Resolution\", cat_imputing_algorithm)\n",
    "        impute_missing_values(X_val, \"Alternative Dispute Resolution\", cat_imputing_algorithm)\n",
    "\n",
    "\n",
    "        # Removing inconsistencies on the train\n",
    "        inconsistent = X_train[(X_train['Age at Injury'] > 80) | (X_train[\"Age at Injury\"] < 16)].index\n",
    "        X_train = X_train.loc[~X_train.index.isin(inconsistent)]\n",
    "        y_train = y_train.loc[~y_train.index.isin(inconsistent)]\n",
    "\n",
    "        #Performing scaling and outlier treatment dependent on the boolean\n",
    "        if scalling_outlier:\n",
    "            for column in num_features:\n",
    "                handle_outliers(X_train, column)\n",
    "                scale_numerical(column,X_train, X_val, scaler)\n",
    "\n",
    "        \"\"\"# Creating an ordinal variable\n",
    "        categorical_ordinal_encode(X_train, X_val, num_features)\"\"\"\n",
    "\n",
    "\n",
    "        # Categorical Prop Encoding\n",
    "        for cat_feature in cat_features:\n",
    "            categorical_prop_encode(X_train, X_val, cat_feature)\n",
    "            \n",
    "        \n",
    "        # Training the classification model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        \n",
    "        # Making the predictions for the training and validation data\n",
    "        pred_train = model.predict(X_train)\n",
    "        pred_val = model.predict(X_val)\n",
    "    \n",
    "        \n",
    "        # Calculating and storing the scores\n",
    "        precision_scores_train.append(precision_score(y_train, pred_train, average='macro'))\n",
    "        precision_scores_val.append(precision_score(y_val, pred_val, average='macro'))\n",
    "        recall_scores_train.append(recall_score(y_train, pred_train, average='macro'))\n",
    "        recall_scores_val.append(recall_score(y_val, pred_val, average='macro'))\n",
    "        f1_scores_train.append(f1_score(y_train, pred_train, average='macro'))\n",
    "        f1_scores_val.append(f1_score(y_val, pred_val, average='macro'))\n",
    "\n",
    "    \n",
    "    precision_scores_train.append(mean(precision_scores_train))\n",
    "    precision_scores_val.append(mean(precision_scores_val))\n",
    "    recall_scores_train.append(mean(recall_scores_train))\n",
    "    recall_scores_val.append(mean(recall_scores_val))\n",
    "    f1_scores_train.append(mean(f1_scores_train))\n",
    "    f1_scores_val.append(mean(f1_scores_val))\n",
    "\n",
    "\n",
    "    # Storing the results in a dataframe\n",
    "    model_results = pd.DataFrame(data={\n",
    "        'Train_precision': precision_scores_train,\n",
    "        'Test_precision': precision_scores_val,\n",
    "        'Train_recall': recall_scores_train,\n",
    "        'Test_recall': recall_scores_val,\n",
    "        'Train_f1_score': f1_scores_train,\n",
    "        'Test_f1_score': f1_scores_val,\n",
    "    }, index=index)\n",
    "    \n",
    "    return model_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb306304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def test_prediction(model, X, y , test, num_inputing_algorithm= XGBRegressor() , cat_inputing_algorithm = XGBClassifier(),scalling_outlier = True, scaler = MinMaxScaler()):\n",
    "\n",
    "    X_train, X_val,y_train, y_val = train_test_split(X,y,\n",
    "                                                train_size = 0.8, \n",
    "                                                shuffle = True, \n",
    "                                                stratify = y)\n",
    "    #Performing scaling and outlier treatment dependent on the boolean\n",
    "    if scalling_outlier:\n",
    "        for column in num_features:\n",
    "            handle_outliers(X_train, column)\n",
    "            scale_numerical(column,X_train, X_val, scaler)\n",
    "\n",
    "    # Missing value inputation\n",
    "    #Filling num missing values\n",
    "    for column in num_features:\n",
    "        impute_missing_values(X_train, column, num_inputing_algorithm)\n",
    "        impute_missing_values(X_val, column, num_inputing_algorithm)\n",
    "        impute_missing_values(test, column, num_inputing_algorithm)\n",
    "\n",
    "    #Filling cat missing values\n",
    "    impute_missing_values(X_train, \"Alternative Dispute Resolution\", cat_inputing_algorithm)\n",
    "    impute_missing_values(X_val, \"Alternative Dispute Resolution\", cat_inputing_algorithm)\n",
    "    impute_missing_values(test, \"Alternative Dispute Resolution\", cat_inputing_algorithm)\n",
    "\n",
    "    # Removing inconsistencies on the train\n",
    "    inconsistent = X_train[(X_train['Age at Injury'] > 80) | (X_train[\"Age at Injury\"] < 16)].index\n",
    "    X_train.drop(inconsistent, inplace=True)\n",
    "    y_train.drop(inconsistent, inplace=True)\n",
    "\n",
    "    # Creating an ordinal variable\n",
    "    for num_feature in num_features:\n",
    "        categorical_ordinal_encode(X_train, X_val,num_feature)\n",
    "        categorical_ordinal_encode(X_train, test,num_feature)\n",
    "\n",
    "    # Categorical Prop Encoding\n",
    "    for cat_feature in cat_features:\n",
    "        categorical_prop_encode(X_train, X_val, cat_feature)\n",
    "        categorical_prop_encode(X_train, test, cat_feature)\n",
    "    \n",
    "    # Fitting the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Veryfing if the model is performing as expected\n",
    "    pred_val = model.predict(X_val)\n",
    "    print(f1_score(y_val, pred_val, average='macro'))\n",
    "\n",
    "    # Using the model to make prediction on the test dataset\n",
    "    pred_test = model.predict(test)\n",
    "\n",
    "    # Inversing the encoding of our target variable \n",
    "    pred_test = le.inverse_transform(pred_test)\n",
    "\n",
    "    # Making a dataframe with the indexes of data_test and predictions converted back to strings\n",
    "    submission_df = pd.DataFrame({\n",
    "        \"Claim Injury Type\": pred_test\n",
    "    }, index=data_test.index)\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fc3195-03a0-4757-bb36-5bffec585a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e76c9ed5",
   "metadata": {},
   "source": [
    "Temos de rever as num features e cat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "49cc0c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = ['Age at Injury', 'Average Weekly Wage', 'Birth Year', 'IME-4 Count', 'Number of Dependents',\n",
    "                \"Accident Year\",\"Accident Month\",\"Accident Day\",\"Accident DayOfWeek\",\"Assembly Date DSA\",\n",
    "                \"C-2 Date DSA\",\"C-3 Date DSA\",\"First Hearing Date DSA\" ,\"Accident Year\",\n",
    "    \"Accident Month\",\n",
    "    \"Accident Day\",\n",
    "    \"Accident DayOfWeek\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "5bb15763",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\n",
    "    \"Alternative Dispute Resolution\",\n",
    "    \"Carrier Name\",\n",
    "    \"Carrier Type\",\n",
    "    \"County of Injury\",\n",
    "    \"District Name\",\n",
    "    \"Gender\",\n",
    "    \"Industry Code\",\n",
    "    \"Medical Fee Region\",\n",
    "    \"WCIO Cause of Injury Code\",\n",
    "    \"WCIO Nature of Injury Code\",\n",
    "    \"WCIO Part Of Body Code\",\n",
    "    \"Age at Injury Category\",\n",
    "    \"Carrier Claim Category\",\n",
    "    \"Body Section\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea71eec",
   "metadata": {},
   "source": [
    "temos de dropar as colunas e por no pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "8f92e4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping redundant variables that carry almost the same information (are extremely correlated (|0.8|))\n",
    "# We believe it was better to keep Age at Injury than birth year since it should be more related to the injury claim type (it will be tested later)\n",
    "# The same logic was applied to dropping the other two dates and two DSA variables since we believe Accident date to be more important\n",
    "# Para `data`\n",
    "data = data.loc[:, ~data.columns.isin(['Birth Year', 'Assembly Date', 'C-2 Date', 'Assembly Date DSA', 'First Hearing Date DSA'])]\n",
    "\n",
    "# Para `data_test`\n",
    "data_test = data_test.loc[:, ~data_test.columns.isin(['Birth Year', 'Assembly Date', 'C-2 Date', 'Assembly Date DSA', 'First Hearing Date DSA'])]\n",
    "\n",
    "\n",
    "for col in ['Birth Year',\"Assembly Date DSA\", \"First Hearing Date DSA\"]:\n",
    "    num_features.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "61c9780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the codes always seem to provide the same or more information than the descriptions (have more categories),\n",
    "#and the codes are consistent (always only having 1 description for code, while descriptions may have multiple codes)\n",
    "#we will drop the description columns.\n",
    "data.drop(['Industry Code Description','WCIO Cause of Injury Description','WCIO Nature of Injury Description','WCIO Part Of Body Description'], axis = 1,inplace = True)\n",
    "data_test.drop(['Industry Code Description','WCIO Cause of Injury Description','WCIO Nature of Injury Description','WCIO Part Of Body Description'], axis = 1,inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e0b67b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Zip Code for reason meantion in pre-processement\n",
    "data.drop(['Zip Code'], axis=1, inplace = True)\n",
    "data_test.drop(['Zip Code'], axis=1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9f0380ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"Claim Injury Type\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "130bc8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"Claim Injury Type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d52056a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4cdcba-054b-43fc-8d41-919cf8f0b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70233f94-61da-42da-9986-7d1d15398230",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['Accident Date', 'Age at Injury', 'Alternative Dispute Resolution', 'Attorney/Representative', 'Average Weekly Wage', 'C-3 Date', 'Carrier Name', 'Carrier Type', 'County of Injury',\n",
    "                      'COVID-19 Indicator', 'District Name', 'First Hearing Date', 'Gender', 'IME-4 Count', 'Industry Code', 'Medical Fee Region', 'WCIO Cause of Injury Code', 'WCIO Nature of Injury Code', \n",
    "                      'WCIO Part Of Body Code', 'Number of Dependents', 'Accident Year', 'Accident Month', 'Accident Day', 'Accident DayOfWeek', 'C-2 Date DSA', 'C-3 Date DSA',\n",
    "                         'Age at Injury Category', # 'Accident Date_missing', 'First Hearing Date_missing', 'C-3 Date_missing', 'Assembly Date_missing', 'C-2 Date_missing',\n",
    "                        'Carrier Claim Category', 'Body Section']\n",
    "\n",
    "# Select Categorical features\n",
    "selected_cat_features = ['Alternative Dispute Resolution', 'Attorney/Representative', 'Carrier Name', 'Carrier Type', 'County of Injury',\n",
    "                        'COVID-19 Indicator', 'District Name', 'Gender', 'Industry Code', 'Medical Fee Region', 'WCIO Cause of Injury Code', 'WCIO Nature of Injury Code',\n",
    "                        'WCIO Part Of Body Code', 'Carrier Claim Category', 'Body Section'] # based on RFE\n",
    "\n",
    "# Select numerical features\n",
    "selected_num_features = ['Age at Injury', 'Average Weekly Wage','IME-4 Count','Number of Dependents',\n",
    "                         'Accident Year','Accident Month', 'Accident Day', 'Accident DayOfWeek',\n",
    "                         'C-2 Date DSA', 'C-3 Date DSA'] # based on RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f430371d-2ed7-4955-ba0d-7d061b421201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_5244/1444211760.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val[feature] = X_val[feature].map(proportion) # Do the same for the valid dataset\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_5244/1444211760.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val[feature] = X_val[feature].fillna(0)  # Handle categories in X_val not seen in X_train with 0\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_5244/1444211760.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val[feature] = X_val[feature].map(proportion) # Do the same for the valid dataset\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_5244/1444211760.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val[feature] = X_val[feature].fillna(0)  # Handle categories in X_val not seen in X_train with 0\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_5244/1444211760.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val[feature] = X_val[feature].map(proportion) # Do the same for the valid dataset\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_5244/1444211760.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val[feature] = X_val[feature].fillna(0)  # Handle categories in X_val not seen in X_train with 0\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_5244/1444211760.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val[feature] = X_val[feature].map(proportion) # Do the same for the valid dataset\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_5244/1444211760.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val[feature] = X_val[feature].fillna(0)  # Handle categories in X_val not seen in X_train with 0\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_5244/1444211760.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val[feature] = X_val[feature].map(proportion) # Do the same for the valid dataset\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_5244/1444211760.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val[feature] = X_val[feature].fillna(0)  # Handle categories in X_val not seen in X_train with 0\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "xgb_results = cv_scores(XGBClassifier(), X, y,num_features,cat_features,scalling_outlier =False)\n",
    "# look at results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "f4ffce66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train_precision</th>\n",
       "      <th>Test_precision</th>\n",
       "      <th>Train_recall</th>\n",
       "      <th>Test_recall</th>\n",
       "      <th>Train_f1_score</th>\n",
       "      <th>Test_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fold 1</th>\n",
       "      <td>0.797052</td>\n",
       "      <td>0.369771</td>\n",
       "      <td>0.603691</td>\n",
       "      <td>0.425841</td>\n",
       "      <td>0.640727</td>\n",
       "      <td>0.295501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 2</th>\n",
       "      <td>0.806168</td>\n",
       "      <td>0.267869</td>\n",
       "      <td>0.625445</td>\n",
       "      <td>0.254185</td>\n",
       "      <td>0.663898</td>\n",
       "      <td>0.246825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 3</th>\n",
       "      <td>0.800956</td>\n",
       "      <td>0.243939</td>\n",
       "      <td>0.625061</td>\n",
       "      <td>0.214755</td>\n",
       "      <td>0.664659</td>\n",
       "      <td>0.219250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 4</th>\n",
       "      <td>0.804798</td>\n",
       "      <td>0.253076</td>\n",
       "      <td>0.607866</td>\n",
       "      <td>0.207213</td>\n",
       "      <td>0.644495</td>\n",
       "      <td>0.217018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 5</th>\n",
       "      <td>0.800574</td>\n",
       "      <td>0.467524</td>\n",
       "      <td>0.585593</td>\n",
       "      <td>0.194822</td>\n",
       "      <td>0.618541</td>\n",
       "      <td>0.144610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>0.801910</td>\n",
       "      <td>0.320436</td>\n",
       "      <td>0.609531</td>\n",
       "      <td>0.259363</td>\n",
       "      <td>0.646464</td>\n",
       "      <td>0.224641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Train_precision  Test_precision  Train_recall  Test_recall  \\\n",
       "Fold 1          0.797052        0.369771      0.603691     0.425841   \n",
       "Fold 2          0.806168        0.267869      0.625445     0.254185   \n",
       "Fold 3          0.800956        0.243939      0.625061     0.214755   \n",
       "Fold 4          0.804798        0.253076      0.607866     0.207213   \n",
       "Fold 5          0.800574        0.467524      0.585593     0.194822   \n",
       "Average         0.801910        0.320436      0.609531     0.259363   \n",
       "\n",
       "         Train_f1_score  Test_f1_score  \n",
       "Fold 1         0.640727       0.295501  \n",
       "Fold 2         0.663898       0.246825  \n",
       "Fold 3         0.664659       0.219250  \n",
       "Fold 4         0.644495       0.217018  \n",
       "Fold 5         0.618541       0.144610  \n",
       "Average        0.646464       0.224641  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923c21bd-9847-47e1-91ff-70b453b3b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction\n",
    "    # Choose best model from KFold above\n",
    "    # Train model on whole train and predict test data\n",
    "    submission = test_prediction(XGBClassifier(),X[selected_features],y,data_test)\n",
    "    submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

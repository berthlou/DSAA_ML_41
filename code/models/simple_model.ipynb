{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e58895a-f32f-4b6f-a51b-8cee8144cee6",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b1650a11-757b-4789-b76e-e8afa69602bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from statistics import mean\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b5516223-7631-42d7-bab5-a512d553cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_memory = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34702d17-d593-4255-9374-391bca21dcfa",
   "metadata": {},
   "source": [
    "### Importing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c72a76ab-e0bf-48e0-b401-83b78f12e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_data.csv\")\n",
    "data = df.copy()\n",
    "data_test = pd.read_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a73431-0d2f-4a53-b31a-0e1add7deb0e",
   "metadata": {},
   "source": [
    "### Defining the types of our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b66c426c-0809-4e1b-963d-80047b048ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_features = ['Accident Date', 'Assembly Date', 'C-2 Date', 'C-3 Date', 'First Hearing Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d9214b6b-3831-49df-abc2-657c89ab3e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = ['Age at Injury', 'Average Weekly Wage', 'Birth Year', 'IME-4 Count', 'Number of Dependents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "291d07d9-9e22-4578-9b98-e4ddb520510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['Alternative Dispute Resolution', 'Attorney/Representative',\n",
    "       'Carrier Name', 'Carrier Type','County of Injury',\n",
    "       'COVID-19 Indicator', 'District Name', 'Gender','Industry Code',\n",
    "       'Industry Code Description', 'Medical Fee Region','WCIO Cause of Injury Code',\n",
    "       'WCIO Cause of Injury Description', 'WCIO Nature of Injury Code', \n",
    "       'WCIO Nature of Injury Description', 'WCIO Part Of Body Code', 'Agreement Reached',\n",
    "       'WCIO Part Of Body Description', 'Zip Code', 'WCB Decision']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98724ed-c478-4821-8349-cfcdc578a029",
   "metadata": {},
   "source": [
    "### Dropping variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9c1f29ce-b118-4957-81ef-3fc5d90ea686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping variables that are definitely useless from our dataset\n",
    "# The first variable has 100% missing values and the second only contains one type of value,\n",
    "# meaning it doesn't provide any useful information + it isn't in the test dataset\n",
    "data.drop([\"OIICS Nature of Injury Description\", \"WCB Decision\"], axis=1, inplace = True)\n",
    "data_test.drop([\"OIICS Nature of Injury Description\"], axis=1, inplace = True)\n",
    "cat_features.remove('WCB Decision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ae509598-8dab-47df-af0f-d80ec0293bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping \"Agreement Reached\" because it is not on the validation dataset\n",
    "#Later we can try to predict this column and then predict the target but for now lets drop ot\n",
    "data = data.drop(['Agreement Reached'], axis=1)\n",
    "cat_features.remove(\"Agreement Reached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9ce1c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll eliminate the Zip_Code variable since we already have its information in other variables plus no model will be able to interpret the location information in this code\n",
    "data = data.drop(['Zip Code'], axis=1)\n",
    "cat_features.remove(\"Zip Code\")\n",
    "data_test = data_test.drop(['Zip Code'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "71ee328b-b562-4d9b-a7cf-ca38cf5d0e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping redundant variables that carry almost the same information (are extremely correlated)\n",
    "# We believe it was better to keep Age at Injury than birth year since it should be more related to the injury claim type (it will be tested later)\n",
    "# The same logic was applied to dropping the other two dates since we believe Accident date to be more important\n",
    "data.drop(['Birth Year','Assembly Date', \"C-2 Date\"], axis = 1, inplace = True)\n",
    "data_test.drop(['Birth Year','Assembly Date', \"C-2 Date\"], axis = 1, inplace = True)\n",
    "for col in ['Assembly Date', \"C-2 Date\"]:\n",
    "    date_features.remove(col)\n",
    "num_features.remove('Birth Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5500e539-35ca-44d6-8ed8-14e699a3ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the codes always seem to provide the same or more information than the descriptions (have more categories),\n",
    "#and the codes are consistent (always only having 1 description for code, while descriptions may have multiple codes)\n",
    "#we will drop the description columns.\n",
    "data.drop(['Industry Code Description','WCIO Cause of Injury Description','WCIO Nature of Injury Description','WCIO Part Of Body Description'], axis = 1,inplace = True)\n",
    "data_test.drop(['Industry Code Description','WCIO Cause of Injury Description','WCIO Nature of Injury Description','WCIO Part Of Body Description'], axis = 1,inplace = True)\n",
    "for col in ['Industry Code Description','WCIO Cause of Injury Description','WCIO Nature of Injury Description','WCIO Part Of Body Description']:\n",
    "    cat_features.remove(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6e8191-1364-4428-a5cf-a1e2ec0169bd",
   "metadata": {},
   "source": [
    "### Removing Inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "94d44e33-c89d-443a-87fc-0f3e6e055ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows where the target variable is NaN\n",
    "#This also removes all the missing values in another 13 variables (which includes all our categorical variables)\n",
    "#and an inconsistency where we have to rows with the same claim Identifier\n",
    "data.dropna(axis = 0 , subset=[\"Claim Injury Type\"],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2c7a0b3f-c165-4e89-82f4-0badf76b841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicates\n",
    "data.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d3b1a6-9526-408e-9733-e3fce1bbf6cd",
   "metadata": {},
   "source": [
    "### Feature Engeneering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ed6bf6ba-fafb-4bed-8913-fb35da97f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a binary column for the dates since it might provide useful information that would be lost when the missing values are inputed\n",
    "#Missing values in dates can mean that it wasn't held yet \n",
    "date_columns = ['Accident Date', 'First Hearing Date', 'C-3 Date']\n",
    "\n",
    "for column in date_columns:\n",
    "    data[column + '_missing'] = data[column].isnull().astype(int)\n",
    "    data_test[column + '_missing'] = data[column].isnull().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10270a2c-2f23-4a45-858c-b4f84cff1c5b",
   "metadata": {},
   "source": [
    "### Defining the indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8e14d6bb-19be-40b5-a089-2b7bbb099d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Claim identifier as the index\n",
    "data.set_index('Claim Identifier', inplace = True)\n",
    "data_test.set_index('Claim Identifier', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ffcbf3-b524-4a42-bc07-fcf6134bc798",
   "metadata": {},
   "source": [
    "### Changing impossible 0's and impossible dates to missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "112e42db-835f-424c-931b-ea61953f5355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing impossible 0's to missing values\n",
    "data[\"Average Weekly Wage\"].replace(0, np.nan, inplace=True)\n",
    "data[\"Age at Injury\"].replace(0, np.nan, inplace=True)\n",
    "data_test[\"Average Weekly Wage\"].replace(0, np.nan, inplace=True)\n",
    "data_test[\"Age at Injury\"].replace(0, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce65fa0e-bbdf-4b43-85a7-a1aed5aa1b96",
   "metadata": {},
   "source": [
    "##### Converting dates into datetime64 and than into integers in miliseconds after 1970"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7c7cee12-15c2-42f8-b429-f8486919f3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in date_features:\n",
    "    data[col] = pd.to_datetime(data[col],format = \"%Y-%m-%d\", errors = \"coerce\")\n",
    "    data_test[col] = pd.to_datetime(data_test[col],format = \"%Y-%m-%d\", errors = \"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ebc8d0cd-c087-4166-bed7-5d3e36585c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is done in the middle step because its when we can compare the dates and remove inconsistencies\n",
    "inconsistent = data[data[\"Accident Date\"] > data[\"C-3 Date\"]].index\n",
    "data[\"C-3 Date\"].loc[inconsistent] = np.nan\n",
    "inconsistent2 = data[data[\"Accident Date\"] > data[\"First Hearing Date\"]].index\n",
    "data[\"First Hearing Date\"].loc[inconsistent2] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7dfcde2c-befb-45f2-baaf-38955e8ef4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting date columns to integers\n",
    "data[date_features] = data[date_features].apply(lambda x: x.astype('int64') // 10**9 if x.dtype == 'datetime64[ns]' else x)\n",
    "data_test[date_features] = data_test[date_features].apply(lambda x: x.astype('int64') // 10**9 if x.dtype == 'datetime64[ns]' else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e99cd8c-36e7-4dcd-badf-6fa83f6b4436",
   "metadata": {},
   "source": [
    "### Converting to binary the binary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "92c93ff6-125b-473f-81b9-dcf9ad8c683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting binary variables to binary\n",
    "data[\"Attorney/Representative\"] = [val if val != val else 1 if val == \"Y\" else 0 for val in data[\"Attorney/Representative\"]]\n",
    "data[\"COVID-19 Indicator\"] = [val if val != val else 1 if val == \"Y\" else 0 for val in data[\"COVID-19 Indicator\"]]\n",
    "data_test[\"Attorney/Representative\"] = [val if val != val else 1 if val == \"Y\" else 0 for val in data_test[\"Attorney/Representative\"]]\n",
    "data_test[\"COVID-19 Indicator\"] = [val if val != val else 1 if val == \"Y\" else 0 for val in data_test[\"COVID-19 Indicator\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d56d8ceb-876d-47e9-981b-8175b4fbb8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Even though it is still categorical we have to remove it so it isnt proportionally encoded\n",
    "cat_features.remove(\"Attorney/Representative\")\n",
    "cat_features.remove(\"COVID-19 Indicator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e61f5f-9e27-4f71-9647-e144c03a6de0",
   "metadata": {},
   "source": [
    "### Converting our target into labels for our model to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0c557798",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3a936478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label inconding our target variable \n",
    "data[\"Claim Injury Type\"] = le.fit_transform(data[\"Claim Injury Type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6105555f-fbbe-4576-96b2-81c922192296",
   "metadata": {},
   "source": [
    "### Converting our categorical variables into numerical ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2006dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using proportionate encoding to turn our categorical data into numerical because since we have a lot of categories, and\n",
    "#this solution keeps the information of the most frequent categories while also not increasing the dimensionality of our dataset manyfold\n",
    "for col in cat_features:\n",
    "            proportion = data[col].value_counts(normalize = True)  # Get the porportion of each category\n",
    "            data[col] = data[col].map(proportion)  # Map the porportions in the column\n",
    "            data_test[col] = data_test[col].map(proportion) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc5dd21-3733-4e89-9130-96ec78304833",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7063b11-b783-4244-87d9-32ec70049cb6",
   "metadata": {},
   "source": [
    "###### We have no missing values for categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1218beb",
   "metadata": {},
   "source": [
    "Numeric Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c7e471ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputing numeric missing values with a decision tree\n",
    "def impute_with_decision_tree(data, target_column):\n",
    "    \n",
    "    # Separating the missing values from the non\n",
    "    available_data = data[data[target_column].notna()]\n",
    "    missing_data = data[data[target_column].isna()]\n",
    "\n",
    "    # Making sure there is enough data to input \n",
    "    if len(available_data) == 0 or len(missing_data) == 0:\n",
    "        return data\n",
    "\n",
    "    # Separating the target column from the rest \n",
    "    X_available = available_data.drop(columns=[target_column])\n",
    "    y_available = available_data[target_column]\n",
    "\n",
    "    # Training the model with the available data\n",
    "    model = DecisionTreeRegressor()\n",
    "    model.fit(X_available, y_available)\n",
    "\n",
    "    # Prediting the missing values\n",
    "    X_missing = missing_data.drop(columns=[target_column])\n",
    "    predicted_values = model.predict(X_missing)\n",
    "\n",
    "    # Inputing the missing values with the predictions\n",
    "    data.loc[data[target_column].isna(), target_column] = predicted_values\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f4928bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputing numeric missing values with a decision tree\n",
    "def impute_with_xgb(data, target_column):\n",
    "    \n",
    "    # Separating the missing values from the non\n",
    "    available_data = data[data[target_column].notna()]\n",
    "    missing_data = data[data[target_column].isna()]\n",
    "\n",
    "    # Making sure there is enough data to input \n",
    "    if len(available_data) == 0 or len(missing_data) == 0:\n",
    "        return data\n",
    "\n",
    "    # Separating the target column from the rest \n",
    "    X_available = available_data.drop(columns=[target_column])\n",
    "    y_available = available_data[target_column]\n",
    "\n",
    "    # Training the model with the available data\n",
    "    model = XGBRegressor()\n",
    "    model.fit(X_available, y_available)\n",
    "\n",
    "    # Prediting the missing values\n",
    "    X_missing = missing_data.drop(columns=[target_column])\n",
    "    predicted_values = model.predict(X_missing)\n",
    "\n",
    "    # Inputing the missing values with the predictions\n",
    "    data.loc[data[target_column].isna(), target_column] = predicted_values\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "46e2767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputing numeric missing values with a decision tree\n",
    "def impute_with_random_forest(data, target_column):\n",
    "    \n",
    "    # Separating the missing values from the non\n",
    "    available_data = data[data[target_column].notna()]\n",
    "    missing_data = data[data[target_column].isna()]\n",
    "\n",
    "    # Making sure there is enough data to input \n",
    "    if len(available_data) == 0 or len(missing_data) == 0:\n",
    "        return data\n",
    "\n",
    "    # Separating the target column from the rest \n",
    "    X_available = available_data.drop(columns=[target_column])\n",
    "    y_available = available_data[target_column]\n",
    "\n",
    "    # Training the model with the available data\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_available, y_available)\n",
    "\n",
    "    # Prediting the missing values\n",
    "    X_missing = missing_data.drop(columns=[target_column])\n",
    "    predicted_values = model.predict(X_missing)\n",
    "\n",
    "    # Inputing the missing values with the predictions\n",
    "    data.loc[data[target_column].isna(), target_column] = predicted_values\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5875a3-e541-4949-b835-050708919f2f",
   "metadata": {},
   "source": [
    "### Defining X and y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9060e3f5-fa7b-459e-8684-74a9147cb371",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"Claim Injury Type\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1d735692-3570-4403-b334-55c4c5476a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"Claim Injury Type\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a152e7-398a-4598-9c34-6e1bfa338170",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab1d5f5-caf7-4e19-8948-5560edb5708e",
   "metadata": {},
   "source": [
    "### RFE (repeated feature elimination) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9705ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rfe(algorithm):\n",
    "\n",
    "    X_train, X_val,y_train, y_val = train_test_split(X,y,\n",
    "                                                train_size = 0.75, \n",
    "                                                shuffle = True, \n",
    "                                                stratify = y)\n",
    "\n",
    "    #Filling missing values\n",
    "    for column in num_features:\n",
    "        impute_with_decision_tree(X_train, column)\n",
    "        impute_with_decision_tree(X_val, column)\n",
    "    \n",
    "    #Generating the variables where we will store our results\n",
    "    nof_list = np.arange(1, len(X_train.columns) + 1)            \n",
    "    high_score = 0\n",
    "    opt_n_features = 0\n",
    "    train_score_list = []\n",
    "    val_score_list = []\n",
    "\n",
    "    #Variable where we will store the optimum amount of features\n",
    "    best_rfe = None\n",
    "\n",
    "    model = algorithm\n",
    "\n",
    "    for n in nof_list:\n",
    "        rfe = RFE(estimator=model, n_features_to_select=n)\n",
    "    \n",
    "    # Fitting the model to rfe\n",
    "        X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "        X_val_rfe = rfe.transform(X_val)\n",
    "    \n",
    "    # Training and predicting\n",
    "        model.fit(X_train_rfe, y_train)\n",
    "        pred_train = model.predict(X_train_rfe)\n",
    "        pred_val = model.predict(X_val_rfe)\n",
    "    \n",
    "    # Evaluating using the macro f1_score\n",
    "        train_score = f1_score(y_train, pred_train, average=\"macro\")\n",
    "        val_score = f1_score(y_val, pred_val, average=\"macro\")\n",
    "        train_score_list.append(train_score)\n",
    "        val_score_list.append(val_score)\n",
    "    \n",
    "    # Checking if this is the best combination of features so far\n",
    "        if val_score >= high_score:\n",
    "            high_score = val_score\n",
    "            opt_n_features = n\n",
    "            best_rfe = rfe  # Armazenar o RFE com o melhor número de features\n",
    "\n",
    "# Checking what amount of features and which features where the best for the model\n",
    "    selected_features = X_train.columns[best_rfe.support_].tolist()\n",
    "\n",
    "    print(\"Optimal number of features: %d\" % opt_n_features)\n",
    "    print(\"Score with %d features: %f\" % (opt_n_features, high_score))\n",
    "    print(\"Selected Features:\\n\", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a935a4-ec5e-4ec6-85d4-4bde28b9bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rfe(DecisionTreeClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf6ea91-f162-4555-9fb8-d629bd23c005",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rfe(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3742196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rfe(XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c006d9ad-0ccf-4060-94c7-8e914b0f812c",
   "metadata": {},
   "source": [
    "### These were the variables we obtain as a result of our RFE with a random forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cddd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['Accident Date', 'Age at Injury', 'Attorney/Representative', 'Average Weekly Wage', 'C-3 Date', 'Carrier Name', 'County of Injury', 'District Name', 'First Hearing Date', 'IME-4 Count', 'Industry Code', 'WCIO Cause of Injury Code', 'WCIO Nature of Injury Code', 'WCIO Part Of Body Code', 'Number of Dependents', 'First Hearing Date_missing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392350dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['Accident Date', 'Age at Injury', 'Attorney/Representative', 'Average Weekly Wage', 'C-3 Date', 'Carrier Name', 'Carrier Type', 'County of Injury', 'COVID-19 Indicator', 'District Name', 'First Hearing Date', 'Gender', 'IME-4 Count', 'Industry Code', 'Medical Fee Region', 'WCIO Cause of Injury Code', 'WCIO Nature of Injury Code', 'WCIO Part Of Body Code', 'Number of Dependents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bc02b4-1c05-4bdc-ad00-9d5513855824",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_num_features = ['Accident Date', 'Age at Injury', 'Average Weekly Wage', 'C-3 Date','First Hearing Date','IME-4 Count','Number of Dependents']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09acaaa4-7ec2-4fce-bfcb-42d3e9a09aee",
   "metadata": {},
   "source": [
    "# Decision tree models and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "81144bdd-d672-488b-873e-d3677c178acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the crossvalidation scores\n",
    "def cv_scores(model, X, y):\n",
    "    # Takes as argument the model used, the predictors and the target. Splits the data using StratifiedKFold, and\n",
    "    # trains model using X and y. Then it returns the results obtained from the stratified cross validation'''\n",
    "    \n",
    "    skf = KFold(n_splits=5)\n",
    "    \n",
    "    # Generating the lists to store our results\n",
    "    precision_scores_train = []\n",
    "    precision_scores_val = []   \n",
    "    recall_scores_train = []  \n",
    "    recall_scores_val = []\n",
    "    f1_scores_train = []    \n",
    "    f1_scores_val = []\n",
    "    index = [f'Fold {i}' for i in range(1,6)]\n",
    "    index.append(\"Average\")\n",
    "    \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        # Dividing our data in validation and train\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Inputing the missing values\n",
    "        for column in num_features:\n",
    "            impute_with_xgb(X_train, column)\n",
    "            impute_with_xgb(X_val, column)\n",
    "\n",
    "        # Data pre-processment\n",
    "        inconsistent = X_train[(X_train['Age at Injury'] > 80) | (X_train[\"Age at Injury\"] < 16)].index\n",
    "        X_train.drop(inconsistent, inplace=True)\n",
    "        y_train.drop(inconsistent, inplace=True)\n",
    "\n",
    "\n",
    "        # Training the classification model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Making the predictions for the training and validation data\n",
    "        pred_train = model.predict(X_train)\n",
    "        pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Calculating and storing the scores\n",
    "        precision_scores_train.append(precision_score(y_train, pred_train, average='macro'))\n",
    "        precision_scores_val.append(precision_score(y_val, pred_val, average='macro'))\n",
    "        recall_scores_train.append(recall_score(y_train, pred_train, average='macro'))\n",
    "        recall_scores_val.append(recall_score(y_val, pred_val, average='macro'))\n",
    "        f1_scores_train.append(f1_score(y_train, pred_train, average='macro'))\n",
    "        f1_scores_val.append(f1_score(y_val, pred_val, average='macro'))\n",
    "\n",
    "    \n",
    "    precision_scores_train.append(mean(precision_scores_train))\n",
    "    precision_scores_val.append(mean(precision_scores_val))\n",
    "    recall_scores_train.append(mean(recall_scores_train))\n",
    "    recall_scores_val.append(mean(recall_scores_val))\n",
    "    f1_scores_train.append(mean(f1_scores_train))\n",
    "    f1_scores_val.append(mean(f1_scores_val))\n",
    "\n",
    "    # Storing the results in a dataframe\n",
    "    model_results = pd.DataFrame(data={\n",
    "        'Train_precision': precision_scores_train,\n",
    "        'Test_precision': precision_scores_val,\n",
    "        'Train_recall': recall_scores_train,\n",
    "        'Test_recall': recall_scores_val,\n",
    "        'Train_f1_score': f1_scores_train,\n",
    "        'Test_f1_score': f1_scores_val,\n",
    "    }, index=index)\n",
    "    \n",
    "    return model_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e38375",
   "metadata": {},
   "source": [
    "todos os f1 em baixo foi com input rf, que foram os melhores valores que obtive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad7918c-b985-4fc9-8e5b-531a93d06008",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_results = cv_scores(DecisionTreeClassifier(), X, y)\n",
    "dt_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7e74f7-4ed4-4b2c-8595-9680cc6c0f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = cv_scores(RandomForestClassifier(), X, y)\n",
    "rf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865c2d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_results = cv_scores(XGBClassifier(), X, y)\n",
    "xgb_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335ba6b1-e9aa-4b7c-b91a-58a18ff24c67",
   "metadata": {},
   "source": [
    "# Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bb09822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(model, X, y , test):\n",
    "\n",
    "    X_train, X_val,y_train, y_val = train_test_split(X,y,\n",
    "                                                train_size = 0.8, \n",
    "                                                shuffle = True, \n",
    "                                                stratify = y)\n",
    "\n",
    "    \n",
    "    # data pre-processment\n",
    "    inconsistent = X_train[(X_train['Age at Injury'] > 80) | (X_train[\"Age at Injury\"] < 14)].index\n",
    "    X_train.drop(inconsistent, inplace=True)\n",
    "    y_train.drop(inconsistent, inplace=True)\n",
    "\n",
    "    # Missing value inputation\n",
    "    for column in num_features:\n",
    "        impute_with_xgb(X_train, column)\n",
    "        impute_with_xgb(test, column)\n",
    "\n",
    "    # Fitting the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    pred_val = model.predict(X_val)\n",
    "    print(f1_score(y_val, pred_val, average='macro'))\n",
    "\n",
    "    # Using the model to make prediction on the test dataset\n",
    "    pred_test = model.predict(test)\n",
    "\n",
    "    #Label enconding our target variable \n",
    "    pred_test = le.inverse_transform(pred_test)\n",
    "\n",
    "    # Making a dataframe with the indexes of data_test and predictions converted back to strings\n",
    "    submission_df = pd.DataFrame({\n",
    "        \"Claim Injury Type\": pred_test\n",
    "    }, index=data_test.index)\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "00633eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3104945550519305\n",
      "Claim Injury Type\n",
      "1                    303682\n",
      "3                     57001\n",
      "2                     21674\n",
      "0                      4549\n",
      "4                      1061\n",
      "7                         8\n",
      "Name: count, dtype: int64\n",
      "Claim Injury Type\n",
      "2. NON-COMP          303682\n",
      "4. TEMPORARY          57001\n",
      "3. MED ONLY           21674\n",
      "1. CANCELLED           4549\n",
      "5. PPD SCH LOSS        1061\n",
      "8. DEATH                  8\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claim Injury Type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claim Identifier</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6165911</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166141</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6165907</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166047</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166102</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553137</th>\n",
       "      <td>1. CANCELLED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553119</th>\n",
       "      <td>1. CANCELLED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553542</th>\n",
       "      <td>1. CANCELLED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553455</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553594</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>387975 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Claim Injury Type\n",
       "Claim Identifier                  \n",
       "6165911                2. NON-COMP\n",
       "6166141                2. NON-COMP\n",
       "6165907                2. NON-COMP\n",
       "6166047                2. NON-COMP\n",
       "6166102                2. NON-COMP\n",
       "...                            ...\n",
       "6553137               1. CANCELLED\n",
       "6553119               1. CANCELLED\n",
       "6553542               1. CANCELLED\n",
       "6553455                2. NON-COMP\n",
       "6553594                2. NON-COMP\n",
       "\n",
       "[387975 rows x 1 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = test_prediction(XGBClassifier(),X,y,data_test)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3b833b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"Submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a814dd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Claim Injury Type    6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.nunique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

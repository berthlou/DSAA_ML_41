{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "039090e4-5406-4b1c-b3c9-5b663cc2872e",
   "metadata": {},
   "source": [
    "Template notebook for one type of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "id": "d29f30c5-6ec1-4339-a2af-a73bd36f91c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier,XGBRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from statistics import mean\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Other imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "id": "27ac0de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_1692/1043285265.py:2: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data= pd.read_csv(\"train_data_enriched.csv\", index_col=\"Claim Identifier\")\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data= pd.read_csv(\"train_data_enriched.csv\", index_col=\"Claim Identifier\")\n",
    "data_test = pd.read_csv(\"test_data_enriched.csv\",index_col=\"Claim Identifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "60061974",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "3fa916c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label inconding our target variable \n",
    "data[\"Claim Injury Type\"] = le.fit_transform(data[\"Claim Injury Type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "41080786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(df, target_column, algorithm):\n",
    "    # Separating the missing values from the non missing values\n",
    "    available_data = df[df[target_column].notna()]\n",
    "    missing_data = df[df[target_column].isna()]\n",
    "\n",
    "    # Diagnóstico inicial\n",
    "    \"\"\"print(f\"\\nImputando valores para coluna: {target_column}\")\n",
    "    print(f\"available data: {len(available_data)}\")\n",
    "    print(f\"missing data: {len(missing_data)}\")\"\"\"\n",
    "\n",
    "    # Verificar se há dados suficientes para imputação\n",
    "    if len(available_data) == 0 or len(missing_data) == 0:\n",
    "        # print(f\"nao deu input na coluna {target_column}\")\n",
    "        return df\n",
    "\n",
    "    # Separating the target column from the rest\n",
    "    X_available = available_data.drop(columns=[target_column])\n",
    "    y_available = available_data[target_column]\n",
    "\n",
    "    # Garantir consistência entre colunas\n",
    "    X_available = X_available.select_dtypes(include=[\"number\"])\n",
    "    X_missing = missing_data.drop(columns=[target_column]).select_dtypes(include=[\"number\"])\n",
    "    common_columns = X_available.columns.intersection(X_missing.columns)\n",
    "    X_available = X_available[common_columns]\n",
    "    X_missing = X_missing[common_columns]\n",
    "\n",
    "    # Verificar se ainda há colunas suficientes após alinhamento\n",
    "    if X_available.shape[1] == 0:\n",
    "        # print(f\"Sem colunas disponíveis para imputar na coluna {target_column}\")\n",
    "        return df\n",
    "\n",
    "    # Training the model with the available data\n",
    "    model = algorithm\n",
    "    model.fit(X_available, y_available)\n",
    "\n",
    "    # Prediting the missing values\n",
    "    predicted_values = model.predict(X_missing)\n",
    "\n",
    "    #print(len(predicted_values))\n",
    "\n",
    "    # Criar uma cópia explícita para evitar problemas com views\n",
    "    df = df.copy()\n",
    "    df.loc[df[target_column].isna(), target_column] = predicted_values\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a5aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"print(\"Antes da imputação:\")\n",
    "print(data[\"C-3 Date\"].isna().sum())\n",
    "\n",
    "# Atribuir o DataFrame resultante à variável 'data'\n",
    "data = impute_missing_values(data, 'C-3 Date', XGBRegressor())\n",
    "\n",
    "print(\"Depois da imputação:\")\n",
    "print(data[\"C-3 Date\"].isna().sum())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "d5e1910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(data, step_name):\n",
    "    print(f\"\\n{step_name}: Valores ausentes restantes:\")\n",
    "    print(data.isnull().sum()[data.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "3f20a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def handle_outliers(data, column):\n",
    "    \"\"\"\n",
    "    Handle outliers in a numerical column by replacing values outside the interquartile range (IQR) with the respective bounds.\n",
    "    Additionally, generates boxplots before and after the outlier treatment.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The dataset containing the column.\n",
    "        column (str): The column name to process.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with treated outliers.\n",
    "    \"\"\"\n",
    "    # Verifica se a coluna possui valores válidos\n",
    "    if data[column].notnull().sum() > 0:  # Apenas processa colunas com dados válidos\n",
    "        \"\"\"# Criar boxplot antes do tratamento\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(data[column])\n",
    "        plt.title(f\"Boxplot Antes do Tratamento de Outliers - {column}\")\n",
    "        plt.show()\"\"\"\n",
    "\n",
    "        # Calcular limites do IQR\n",
    "        Q1 = data[column].quantile(0.25)\n",
    "        Q3 = data[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Substituir valores fora dos limites pelos próprios limites\n",
    "        data[column] = np.where(data[column] < lower_bound, np.nan, data[column])\n",
    "        data[column] = np.where(data[column] > upper_bound, np.nan, data[column])\n",
    "\n",
    "        \"\"\"# Criar boxplot após o tratamento\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(data[column])\n",
    "        plt.title(f\"Boxplot Após o Tratamento de Outliers - {column}\")\n",
    "        plt.show()\"\"\"\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "c9e310a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_numerical(column, X_train, X_val, scaler):\n",
    "    # Certifique-se de que a coluna é numérica\n",
    "    if not pd.api.types.is_numeric_dtype(X_train[column]):\n",
    "        print(f\"A coluna '{column}' não é numérica e será ignorada.\")\n",
    "        return\n",
    "\n",
    "    # Escala os dados e substitui os valores na coluna\n",
    "    try:\n",
    "        X_train[column] = scaler.fit_transform(X_train[[column]])\n",
    "        X_val[column] = scaler.transform(X_val[[column]])\n",
    "    except ValueError as e:\n",
    "        print(f\"Erro ao escalonar a coluna '{column}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "92397747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  claim_carrier_categories(X_train, X_val):\n",
    "    # Define a function to categorize each carrier based on its claim count\n",
    "    count = X_train['Carrier Name'].value_counts()\n",
    "    def categorize_claims(count):\n",
    "        if count >= 40000:\n",
    "            return 2\n",
    "        elif 4000 <= count < 40000:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # Apply the categorization to create a mapping dictionary\n",
    "    carrier_category_map = count.apply(categorize_claims)\n",
    "\n",
    "    # Map the `Carrier Name` to the new `Carrier Claim Category`\n",
    "    X_train['Carrier Claim Category'] = X_train['Carrier Name'].map(carrier_category_map)\n",
    "    X_val['Carrier Claim Category'] = X_val['Carrier Name'].map(carrier_category_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "id": "db2cfcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical encoder function\n",
    "def categorical_prop_encode(X_train, X_val, feature):\n",
    "    proportion = X_train[feature].value_counts(normalize = True)  # Get the porportion of each category\n",
    "    X_train[feature] = X_train[feature].map(proportion)  # Map the porportions in the column\n",
    "    X_val[feature] = X_val[feature].map(proportion) # Do the same for the valid dataset\n",
    "    X_val[feature] = X_val[feature].fillna(0)  # Handle categories in X_val not seen in X_train with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4efb3e3-69d9-4968-a39c-9f2cd20c4ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rfe(algorithm,num_features,cat_features,num_imputing_algorithm= DecisionTreeRegressor() , cat_imputing_algorithm = DecisionTreeClassifier(), scaling_outlier = False, scaler = MinMaxScaler()):\n",
    "\n",
    "    X_train, X_val,y_train, y_val = train_test_split(X,y,\n",
    "                                                train_size = 0.8, \n",
    "                                                shuffle = True, \n",
    "                                                stratify = y)\n",
    "\n",
    "    #Filling num missing values\n",
    "    for column in num_features:\n",
    "        X_train = impute_missing_values(X_train, column, num_imputing_algorithm)\n",
    "        X_val = impute_missing_values(X_val, column, num_imputing_algorithm)\n",
    "\n",
    "    # Filling categorical missing values\n",
    "    X_train = impute_missing_values(X_train, \"Alternative Dispute Resolution\", cat_imputing_algorithm)\n",
    "    X_val = impute_missing_values(X_val, \"Alternative Dispute Resolution\", cat_imputing_algorithm)\n",
    "\n",
    "    # Removing inconsistencies on the train\n",
    "    inconsistent = X_train[(X_train['Age at Injury'] > 80) | (X_train[\"Age at Injury\"] < 16)].index\n",
    "    X_train = X_train.loc[~X_train.index.isin(inconsistent)]\n",
    "    y_train = y_train.loc[~y_train.index.isin(inconsistent)]\n",
    "\n",
    "    # Performing scaling and outlier treatment dependent on the boolean\n",
    "    if scaling_outlier:\n",
    "        for column in num_features:\n",
    "            handle_outliers(X_train, column)\n",
    "            check_missing_values(X_train, \"Apos handle_outliers (train)\")\n",
    "            check_missing_values(X_val, \"Apos handle_outliers (validation)\")\n",
    "            X_train = impute_missing_values(X_train, column, num_imputing_algorithm)\n",
    "\n",
    "        for column in num_features:\n",
    "            scale_numerical(column, X_train, X_val, scaler)\n",
    "\n",
    "        check_missing_values(X_train, \"Apos scalling (train)\")\n",
    "        check_missing_values(X_val, \"Apos scalling (validation)\")\n",
    "\n",
    "    # Creating an ordinal variable\n",
    "    claim_carrier_categories(X_train, X_val)\n",
    "\n",
    "    X_val = impute_missing_values(X_val, \"Carrier Claim Category\", cat_imputing_algorithm)\n",
    "\n",
    "    check_missing_values(X_train, \"Antes do modelo (train)\")\n",
    "    check_missing_values(X_val, \"Antes do modelo (validation)\")\n",
    "\n",
    "    # Categorical Prop Encoding\n",
    "    for cat_feature in cat_features:\n",
    "        categorical_prop_encode(X_train, X_val, cat_feature)\n",
    "    \n",
    "    #Generating the variables where we will store our results\n",
    "    nof_list = np.arange(1, len(X_train.columns) + 1)            \n",
    "    high_score = 0\n",
    "    opt_n_features = 0\n",
    "    train_score_list = []\n",
    "    val_score_list = []\n",
    "\n",
    "    #Variable where we will store the optimum amount of features\n",
    "    best_rfe = None\n",
    "\n",
    "    model = algorithm\n",
    "\n",
    "    for n in nof_list:\n",
    "        rfe = RFE(estimator=model, n_features_to_select=n)\n",
    "    \n",
    "    # Fitting the model to rfe\n",
    "        X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "        X_val_rfe = rfe.transform(X_val)\n",
    "    \n",
    "    # Training and predicting\n",
    "        model.fit(X_train_rfe, y_train)\n",
    "        pred_train = model.predict(X_train_rfe)\n",
    "        pred_val = model.predict(X_val_rfe)\n",
    "    \n",
    "    # Evaluating using the macro f1_score\n",
    "        train_score = f1_score(y_train, pred_train, average=\"macro\")\n",
    "        val_score = f1_score(y_val, pred_val, average=\"macro\")\n",
    "        train_score_list.append(train_score)\n",
    "        val_score_list.append(val_score)\n",
    "    \n",
    "    # Checking if this is the best combination of features so far\n",
    "        if val_score >= high_score:\n",
    "            high_score = val_score\n",
    "            opt_n_features = n\n",
    "            best_rfe = rfe  # Armazenar o RFE com o melhor número de features\n",
    "\n",
    "# Checking what amount of features and which features where the best for the model\n",
    "    selected_features = X_train.columns[best_rfe.support_].tolist()\n",
    "\n",
    "    print(\"Optimal number of features: %d\" % opt_n_features)\n",
    "    print(\"Score with %d features: %f\" % (opt_n_features, high_score))\n",
    "    print(\"Selected Features:\\n\", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc04b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_scores(model, X, y, num_features, cat_features, num_imputing_algorithm=RandomForestRegressor(), cat_imputing_algorithm=RandomForestClassifier(), scaling_outlier= False, scaler=MinMaxScaler()):\n",
    "    \"\"\"\n",
    "    Takes as argument the model used, the predictors and the target, the models used for imputing numerical and categorical \n",
    "    features, if any scaling and outlier removed should be performed, and what scaling method should be used.\n",
    "    Then it returns the results obtained from the stratified cross-validation for the given model.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # Generating the lists to store our results\n",
    "    precision_scores_train = []\n",
    "    precision_scores_val = []   \n",
    "    recall_scores_train = []  \n",
    "    recall_scores_val = []\n",
    "    f1_scores_train = []    \n",
    "    f1_scores_val = []\n",
    "    index = [f'Fold {i}' for i in range(1, 6)]\n",
    "    index.append(\"Average\")\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        # Dividing our data in validation and train\n",
    "        X_train, X_val = X.iloc[train_index].copy(), X.iloc[test_index].copy()\n",
    "        y_train, y_val = y.iloc[train_index].copy(), y.iloc[test_index].copy()\n",
    "\n",
    "        # Filling numerical missing values\n",
    "        for column in num_features:\n",
    "            X_train = impute_missing_values(X_train, column, num_imputing_algorithm)\n",
    "            X_val = impute_missing_values(X_val, column, num_imputing_algorithm)\n",
    "\n",
    "        # Filling categorical missing values\n",
    "        X_train = impute_missing_values(X_train, \"Alternative Dispute Resolution\", cat_imputing_algorithm)\n",
    "        X_val = impute_missing_values(X_val, \"Alternative Dispute Resolution\", cat_imputing_algorithm)\n",
    "\n",
    "        check_missing_values(X_train, \"Apos tratar de missing values (train)\")\n",
    "        check_missing_values(X_val, \"Apos tratar de missing values (validation)\")\n",
    "\n",
    "        # Removing inconsistencies on the train\n",
    "        inconsistent = X_train[(X_train['Age at Injury'] > 80) | (X_train[\"Age at Injury\"] < 16)].index\n",
    "        X_train = X_train.loc[~X_train.index.isin(inconsistent)]\n",
    "        y_train = y_train.loc[~y_train.index.isin(inconsistent)]\n",
    "\n",
    "        # Performing scaling and outlier treatment dependent on the boolean\n",
    "        if scaling_outlier:\n",
    "            for column in num_features:\n",
    "                handle_outliers(X_train, column)\n",
    "                check_missing_values(X_train, \"Apos handle_outliers (train)\")\n",
    "                check_missing_values(X_val, \"Apos handle_outliers (validation)\")\n",
    "                X_train = impute_missing_values(X_train, column, num_imputing_algorithm)\n",
    "\n",
    "            for column in num_features:\n",
    "                scale_numerical(column, X_train, X_val, scaler)\n",
    "            \n",
    "            print(X_train)\n",
    "\n",
    "            check_missing_values(X_train, \"Apos scalling (train)\")\n",
    "            check_missing_values(X_val, \"Apos scalling (validation)\")\n",
    "\n",
    "        # Creating an ordinal variable\n",
    "        claim_carrier_categories(X_train, X_val)\n",
    "\n",
    "        X_val = impute_missing_values(X_val, \"Carrier Claim Category\", cat_imputing_algorithm)\n",
    "\n",
    "        check_missing_values(X_train, \"Antes do modelo (train)\")\n",
    "        check_missing_values(X_val, \"Antes do modelo (validation)\")\n",
    "\n",
    "        # Categorical Prop Encoding\n",
    "        for cat_feature in cat_features:\n",
    "            categorical_prop_encode(X_train, X_val, cat_feature)\n",
    "\n",
    "        # Training the classification model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Making the predictions for the training and validation data\n",
    "        pred_train = model.predict(X_train)\n",
    "        pred_val = model.predict(X_val)\n",
    "\n",
    "        # Calculating and storing the scores\n",
    "        precision_scores_train.append(precision_score(y_train, pred_train, average='macro'))\n",
    "        precision_scores_val.append(precision_score(y_val, pred_val, average='macro'))\n",
    "        recall_scores_train.append(recall_score(y_train, pred_train, average='macro'))\n",
    "        recall_scores_val.append(recall_score(y_val, pred_val, average='macro'))\n",
    "        f1_scores_train.append(f1_score(y_train, pred_train, average='macro'))\n",
    "        f1_scores_val.append(f1_score(y_val, pred_val, average='macro'))\n",
    "\n",
    "    precision_scores_train.append(mean(precision_scores_train))\n",
    "    precision_scores_val.append(mean(precision_scores_val))\n",
    "    recall_scores_train.append(mean(recall_scores_train))\n",
    "    recall_scores_val.append(mean(recall_scores_val))\n",
    "    f1_scores_train.append(mean(f1_scores_train))\n",
    "    f1_scores_val.append(mean(f1_scores_val))\n",
    "\n",
    "    # Storing the results in a dataframe\n",
    "    model_results = pd.DataFrame(data={\n",
    "        'Train_precision': precision_scores_train,\n",
    "        'Test_precision': precision_scores_val,\n",
    "        'Train_recall': recall_scores_train,\n",
    "        'Test_recall': recall_scores_val,\n",
    "        'Train_f1_score': f1_scores_train,\n",
    "        'Test_f1_score': f1_scores_val,\n",
    "    }, index=index)\n",
    "\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "bb306304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(model, X, y, num_features, cat_features, data_test, \n",
    "                    num_imputing_algorithm=XGBRegressor(), \n",
    "                    cat_imputing_algorithm=XGBClassifier(), scaling_outlier = False , \n",
    "                    scaler=MinMaxScaler()):\n",
    "\n",
    "    # Etapa 1: Imputação inicial de valores ausentes\n",
    "    for column in num_features:\n",
    "        impute_missing_values(X, column, num_imputing_algorithm)\n",
    "        impute_missing_values(data_test, column, num_imputing_algorithm)\n",
    "    impute_missing_values(X, \"Alternative Dispute Resolution\", cat_imputing_algorithm)\n",
    "    impute_missing_values(data_test, \"Alternative Dispute Resolution\", cat_imputing_algorithm)\n",
    "\n",
    "    check_missing_values(X, \"Apos tratar de missing values (train)\")\n",
    "    check_missing_values(data_test, \"Apos tratar de missing values (validation)\")\n",
    "\n",
    "    # Etapa 2: Remoção de inconsistências\n",
    "    inconsistent = X[(X['Age at Injury'] > 80) | (X[\"Age at Injury\"] < 16)].index\n",
    "    X.drop(inconsistent, inplace=True)\n",
    "    y.drop(inconsistent, inplace=True)\n",
    "\n",
    "    if scaling_outlier:\n",
    "        # Etapa 3: Tratamento de outliers\n",
    "        for column in num_features:\n",
    "            handle_outliers(X, column)\n",
    "            check_missing_values(X, \"Apos handle_outliers (train)\")\n",
    "            X = impute_missing_values(X, column, num_imputing_algorithm)\n",
    "\n",
    "        for column in num_features:\n",
    "            scale_numerical(column, X, data_test, scaler)\n",
    "        \n",
    "        check_missing_values(X, \"Apos scalling (train)\")\n",
    "        check_missing_values(data_test, \"Apos scalling (validation)\")\n",
    "    \n",
    "    # Creating an ordinal variable\n",
    "    claim_carrier_categories(X, data_test)\n",
    "    X = impute_missing_values(X, \"Carrier Claim Category\", cat_imputing_algorithm)\n",
    "\n",
    "    # Categorical Prop Encoding\n",
    "    for cat_feature in cat_features:\n",
    "        categorical_prop_encode(X, data_test, cat_feature)\n",
    "\n",
    "    check_missing_values(X, \"Antes do modelo (train)\")\n",
    "    check_missing_values(data_test, \"Antes do modelo (validation)\")\n",
    "\n",
    "    model.fit(X, y)\n",
    "    pred_test = model.predict(data_test)\n",
    "    pred_test = le.inverse_transform(pred_test)\n",
    "\n",
    "    submission_df = pd.DataFrame({\n",
    "        \"Claim Injury Type\": pred_test\n",
    "    }, index=data_test.index)\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fc3195-03a0-4757-bb36-5bffec585a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e76c9ed5",
   "metadata": {},
   "source": [
    "Temos de rever as num features e cat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "49cc0c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = ['Age at Injury', 'Average Weekly Wage', 'Birth Year', 'IME-4 Count', 'Number of Dependents',\n",
    "                \"Accident Year\",\"Accident Month\",\"Accident Day\",\"Accident DayOfWeek\",\"Assembly Date DSA\",\n",
    "                \"C-2 Date DSA\",\"C-3 Date DSA\",\"First Hearing Date DSA\" ,\"Accident Year\",\n",
    "    \"Accident Month\",\n",
    "    \"Accident Day\",\n",
    "    \"Accident DayOfWeek\",\"Accident Date\",\"C-3 Date\",\"First Hearing Date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "5bb15763",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\n",
    "    \"Alternative Dispute Resolution\",\n",
    "    \"Carrier Name\",\n",
    "    \"Carrier Type\",\n",
    "    \"County of Injury\",\n",
    "    \"District Name\",\n",
    "    \"Gender\",\n",
    "    \"Industry Code\",\n",
    "    \"Medical Fee Region\",\n",
    "    \"WCIO Cause of Injury Code\",\n",
    "    \"WCIO Nature of Injury Code\",\n",
    "    \"WCIO Part Of Body Code\",\n",
    "    \"Age at Injury Category\",\n",
    "    \"Carrier Claim Category\",\n",
    "    \"Body Section\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea71eec",
   "metadata": {},
   "source": [
    "temos de dropar as colunas e por no pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "8f92e4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping redundant variables that carry almost the same information (are extremely correlated (|0.8|))\n",
    "# We believe it was better to keep Age at Injury than birth year since it should be more related to the injury claim type (it will be tested later)\n",
    "# The same logic was applied to dropping the other two dates and two DSA variables since we believe Accident date to be more important\n",
    "# Para `data`\n",
    "data = data.loc[:, ~data.columns.isin(['Birth Year', 'Assembly Date', 'C-2 Date', 'Assembly Date DSA', 'First Hearing Date DSA'])]\n",
    "\n",
    "# Para `data_test`\n",
    "data_test = data_test.loc[:, ~data_test.columns.isin(['Birth Year', 'Assembly Date', 'C-2 Date', 'Assembly Date DSA', 'First Hearing Date DSA'])]\n",
    "\n",
    "\n",
    "for col in ['Birth Year',\"Assembly Date DSA\", \"First Hearing Date DSA\"]:\n",
    "    num_features.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "61c9780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the codes always seem to provide the same or more information than the descriptions (have more categories),\n",
    "#and the codes are consistent (always only having 1 description for code, while descriptions may have multiple codes)\n",
    "#we will drop the description columns.\n",
    "data.drop(['Industry Code Description','WCIO Cause of Injury Description','WCIO Nature of Injury Description','WCIO Part Of Body Description'], axis = 1,inplace = True)\n",
    "data_test.drop(['Industry Code Description','WCIO Cause of Injury Description','WCIO Nature of Injury Description','WCIO Part Of Body Description'], axis = 1,inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "id": "e0b67b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Zip Code for reason meantion in pre-processement\n",
    "data.drop(['Zip Code'], axis=1, inplace = True)\n",
    "data_test.drop(['Zip Code'], axis=1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "id": "9f0380ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"Claim Injury Type\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "130bc8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"Claim Injury Type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4cdcba-054b-43fc-8d41-919cf8f0b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aecf356",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rfe(RandomForestClassifier(),num_features,cat_features,scaling_outlier = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70233f94-61da-42da-9986-7d1d15398230",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Select Categorical features\n",
    "selected_cat_features = ['',''] # based on RFE\n",
    "\n",
    "# Select numerical features\n",
    "selected_num_features = ['Age at Injury', 'Average Weekly Wage','IME-4 Count','Number of Dependents',\n",
    "                         'Accident Year','Accident Month', 'Accident Day', 'Accident DayOfWeek',\n",
    "                         'C-2 Date DSA', 'C-3 Date DSA'] # based on RFE\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "f430371d-2ed7-4955-ba0d-7d061b421201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Apos tratar de missing values (train): Valores ausentes restantes:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Apos tratar de missing values (validation): Valores ausentes restantes:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Antes do modelo (train): Valores ausentes restantes:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Antes do modelo (validation): Valores ausentes restantes:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Apos tratar de missing values (train): Valores ausentes restantes:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Apos tratar de missing values (validation): Valores ausentes restantes:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Antes do modelo (train): Valores ausentes restantes:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Antes do modelo (validation): Valores ausentes restantes:\n",
      "Series([], dtype: int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Apos tratar de missing values (train): Valores ausentes restantes:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Apos tratar de missing values (validation): Valores ausentes restantes:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Antes do modelo (train): Valores ausentes restantes:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Antes do modelo (validation): Valores ausentes restantes:\n",
      "Series([], dtype: int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Apos tratar de missing values (train): Valores ausentes restantes:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Apos tratar de missing values (validation): Valores ausentes restantes:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Antes do modelo (train): Valores ausentes restantes:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Antes do modelo (validation): Valores ausentes restantes:\n",
      "Series([], dtype: int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Apos tratar de missing values (train): Valores ausentes restantes:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Apos tratar de missing values (validation): Valores ausentes restantes:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Antes do modelo (train): Valores ausentes restantes:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Antes do modelo (validation): Valores ausentes restantes:\n",
      "Series([], dtype: int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "rf_results = cv_scores(RandomForestClassifier(), X, y,num_features,cat_features,scaling_outlier = False)\n",
    "# look at results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "b33bf497",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[711], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrf_results\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rf_results' is not defined"
     ]
    }
   ],
   "source": [
    "rf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "id": "f4ffce66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train_precision</th>\n",
       "      <th>Test_precision</th>\n",
       "      <th>Train_recall</th>\n",
       "      <th>Test_recall</th>\n",
       "      <th>Train_f1_score</th>\n",
       "      <th>Test_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fold 1</th>\n",
       "      <td>0.795219</td>\n",
       "      <td>0.374950</td>\n",
       "      <td>0.602407</td>\n",
       "      <td>0.440832</td>\n",
       "      <td>0.638855</td>\n",
       "      <td>0.328713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 2</th>\n",
       "      <td>0.804514</td>\n",
       "      <td>0.284908</td>\n",
       "      <td>0.620631</td>\n",
       "      <td>0.264753</td>\n",
       "      <td>0.660613</td>\n",
       "      <td>0.262046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 3</th>\n",
       "      <td>0.800945</td>\n",
       "      <td>0.243091</td>\n",
       "      <td>0.625640</td>\n",
       "      <td>0.216288</td>\n",
       "      <td>0.664137</td>\n",
       "      <td>0.221379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 4</th>\n",
       "      <td>0.804848</td>\n",
       "      <td>0.253895</td>\n",
       "      <td>0.608262</td>\n",
       "      <td>0.217689</td>\n",
       "      <td>0.644059</td>\n",
       "      <td>0.225135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 5</th>\n",
       "      <td>0.799124</td>\n",
       "      <td>0.464812</td>\n",
       "      <td>0.581307</td>\n",
       "      <td>0.208376</td>\n",
       "      <td>0.615276</td>\n",
       "      <td>0.156410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>0.800930</td>\n",
       "      <td>0.324332</td>\n",
       "      <td>0.607649</td>\n",
       "      <td>0.269587</td>\n",
       "      <td>0.644588</td>\n",
       "      <td>0.238737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Train_precision  Test_precision  Train_recall  Test_recall  \\\n",
       "Fold 1          0.795219        0.374950      0.602407     0.440832   \n",
       "Fold 2          0.804514        0.284908      0.620631     0.264753   \n",
       "Fold 3          0.800945        0.243091      0.625640     0.216288   \n",
       "Fold 4          0.804848        0.253895      0.608262     0.217689   \n",
       "Fold 5          0.799124        0.464812      0.581307     0.208376   \n",
       "Average         0.800930        0.324332      0.607649     0.269587   \n",
       "\n",
       "         Train_f1_score  Test_f1_score  \n",
       "Fold 1         0.638855       0.328713  \n",
       "Fold 2         0.660613       0.262046  \n",
       "Fold 3         0.664137       0.221379  \n",
       "Fold 4         0.644059       0.225135  \n",
       "Fold 5         0.615276       0.156410  \n",
       "Average        0.644588       0.238737  "
      ]
     },
     "execution_count": 689,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "id": "923c21bd-9847-47e1-91ff-70b453b3b311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored on calling ctypes callback function <bound method DataIter._next_wrapper of <xgboost.data.SingleBatchInternalIter object at 0x150f93960>>:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/core.py\", line 582, in _next_wrapper\n",
      "    def _next_wrapper(self, this: None) -> int:  # pylint: disable=unused-argument\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[13:36:53] /Users/runner/work/xgboost/xgboost/src/data/iterative_dmatrix.cc:263: Check failed: rbegin == Info().num_row_ (1148042 vs. 574021) : \nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x0000000111d38454 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x0000000111ecb0c0 xgboost::data::IterativeDMatrix::InitFromCPU(xgboost::Context const*, xgboost::BatchParam const&, void*, float, std::__1::shared_ptr<xgboost::DMatrix>) + 12068\n  [bt] (2) 3   libxgboost.dylib                    0x0000000111ec7db0 xgboost::data::IterativeDMatrix::IterativeDMatrix(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 840\n  [bt] (3) 4   libxgboost.dylib                    0x0000000111e842d8 xgboost::DMatrix* xgboost::DMatrix::Create<void*, void*, void (void*), int (void*)>(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 140\n  [bt] (4) 5   libxgboost.dylib                    0x0000000111d41628 XGQuantileDMatrixCreateFromCallback + 496\n  [bt] (5) 6   libffi.dylib                        0x00000001a325b050 ffi_call_SYSV + 80\n  [bt] (6) 7   libffi.dylib                        0x00000001a3263b04 ffi_call_int + 1208\n  [bt] (7) 8   _ctypes.cpython-313-darwin.so       0x000000010577f890 _ctypes_callproc + 940\n  [bt] (8) 9   _ctypes.cpython-313-darwin.so       0x00000001057755f0 PyCFuncPtr_call + 256\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[715], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test prediction\u001b[39;00m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Choose best model from KFold above\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Train model on whole train and predict test data\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m submission \u001b[38;5;241m=\u001b[39m \u001b[43mtest_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXGBClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m submission\n",
      "Cell \u001b[0;32mIn[714], line 10\u001b[0m, in \u001b[0;36mtest_prediction\u001b[0;34m(model, X, y, num_features, cat_features, data_test, num_imputing_algorithm, cat_imputing_algorithm, scaling_outlier, scaler)\u001b[0m\n\u001b[1;32m      8\u001b[0m     impute_missing_values(X, column, num_imputing_algorithm)\n\u001b[1;32m      9\u001b[0m     impute_missing_values(data_test, column, num_imputing_algorithm)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mimpute_missing_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAlternative Dispute Resolution\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_imputing_algorithm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m impute_missing_values(data_test, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlternative Dispute Resolution\u001b[39m\u001b[38;5;124m\"\u001b[39m, cat_imputing_algorithm)\n\u001b[1;32m     13\u001b[0m check_missing_values(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApos tratar de missing values (train)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[694], line 34\u001b[0m, in \u001b[0;36mimpute_missing_values\u001b[0;34m(df, target_column, algorithm)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Training the model with the available data\u001b[39;00m\n\u001b[1;32m     33\u001b[0m model \u001b[38;5;241m=\u001b[39m algorithm\n\u001b[0;32m---> 34\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_available\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_available\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Prediting the missing values\u001b[39;00m\n\u001b[1;32m     37\u001b[0m predicted_values \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_missing)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/sklearn.py:1512\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_class\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_\n\u001b[1;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[0;32m-> 1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_evaluation_matrices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1515\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_qid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_dmatrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[1;32m   1532\u001b[0m     params,\n\u001b[1;32m   1533\u001b[0m     train_dmatrix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1542\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[1;32m   1543\u001b[0m )\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/sklearn.py:596\u001b[0m, in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_evaluation_matrices\u001b[39m(\n\u001b[1;32m    577\u001b[0m     missing: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    578\u001b[0m     X: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[1;32m    593\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, List[Tuple[Any, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;124;03m    way.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 596\u001b[0m     train_dmatrix \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dmatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m     n_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_set \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eval_set)\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_or_none\u001b[39m(meta: Optional[Sequence], name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/sklearn.py:1003\u001b[0m, in \u001b[0;36mXGBModel._create_dmatrix\u001b[0;34m(self, ref, **kwargs)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _can_use_qdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_method) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbooster \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1003\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQuantileDMatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnthread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_bin\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/core.py:1573\u001b[0m, in \u001b[0;36mQuantileDMatrix.__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, data_split_mode)\u001b[0m\n\u001b[1;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   1554\u001b[0m         info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1555\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m         )\n\u001b[1;32m   1567\u001b[0m     ):\n\u001b[1;32m   1568\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1569\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf data iterator is used as input, data like label should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1570\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified as batch argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1571\u001b[0m         )\n\u001b[0;32m-> 1573\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/core.py:1634\u001b[0m, in \u001b[0;36mQuantileDMatrix._init\u001b[0;34m(self, data, ref, enable_categorical, **meta)\u001b[0m\n\u001b[1;32m   1632\u001b[0m it\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;66;03m# delay check_call to throw intermediate exception first\u001b[39;00m\n\u001b[0;32m-> 1634\u001b[0m \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;241m=\u001b[39m handle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/core.py:284\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [13:36:53] /Users/runner/work/xgboost/xgboost/src/data/iterative_dmatrix.cc:263: Check failed: rbegin == Info().num_row_ (1148042 vs. 574021) : \nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x0000000111d38454 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x0000000111ecb0c0 xgboost::data::IterativeDMatrix::InitFromCPU(xgboost::Context const*, xgboost::BatchParam const&, void*, float, std::__1::shared_ptr<xgboost::DMatrix>) + 12068\n  [bt] (2) 3   libxgboost.dylib                    0x0000000111ec7db0 xgboost::data::IterativeDMatrix::IterativeDMatrix(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 840\n  [bt] (3) 4   libxgboost.dylib                    0x0000000111e842d8 xgboost::DMatrix* xgboost::DMatrix::Create<void*, void*, void (void*), int (void*)>(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 140\n  [bt] (4) 5   libxgboost.dylib                    0x0000000111d41628 XGQuantileDMatrixCreateFromCallback + 496\n  [bt] (5) 6   libffi.dylib                        0x00000001a325b050 ffi_call_SYSV + 80\n  [bt] (6) 7   libffi.dylib                        0x00000001a3263b04 ffi_call_int + 1208\n  [bt] (7) 8   _ctypes.cpython-313-darwin.so       0x000000010577f890 _ctypes_callproc + 940\n  [bt] (8) 9   _ctypes.cpython-313-darwin.so       0x00000001057755f0 PyCFuncPtr_call + 256\n\n"
     ]
    }
   ],
   "source": [
    "# Test prediction\n",
    "    # Choose best model from KFold above\n",
    "    # Train model on whole train and predict test data\n",
    "submission = test_prediction(XGBClassifier(),X,y,num_features,cat_features,data_test)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e26d1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.nunique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "039090e4-5406-4b1c-b3c9-5b663cc2872e",
   "metadata": {},
   "source": [
    "Template notebook for one type of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d29f30c5-6ec1-4339-a2af-a73bd36f91c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier,XGBRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from statistics import mean\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Other imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27ac0de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_4772/1043285265.py:2: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data= pd.read_csv(\"train_data_enriched.csv\", index_col=\"Claim Identifier\")\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data= pd.read_csv(\"train_data_enriched.csv\", index_col=\"Claim Identifier\")\n",
    "data_test = pd.read_csv(\"test_data_enriched.csv\",index_col=\"Claim Identifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60061974",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fa916c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label inconding our target variable \n",
    "data[\"Claim Injury Type\"] = le.fit_transform(data[\"Claim Injury Type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41080786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(df, target_column, algorithm):\n",
    "    # Separating the missing values from the non missing values\n",
    "    available_data = df[df[target_column].notna()]\n",
    "    missing_data = df[df[target_column].isna()]\n",
    "\n",
    "    # Diagnóstico inicial\n",
    "    \"\"\"print(f\"\\nImputando valores para coluna: {target_column}\")\n",
    "    print(f\"available data: {len(available_data)}\")\n",
    "    print(f\"missing data: {len(missing_data)}\")\"\"\"\n",
    "\n",
    "    # Verificar se há dados suficientes para imputação\n",
    "    if len(available_data) == 0 or len(missing_data) == 0:\n",
    "        # print(f\"nao deu input na coluna {target_column}\")\n",
    "        return df\n",
    "\n",
    "    # Separating the target column from the rest\n",
    "    X_available = available_data.drop(columns=[target_column])\n",
    "    y_available = available_data[target_column]\n",
    "\n",
    "    # Garantir consistência entre colunas\n",
    "    X_available = X_available.select_dtypes(include=[\"number\"])\n",
    "    X_missing = missing_data.drop(columns=[target_column]).select_dtypes(include=[\"number\"])\n",
    "    common_columns = X_available.columns.intersection(X_missing.columns)\n",
    "    X_available = X_available[common_columns]\n",
    "    X_missing = X_missing[common_columns]\n",
    "\n",
    "    # Verificar se ainda há colunas suficientes após alinhamento\n",
    "    if X_available.shape[1] == 0:\n",
    "        # print(f\"Sem colunas disponíveis para imputar na coluna {target_column}\")\n",
    "        return df\n",
    "\n",
    "    # Training the model with the available data\n",
    "    model = algorithm\n",
    "    model.fit(X_available, y_available)\n",
    "\n",
    "    # Prediting the missing values\n",
    "    predicted_values = model.predict(X_missing)\n",
    "\n",
    "    #print(len(predicted_values))\n",
    "\n",
    "    # Criar uma cópia explícita para evitar problemas com views\n",
    "    df = df.copy()\n",
    "    df.loc[df[target_column].isna(), target_column] = predicted_values\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85a5aa8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(\"Antes da imputação:\")\\nprint(data[\"C-3 Date\"].isna().sum())\\n\\n# Atribuir o DataFrame resultante à variável \\'data\\'\\ndata = impute_missing_values(data, \\'C-3 Date\\', XGBRegressor())\\n\\nprint(\"Depois da imputação:\")\\nprint(data[\"C-3 Date\"].isna().sum())'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"print(\"Antes da imputação:\")\n",
    "print(data[\"C-3 Date\"].isna().sum())\n",
    "\n",
    "# Atribuir o DataFrame resultante à variável 'data'\n",
    "data = impute_missing_values(data, 'C-3 Date', XGBRegressor())\n",
    "\n",
    "print(\"Depois da imputação:\")\n",
    "print(data[\"C-3 Date\"].isna().sum())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5e1910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(data, step_name):\n",
    "    print(f\"\\n{step_name}: Valores ausentes restantes:\")\n",
    "    print(data.isnull().sum()[data.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f20a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def handle_outliers(data, column):\n",
    "    \"\"\"\n",
    "    Handle outliers in a numerical column by replacing values outside the interquartile range (IQR) with the respective bounds.\n",
    "    Additionally, generates boxplots before and after the outlier treatment.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The dataset containing the column.\n",
    "        column (str): The column name to process.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with treated outliers.\n",
    "    \"\"\"\n",
    "    # Verifica se a coluna possui valores válidos\n",
    "    if data[column].notnull().sum() > 0:  # Apenas processa colunas com dados válidos\n",
    "        \"\"\"# Criar boxplot antes do tratamento\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(data[column])\n",
    "        plt.title(f\"Boxplot Antes do Tratamento de Outliers - {column}\")\n",
    "        plt.show()\"\"\"\n",
    "\n",
    "        # Calcular limites do IQR\n",
    "        Q1 = data[column].quantile(0.25)\n",
    "        Q3 = data[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Substituir valores fora dos limites pelos próprios limites\n",
    "        data[column] = np.where(data[column] < lower_bound, np.nan, data[column])\n",
    "        data[column] = np.where(data[column] > upper_bound, np.nan, data[column])\n",
    "\n",
    "        \"\"\"# Criar boxplot após o tratamento\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(data[column])\n",
    "        plt.title(f\"Boxplot Após o Tratamento de Outliers - {column}\")\n",
    "        plt.show()\"\"\"\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9e310a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_numerical(column, X_train, X_val, scaler):\n",
    "    # Certifique-se de que a coluna é numérica\n",
    "    if not pd.api.types.is_numeric_dtype(X_train[column]):\n",
    "        print(f\"A coluna '{column}' não é numérica e será ignorada.\")\n",
    "        return\n",
    "\n",
    "    # Escala os dados e substitui os valores na coluna\n",
    "    try:\n",
    "        X_train[column] = scaler.fit_transform(X_train[[column]])\n",
    "        X_val[column] = scaler.transform(X_val[[column]])\n",
    "    except ValueError as e:\n",
    "        print(f\"Erro ao escalonar a coluna '{column}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92397747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  claim_carrier_categories(X_train, X_val):\n",
    "    # Define a function to categorize each carrier based on its claim count\n",
    "    count = X_train['Carrier Name'].value_counts()\n",
    "    def categorize_claims(count):\n",
    "        if count >= 40000:\n",
    "            return 2\n",
    "        elif 4000 <= count < 40000:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # Apply the categorization to create a mapping dictionary\n",
    "    carrier_category_map = count.apply(categorize_claims)\n",
    "\n",
    "    # Map the `Carrier Name` to the new `Carrier Claim Category`\n",
    "    X_train['Carrier Claim Category'] = X_train['Carrier Name'].map(carrier_category_map)\n",
    "    X_val['Carrier Claim Category'] = X_val['Carrier Name'].map(carrier_category_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db2cfcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical encoder function\n",
    "def categorical_prop_encode(X_train, X_val, feature):\n",
    "    proportion = X_train[feature].value_counts(normalize = True)  # Get the porportion of each category\n",
    "    X_train[feature] = X_train[feature].map(proportion)  # Map the porportions in the column\n",
    "    X_val[feature] = X_val[feature].map(proportion) # Do the same for the valid dataset\n",
    "    X_val[feature] = X_val[feature].fillna(0)  # Handle categories in X_val not seen in X_train with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4efb3e3-69d9-4968-a39c-9f2cd20c4ba2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DecisionTreeRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mRfe\u001b[39m(algorithm,num_features,cat_features,num_imputing_algorithm\u001b[38;5;241m=\u001b[39m \u001b[43mDecisionTreeRegressor\u001b[49m() , cat_imputing_algorithm \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(), scaling_outlier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()):\n\u001b[1;32m      3\u001b[0m     X_train, X_val,y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(X,y,\n\u001b[1;32m      4\u001b[0m                                                 train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m, \n\u001b[1;32m      5\u001b[0m                                                 shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m      6\u001b[0m                                                 stratify \u001b[38;5;241m=\u001b[39m y)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#Filling num missing values\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DecisionTreeRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "def Rfe(algorithm,num_features,cat_features,num_imputing_algorithm= DecisionTreeRegressor() , cat_imputing_algorithm = DecisionTreeClassifier(), scaling_outlier = False, scaler = MinMaxScaler()):\n",
    "\n",
    "    X_train, X_val,y_train, y_val = train_test_split(X,y,\n",
    "                                                train_size = 0.8, \n",
    "                                                shuffle = True, \n",
    "                                                stratify = y)\n",
    "\n",
    "    #Filling num missing values\n",
    "    for column in num_features:\n",
    "        X_train = impute_missing_values(X_train, column, num_imputing_algorithm)\n",
    "        X_val = impute_missing_values(X_val, column, num_imputing_algorithm)\n",
    "\n",
    "    # Filling categorical missing values\n",
    "    X_train = impute_missing_values(X_train, \"Alternative Dispute Resolution\", cat_imputing_algorithm)\n",
    "    X_val = impute_missing_values(X_val, \"Alternative Dispute Resolution\", cat_imputing_algorithm)\n",
    "\n",
    "    # Removing inconsistencies on the train\n",
    "    inconsistent = X_train[(X_train['Age at Injury'] > 80) | (X_train[\"Age at Injury\"] < 16)].index\n",
    "    X_train = X_train.loc[~X_train.index.isin(inconsistent)]\n",
    "    y_train = y_train.loc[~y_train.index.isin(inconsistent)]\n",
    "\n",
    "    # Performing scaling and outlier treatment dependent on the boolean\n",
    "    if scaling_outlier:\n",
    "        for column in num_features:\n",
    "            handle_outliers(X_train, column)\n",
    "            check_missing_values(X_train, \"Apos handle_outliers (train)\")\n",
    "            check_missing_values(X_val, \"Apos handle_outliers (validation)\")\n",
    "            X_train = impute_missing_values(X_train, column, num_imputing_algorithm)\n",
    "\n",
    "        for column in num_features:\n",
    "            scale_numerical(column, X_train, X_val, scaler)\n",
    "\n",
    "        check_missing_values(X_train, \"Apos scalling (train)\")\n",
    "        check_missing_values(X_val, \"Apos scalling (validation)\")\n",
    "\n",
    "    # Creating an ordinal variable\n",
    "    claim_carrier_categories(X_train, X_val)\n",
    "\n",
    "    X_val = impute_missing_values(X_val, \"Carrier Claim Category\", cat_imputing_algorithm)\n",
    "\n",
    "    check_missing_values(X_train, \"Antes do modelo (train)\")\n",
    "    check_missing_values(X_val, \"Antes do modelo (validation)\")\n",
    "\n",
    "    # Categorical Prop Encoding\n",
    "    for cat_feature in cat_features:\n",
    "        categorical_prop_encode(X_train, X_val, cat_feature)\n",
    "    \n",
    "    #Generating the variables where we will store our results\n",
    "    nof_list = np.arange(1, len(X_train.columns) + 1)            \n",
    "    high_score = 0\n",
    "    opt_n_features = 0\n",
    "    train_score_list = []\n",
    "    val_score_list = []\n",
    "\n",
    "    #Variable where we will store the optimum amount of features\n",
    "    best_rfe = None\n",
    "\n",
    "    model = algorithm\n",
    "\n",
    "    for n in nof_list:\n",
    "        rfe = RFE(estimator=model, n_features_to_select=n)\n",
    "    \n",
    "    # Fitting the model to rfe\n",
    "        X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "        X_val_rfe = rfe.transform(X_val)\n",
    "    \n",
    "    # Training and predicting\n",
    "        model.fit(X_train_rfe, y_train)\n",
    "        pred_train = model.predict(X_train_rfe)\n",
    "        pred_val = model.predict(X_val_rfe)\n",
    "    \n",
    "    # Evaluating using the macro f1_score\n",
    "        train_score = f1_score(y_train, pred_train, average=\"macro\")\n",
    "        val_score = f1_score(y_val, pred_val, average=\"macro\")\n",
    "        train_score_list.append(train_score)\n",
    "        val_score_list.append(val_score)\n",
    "    \n",
    "    # Checking if this is the best combination of features so far\n",
    "        if val_score >= high_score:\n",
    "            high_score = val_score\n",
    "            opt_n_features = n\n",
    "            best_rfe = rfe  # Armazenar o RFE com o melhor número de features\n",
    "\n",
    "# Checking what amount of features and which features where the best for the model\n",
    "    selected_features = X_train.columns[best_rfe.support_].tolist()\n",
    "\n",
    "    print(\"Optimal number of features: %d\" % opt_n_features)\n",
    "    print(\"Score with %d features: %f\" % (opt_n_features, high_score))\n",
    "    print(\"Selected Features:\\n\", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc04b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_scores(model, X, y, num_features, cat_features, num_imputing_algorithm=RandomForestRegressor(), cat_imputing_algorithm=RandomForestClassifier(), scaling_outlier= False, scaler=MinMaxScaler()):\n",
    "    \"\"\"\n",
    "    Takes as argument the model used, the predictors and the target, the models used for imputing numerical and categorical \n",
    "    features, if any scaling and outlier removed should be performed, and what scaling method should be used.\n",
    "    Then it returns the results obtained from the stratified cross-validation for the given model.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # Generating the lists to store our results\n",
    "    precision_scores_train = []\n",
    "    precision_scores_val = []   \n",
    "    recall_scores_train = []  \n",
    "    recall_scores_val = []\n",
    "    f1_scores_train = []    \n",
    "    f1_scores_val = []\n",
    "    index = [f'Fold {i}' for i in range(1, 6)]\n",
    "    index.append(\"Average\")\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        # Dividing our data in validation and train\n",
    "        X_train, X_val = X.iloc[train_index].copy(), X.iloc[test_index].copy()\n",
    "        y_train, y_val = y.iloc[train_index].copy(), y.iloc[test_index].copy()\n",
    "\n",
    "        # Filling numerical missing values\n",
    "        for column in num_features:\n",
    "            X_train = impute_missing_values(X_train, column, num_imputing_algorithm)\n",
    "            X_val = impute_missing_values(X_val, column, num_imputing_algorithm)\n",
    "\n",
    "        # Filling categorical missing values\n",
    "        X_train = impute_missing_values(X_train, \"Alternative Dispute Resolution\", cat_imputing_algorithm)\n",
    "        X_val = impute_missing_values(X_val, \"Alternative Dispute Resolution\", cat_imputing_algorithm)\n",
    "\n",
    "        check_missing_values(X_train, \"Apos tratar de missing values (train)\")\n",
    "        check_missing_values(X_val, \"Apos tratar de missing values (validation)\")\n",
    "\n",
    "        # Removing inconsistencies on the train\n",
    "        inconsistent = X_train[(X_train['Age at Injury'] > 80) | (X_train[\"Age at Injury\"] < 16)].index\n",
    "        X_train = X_train.loc[~X_train.index.isin(inconsistent)]\n",
    "        y_train = y_train.loc[~y_train.index.isin(inconsistent)]\n",
    "\n",
    "        # Performing scaling and outlier treatment dependent on the boolean\n",
    "        if scaling_outlier:\n",
    "            for column in num_features:\n",
    "                handle_outliers(X_train, column)\n",
    "                check_missing_values(X_train, \"Apos handle_outliers (train)\")\n",
    "                check_missing_values(X_val, \"Apos handle_outliers (validation)\")\n",
    "                X_train = impute_missing_values(X_train, column, num_imputing_algorithm)\n",
    "\n",
    "            for column in num_features:\n",
    "                scale_numerical(column, X_train, X_val, scaler)\n",
    "            \n",
    "            print(X_train)\n",
    "\n",
    "            check_missing_values(X_train, \"Apos scalling (train)\")\n",
    "            check_missing_values(X_val, \"Apos scalling (validation)\")\n",
    "\n",
    "        # Creating an ordinal variable\n",
    "        claim_carrier_categories(X_train, X_val)\n",
    "\n",
    "        X_val = impute_missing_values(X_val, \"Carrier Claim Category\", cat_imputing_algorithm)\n",
    "\n",
    "        check_missing_values(X_train, \"Antes do modelo (train)\")\n",
    "        check_missing_values(X_val, \"Antes do modelo (validation)\")\n",
    "\n",
    "        # Categorical Prop Encoding\n",
    "        for cat_feature in cat_features:\n",
    "            categorical_prop_encode(X_train, X_val, cat_feature)\n",
    "\n",
    "        # Training the classification model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Making the predictions for the training and validation data\n",
    "        pred_train = model.predict(X_train)\n",
    "        pred_val = model.predict(X_val)\n",
    "\n",
    "        # Calculating and storing the scores\n",
    "        precision_scores_train.append(precision_score(y_train, pred_train, average='macro'))\n",
    "        precision_scores_val.append(precision_score(y_val, pred_val, average='macro'))\n",
    "        recall_scores_train.append(recall_score(y_train, pred_train, average='macro'))\n",
    "        recall_scores_val.append(recall_score(y_val, pred_val, average='macro'))\n",
    "        f1_scores_train.append(f1_score(y_train, pred_train, average='macro'))\n",
    "        f1_scores_val.append(f1_score(y_val, pred_val, average='macro'))\n",
    "\n",
    "    precision_scores_train.append(mean(precision_scores_train))\n",
    "    precision_scores_val.append(mean(precision_scores_val))\n",
    "    recall_scores_train.append(mean(recall_scores_train))\n",
    "    recall_scores_val.append(mean(recall_scores_val))\n",
    "    f1_scores_train.append(mean(f1_scores_train))\n",
    "    f1_scores_val.append(mean(f1_scores_val))\n",
    "\n",
    "    # Storing the results in a dataframe\n",
    "    model_results = pd.DataFrame(data={\n",
    "        'Train_precision': precision_scores_train,\n",
    "        'Test_precision': precision_scores_val,\n",
    "        'Train_recall': recall_scores_train,\n",
    "        'Test_recall': recall_scores_val,\n",
    "        'Train_f1_score': f1_scores_train,\n",
    "        'Test_f1_score': f1_scores_val,\n",
    "    }, index=index)\n",
    "\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bb306304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(model, X, y, num_features, cat_features, data_test, \n",
    "                    num_imputing_algorithm=XGBRegressor(), \n",
    "                    cat_imputing_algorithm=XGBClassifier(), scaling_outlier = False , \n",
    "                    scaler=MinMaxScaler()):\n",
    "\n",
    "    # Etapa 1: Imputação inicial de valores ausentes\n",
    "    for column in num_features:\n",
    "        impute_missing_values(X, column, num_imputing_algorithm)\n",
    "        impute_missing_values(data_test, column, num_imputing_algorithm)\n",
    "    impute_missing_values(X, \"Alternative Dispute Resolution\", cat_imputing_algorithm)\n",
    "    impute_missing_values(data_test, \"Alternative Dispute Resolution\", cat_imputing_algorithm)\n",
    "\n",
    "    check_missing_values(X, \"Apos tratar de missing values (train)\")\n",
    "    check_missing_values(data_test, \"Apos tratar de missing values (validation)\")\n",
    "\n",
    "    # Etapa 2: Remoção de inconsistências\n",
    "    inconsistent = X[(X['Age at Injury'] > 80) | (X[\"Age at Injury\"] < 16)].index\n",
    "    X.drop(inconsistent, inplace=True)\n",
    "    y.drop(inconsistent, inplace=True)\n",
    "\n",
    "    if scaling_outlier:\n",
    "        # Etapa 3: Tratamento de outliers\n",
    "        for column in num_features:\n",
    "            handle_outliers(X, column)\n",
    "            check_missing_values(X, \"Apos handle_outliers (train)\")\n",
    "            X = impute_missing_values(X, column, num_imputing_algorithm)\n",
    "\n",
    "        for column in num_features:\n",
    "            scale_numerical(column, X, data_test, scaler)\n",
    "        \n",
    "        check_missing_values(X, \"Apos scalling (train)\")\n",
    "        check_missing_values(data_test, \"Apos scalling (validation)\")\n",
    "    \n",
    "    # Creating an ordinal variable\n",
    "    claim_carrier_categories(X, data_test)\n",
    "    X = impute_missing_values(X, \"Carrier Claim Category\", cat_imputing_algorithm)\n",
    "\n",
    "    # Categorical Prop Encoding\n",
    "    for cat_feature in cat_features:\n",
    "        categorical_prop_encode(X, data_test, cat_feature)\n",
    "\n",
    "    check_missing_values(X, \"Antes do modelo (train)\")\n",
    "    check_missing_values(data_test, \"Antes do modelo (validation)\")\n",
    "\n",
    "    model.fit(X, y)\n",
    "    pred_test = model.predict(data_test)\n",
    "    pred_test = le.inverse_transform(pred_test)\n",
    "\n",
    "    submission_df = pd.DataFrame({\n",
    "        \"Claim Injury Type\": pred_test\n",
    "    }, index=data_test.index)\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fc3195-03a0-4757-bb36-5bffec585a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e76c9ed5",
   "metadata": {},
   "source": [
    "Temos de rever as num features e cat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "49cc0c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = ['Age at Injury', 'Average Weekly Wage', 'Birth Year', 'IME-4 Count', 'Number of Dependents',\n",
    "                \"Accident Year\",\"Accident Month\",\"Accident Day\",\"Accident DayOfWeek\",\"Assembly Date DSA\",\n",
    "                \"C-2 Date DSA\",\"C-3 Date DSA\",\"First Hearing Date DSA\" ,\"Accident Year\",\n",
    "    \"Accident Month\",\n",
    "    \"Accident Day\",\n",
    "    \"Accident DayOfWeek\",\"Accident Date\",\"C-3 Date\",\"First Hearing Date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5bb15763",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\n",
    "    \"Alternative Dispute Resolution\",\n",
    "    \"Carrier Name\",\n",
    "    \"Carrier Type\",\n",
    "    \"County of Injury\",\n",
    "    \"District Name\",\n",
    "    \"Gender\",\n",
    "    \"Industry Code\",\n",
    "    \"Medical Fee Region\",\n",
    "    \"WCIO Cause of Injury Code\",\n",
    "    \"WCIO Nature of Injury Code\",\n",
    "    \"WCIO Part Of Body Code\",\n",
    "    \"Age at Injury Category\",\n",
    "    \"Carrier Claim Category\",\n",
    "    \"Body Section\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea71eec",
   "metadata": {},
   "source": [
    "temos de dropar as colunas e por no pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f92e4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping redundant variables that carry almost the same information (are extremely correlated (|0.8|))\n",
    "# We believe it was better to keep Age at Injury than birth year since it should be more related to the injury claim type (it will be tested later)\n",
    "# The same logic was applied to dropping the other two dates and two DSA variables since we believe Accident date to be more important\n",
    "# Para `data`\n",
    "data = data.loc[:, ~data.columns.isin(['Birth Year', 'Assembly Date', 'C-2 Date', 'Assembly Date DSA', 'First Hearing Date DSA'])]\n",
    "\n",
    "# Para `data_test`\n",
    "data_test = data_test.loc[:, ~data_test.columns.isin(['Birth Year', 'Assembly Date', 'C-2 Date', 'Assembly Date DSA', 'First Hearing Date DSA'])]\n",
    "\n",
    "\n",
    "for col in ['Birth Year',\"Assembly Date DSA\", \"First Hearing Date DSA\"]:\n",
    "    num_features.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "61c9780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the codes always seem to provide the same or more information than the descriptions (have more categories),\n",
    "#and the codes are consistent (always only having 1 description for code, while descriptions may have multiple codes)\n",
    "#we will drop the description columns.\n",
    "data.drop(['Industry Code Description','WCIO Cause of Injury Description','WCIO Nature of Injury Description','WCIO Part Of Body Description'], axis = 1,inplace = True)\n",
    "data_test.drop(['Industry Code Description','WCIO Cause of Injury Description','WCIO Nature of Injury Description','WCIO Part Of Body Description'], axis = 1,inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e0b67b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Zip Code for reason meantion in pre-processement\n",
    "data.drop(['Zip Code'], axis=1, inplace = True)\n",
    "data_test.drop(['Zip Code'], axis=1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9f0380ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"Claim Injury Type\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "130bc8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"Claim Injury Type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4cdcba-054b-43fc-8d41-919cf8f0b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aecf356",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rfe(RandomForestClassifier(),num_features,cat_features,scaling_outlier = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70233f94-61da-42da-9986-7d1d15398230",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Select Categorical features\n",
    "selected_cat_features = ['',''] # based on RFE\n",
    "\n",
    "# Select numerical features\n",
    "selected_num_features = ['Age at Injury', 'Average Weekly Wage','IME-4 Count','Number of Dependents',\n",
    "                         'Accident Year','Accident Month', 'Accident Day', 'Accident DayOfWeek',\n",
    "                         'C-2 Date DSA', 'C-3 Date DSA'] # based on RFE\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f430371d-2ed7-4955-ba0d-7d061b421201",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = cv_scores(RandomForestClassifier(), X, y,num_features,cat_features,scaling_outlier = False)\n",
    "# look at results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33bf497",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ffce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "923c21bd-9847-47e1-91ff-70b453b3b311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Apos tratar de missing values (train): Valores ausentes restantes:\n",
      "Accident Date                       3689\n",
      "Age at Injury                       5464\n",
      "Alternative Dispute Resolution         5\n",
      "Average Weekly Wage               364101\n",
      "C-3 Date                          462692\n",
      "First Hearing Date                430804\n",
      "IME-4 Count                       441223\n",
      "Accident Year                       3689\n",
      "Accident Month                      3689\n",
      "Accident Day                        3689\n",
      "Accident DayOfWeek                  3689\n",
      "C-2 Date DSA                       17662\n",
      "C-3 Date DSA                      464099\n",
      "dtype: int64\n",
      "\n",
      "Apos tratar de missing values (validation): Valores ausentes restantes:\n",
      "Accident Date                       2444\n",
      "Age at Injury                       3248\n",
      "Alternative Dispute Resolution         1\n",
      "Average Weekly Wage               335753\n",
      "C-3 Date                          337724\n",
      "Carrier Name                        1063\n",
      "First Hearing Date                346311\n",
      "IME-4 Count                       352726\n",
      "Accident Year                       2444\n",
      "Accident Month                      2444\n",
      "Accident Day                        2444\n",
      "Accident DayOfWeek                  2444\n",
      "C-2 Date DSA                       10938\n",
      "C-3 Date DSA                      338497\n",
      "Carrier Claim Category              1063\n",
      "dtype: int64\n",
      "\n",
      "Antes do modelo (train): Valores ausentes restantes:\n",
      "Accident Date                       3689\n",
      "Age at Injury                       5464\n",
      "Alternative Dispute Resolution         5\n",
      "Average Weekly Wage               363354\n",
      "C-3 Date                          461801\n",
      "First Hearing Date                430013\n",
      "IME-4 Count                       440350\n",
      "Accident Year                       3689\n",
      "Accident Month                      3689\n",
      "Accident Day                        3689\n",
      "Accident DayOfWeek                  3689\n",
      "C-2 Date DSA                       17649\n",
      "C-3 Date DSA                      463208\n",
      "dtype: int64\n",
      "\n",
      "Antes do modelo (validation): Valores ausentes restantes:\n",
      "Accident Date            2444\n",
      "Age at Injury            3248\n",
      "Average Weekly Wage    335753\n",
      "C-3 Date               337724\n",
      "First Hearing Date     346311\n",
      "IME-4 Count            352726\n",
      "Accident Year            2444\n",
      "Accident Month           2444\n",
      "Accident Day             2444\n",
      "Accident DayOfWeek       2444\n",
      "C-2 Date DSA            10938\n",
      "C-3 Date DSA           338497\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claim Injury Type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claim Identifier</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6165911</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166141</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6165907</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166047</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166102</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553137</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553119</th>\n",
       "      <td>1. CANCELLED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553542</th>\n",
       "      <td>1. CANCELLED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553455</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553594</th>\n",
       "      <td>1. CANCELLED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>387975 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Claim Injury Type\n",
       "Claim Identifier                  \n",
       "6165911                2. NON-COMP\n",
       "6166141                2. NON-COMP\n",
       "6165907                2. NON-COMP\n",
       "6166047                2. NON-COMP\n",
       "6166102                2. NON-COMP\n",
       "...                            ...\n",
       "6553137                2. NON-COMP\n",
       "6553119               1. CANCELLED\n",
       "6553542               1. CANCELLED\n",
       "6553455                2. NON-COMP\n",
       "6553594               1. CANCELLED\n",
       "\n",
       "[387975 rows x 1 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test prediction\n",
    "    # Choose best model from KFold above\n",
    "    # Train model on whole train and predict test data\n",
    "submission = test_prediction(XGBClassifier(),X,y,num_features,cat_features,data_test)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fe86d483",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"Submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8e26d1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Claim Injury Type    7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.nunique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

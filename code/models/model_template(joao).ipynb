{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "039090e4-5406-4b1c-b3c9-5b663cc2872e",
   "metadata": {},
   "source": [
    "Template notebook for one type of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d29f30c5-6ec1-4339-a2af-a73bd36f91c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier,XGBRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from statistics import mean\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Other imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27ac0de3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_data_enriched.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data\u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_data_enriched.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClaim Identifier\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m data_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_data_enriched.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClaim Identifier\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_data_enriched.csv'"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data= pd.read_csv(\"train_data_enriched.csv\", index_col=\"Claim Identifier\")\n",
    "data_test = pd.read_csv(\"test_data_enriched.csv\",index_col=\"Claim Identifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60061974",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fa916c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label inconding our target variable \n",
    "data[\"Claim Injury Type\"] = le.fit_transform(data[\"Claim Injury Type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41080786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(data, target_column, algorithm):\n",
    "    # Separating the missing values from the non missing values\n",
    "    available_data = data[data[target_column].notna()]\n",
    "    missing_data = data[data[target_column].isna()]\n",
    "\n",
    "    # Diagnóstico inicial\n",
    "    print(f\"\\nImputando valores para coluna: {target_column}\")\n",
    "    print(f\"Linhas disponíveis para treino: {len(available_data)}\")\n",
    "    print(f\"Linhas com valores ausentes: {len(missing_data)}\")\n",
    "\n",
    "    # Verificar se há dados suficientes para imputação\n",
    "    if len(available_data) == 0 or len(missing_data) == 0:\n",
    "        print(f\"Sem dados suficientes para imputar valores na coluna {target_column}\")\n",
    "        return data\n",
    "\n",
    "    # Separating the target column from the rest\n",
    "    X_available = available_data.drop(columns=[target_column])\n",
    "    y_available = available_data[target_column]\n",
    "\n",
    "    # Garantir consistência entre colunas\n",
    "    X_available = X_available.select_dtypes(include=[\"number\"])\n",
    "    X_missing = missing_data.drop(columns=[target_column]).select_dtypes(include=[\"number\"])\n",
    "    common_columns = X_available.columns.intersection(X_missing.columns)\n",
    "    X_available = X_available[common_columns]\n",
    "    X_missing = X_missing[common_columns]\n",
    "\n",
    "    # Verificar se ainda há colunas suficientes após alinhamento\n",
    "    if X_available.shape[1] == 0:\n",
    "        print(f\"Sem colunas disponíveis para imputar na coluna {target_column}\")\n",
    "        return data\n",
    "\n",
    "    # Training the model with the available data\n",
    "    model = algorithm\n",
    "    model.fit(X_available, y_available)\n",
    "\n",
    "    # Prediting the missing values\n",
    "    predicted_values = model.predict(X_missing)\n",
    "\n",
    "    # Inputing the missing values with the predictions\n",
    "    data.loc[data[target_column].isna(), target_column] = predicted_values\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9513925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values_with_fallback(data, target_column, algorithm):\n",
    "    available_data = data[data[target_column].notna()]\n",
    "    missing_data = data[data[target_column].isna()]\n",
    "\n",
    "    if len(available_data) == 0 or len(missing_data) == 0:\n",
    "        print(f\"Sem dados suficientes para imputar a coluna '{target_column}', usando mediana.\")\n",
    "        data[target_column].fillna(data[target_column].median(), inplace=True)\n",
    "        return data\n",
    "\n",
    "    X_available = available_data.drop(columns=[target_column]).select_dtypes(include=[\"number\"])\n",
    "    y_available = available_data[target_column]\n",
    "    X_missing = missing_data.drop(columns=[target_column]).select_dtypes(include=[\"number\"])\n",
    "\n",
    "    common_columns = X_available.columns.intersection(X_missing.columns)\n",
    "    X_available = X_available[common_columns]\n",
    "    X_missing = X_missing[common_columns]\n",
    "\n",
    "    if X_available.shape[1] == 0 or X_missing.shape[1] == 0:\n",
    "        print(f\"Sem colunas suficientes para imputar '{target_column}', usando mediana.\")\n",
    "        data[target_column].fillna(data[target_column].median(), inplace=True)\n",
    "        return data\n",
    "\n",
    "    model = algorithm\n",
    "    model.fit(X_available, y_available)\n",
    "    predicted_values = model.predict(X_missing)\n",
    "    data.loc[data[target_column].isna(), target_column] = predicted_values\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5e1910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(data, step_name):\n",
    "    print(f\"\\n{step_name}: Valores ausentes restantes:\")\n",
    "    print(data.isnull().sum()[data.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f20a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers(data, column):\n",
    "    if data[column].notnull().sum() > 0:  # Apenas processa colunas com dados válidos\n",
    "        Q1 = data[column].quantile(0.25)\n",
    "        Q3 = data[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Substituir valores fora dos limites pelos próprios limites\n",
    "        data[column] = np.where(data[column] < lower_bound, lower_bound, data[column])\n",
    "        data[column] = np.where(data[column] > upper_bound, upper_bound, data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9e310a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_numerical(column, X_train, X_val, scaler):\n",
    "    # Certifique-se de que a coluna é numérica\n",
    "    if not pd.api.types.is_numeric_dtype(X_train[column]):\n",
    "        print(f\"A coluna '{column}' não é numérica e será ignorada.\")\n",
    "        return\n",
    "\n",
    "    # Escala os dados e substitui os valores na coluna\n",
    "    try:\n",
    "        X_train[column] = scaler.fit_transform(X_train[[column]].values.reshape(-1, 1))\n",
    "        X_val[column] = scaler.transform(X_val[[column]].values.reshape(-1, 1))\n",
    "    except ValueError as e:\n",
    "        print(f\"Erro ao escalonar a coluna '{column}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92397747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  claim_carrier_categories(X_train, X_val):\n",
    "    # Define a function to categorize each carrier based on its claim count\n",
    "    count = X_train['Carrier Name'].value_counts()\n",
    "    def categorize_claims(count):\n",
    "        if count >= 40000:\n",
    "            return 2\n",
    "        elif 4000 <= count < 40000:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # Apply the categorization to create a mapping dictionary\n",
    "    carrier_category_map = count.apply(categorize_claims)\n",
    "\n",
    "    # Map the `Carrier Name` to the new `Carrier Claim Category`\n",
    "    X_train['Carrier Claim Category'] = X_train['Carrier Name'].map(carrier_category_map)\n",
    "    X_val['Carrier Claim Category'] = X_val['Carrier Name'].map(carrier_category_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db2cfcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical encoder function\n",
    "def categorical_prop_encode(X_train, X_val, feature):\n",
    "    proportion = X_train[feature].value_counts(normalize = True)  # Get the porportion of each category\n",
    "    X_train[feature] = X_train[feature].map(proportion)  # Map the porportions in the column\n",
    "    X_val[feature] = X_val[feature].map(proportion) # Do the same for the valid dataset\n",
    "    X_val[feature] = X_val[feature].fillna(0)  # Handle categories in X_val not seen in X_train with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4efb3e3-69d9-4968-a39c-9f2cd20c4ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rfe(algorithm, num_inputing_algorithm= DecisionTreeRegressor() , cat_inputing_algorithm = DecisionTreeClassifier(), scaling_outlier = True):\n",
    "\n",
    "    X_train, X_val,y_train, y_val = train_test_split(X,y,\n",
    "                                                train_size = 0.8, \n",
    "                                                shuffle = True, \n",
    "                                                stratify = y)\n",
    "\n",
    "    #Filling num missing values\n",
    "    for column in num_features:\n",
    "        impute_missing_values(X_train, column, num_inputing_algorithm)\n",
    "        impute_missing_values(X_val, column, num_inputing_algorithm)\n",
    "\n",
    "    #Filling cat missing values\n",
    "    impute_missing_values(X_train, \"Alternative Dispute Resolution\", cat_inputing_algorithm)\n",
    "    impute_missing_values(X_val, \"Alternative Dispute Resolution\", cat_inputing_algorithm)\n",
    "\n",
    "    # Removing inconsistencies on the train\n",
    "    inconsistent = X_train[(X_train['Age at Injury'] > 80) | (X_train[\"Age at Injury\"] < 16)].index\n",
    "    X_train = X_train.loc[~X_train.index.isin(inconsistent)]\n",
    "    y_train = y_train.loc[~y_train.index.isin(inconsistent)]\n",
    "\n",
    "    #Performing scaling and outlier treatment dependent on the boolean\n",
    "    if scaling_outlier:\n",
    "        for column in num_features:\n",
    "            handle_outliers(X_train, column)\n",
    "            impute_missing_values(X_train, column, num_imputing_algorithm)\n",
    "                \n",
    "        for column in num_features:\n",
    "            scale_numerical(column,X_train, X_val, scaler)\n",
    "\n",
    "    # Creating an ordinal variable\n",
    "    claim_carrier_categories(X_train, X_val)\n",
    "\n",
    "    impute_missing_values(X_val, \"Carrier Claim Category\", cat_inputing_algorithm)\n",
    "\n",
    "    # Categorical Prop Encoding\n",
    "    for cat_feature in cat_features:\n",
    "        categorical_prop_encode(X_train, X_val, cat_feature)\n",
    "    \n",
    "    #Generating the variables where we will store our results\n",
    "    nof_list = np.arange(1, len(X_train.columns) + 1)            \n",
    "    high_score = 0\n",
    "    opt_n_features = 0\n",
    "    train_score_list = []\n",
    "    val_score_list = []\n",
    "\n",
    "    #Variable where we will store the optimum amount of features\n",
    "    best_rfe = None\n",
    "\n",
    "    model = algorithm\n",
    "\n",
    "    for n in nof_list:\n",
    "        rfe = RFE(estimator=model, n_features_to_select=n)\n",
    "    \n",
    "    # Fitting the model to rfe\n",
    "        X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "        X_val_rfe = rfe.transform(X_val)\n",
    "    \n",
    "    # Training and predicting\n",
    "        model.fit(X_train_rfe, y_train)\n",
    "        pred_train = model.predict(X_train_rfe)\n",
    "        pred_val = model.predict(X_val_rfe)\n",
    "    \n",
    "    # Evaluating using the macro f1_score\n",
    "        train_score = f1_score(y_train, pred_train, average=\"macro\")\n",
    "        val_score = f1_score(y_val, pred_val, average=\"macro\")\n",
    "        train_score_list.append(train_score)\n",
    "        val_score_list.append(val_score)\n",
    "    \n",
    "    # Checking if this is the best combination of features so far\n",
    "        if val_score >= high_score:\n",
    "            high_score = val_score\n",
    "            opt_n_features = n\n",
    "            best_rfe = rfe  # Armazenar o RFE com o melhor número de features\n",
    "\n",
    "# Checking what amount of features and which features where the best for the model\n",
    "    selected_features = X_train.columns[best_rfe.support_].tolist()\n",
    "\n",
    "    print(\"Optimal number of features: %d\" % opt_n_features)\n",
    "    print(\"Score with %d features: %f\" % (opt_n_features, high_score))\n",
    "    print(\"Selected Features:\\n\", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc04b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_scores(model, X, y, num_features, cat_features, num_imputing_algorithm= XGBRegressor() , cat_imputing_algorithm = XGBClassifier(), scaling_outlier = True, scaler = MinMaxScaler()):\n",
    "    ''' Takes as argument the model used, the predictors and the target, the models used for imputing numerical and categorical \n",
    "      features, if any scaling and outlier removed should be performed, and what scaling method should be used.\n",
    "     Then it returns the results obtained from the stratified cross validation for the given model'''\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    # Generating the lists to store our results\n",
    "    precision_scores_train = []\n",
    "    precision_scores_val = []   \n",
    "    recall_scores_train = []  \n",
    "    recall_scores_val = []\n",
    "    f1_scores_train = []    \n",
    "    f1_scores_val = []\n",
    "    index = [f'Fold {i}' for i in range(1,6)]\n",
    "    index.append(\"Average\")\n",
    "    \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        # Dividing our data in validation and train\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    " \n",
    "        #Filling num missing values\n",
    "        for column in num_features:\n",
    "            impute_missing_values(X_train, column, num_imputing_algorithm)\n",
    "            impute_missing_values(X_val, column, num_imputing_algorithm)\n",
    "\n",
    "        #Filling cat missing values\n",
    "        impute_missing_values(X_train, \"Alternative Dispute Resolution\", cat_imputing_algorithm)\n",
    "        impute_missing_values(X_val, \"Alternative Dispute Resolution\", cat_imputing_algorithm)\n",
    "\n",
    "\n",
    "        # Removing inconsistencies on the train\n",
    "        inconsistent = X_train[(X_train['Age at Injury'] > 80) | (X_train[\"Age at Injury\"] < 16)].index\n",
    "        X_train = X_train.loc[~X_train.index.isin(inconsistent)]\n",
    "        y_train = y_train.loc[~y_train.index.isin(inconsistent)]\n",
    "\n",
    "        #Performing scaling and outlier treatment dependent on the boolean\n",
    "        if scaling_outlier:\n",
    "            for column in num_features:\n",
    "                handle_outliers(X_train, column)\n",
    "                impute_missing_values(X_train, column, num_imputing_algorithm)\n",
    "                \n",
    "            for column in num_features:\n",
    "                scale_numerical(column,X_train, X_val, scaler)\n",
    "\n",
    "        # Creating an ordinal variable\n",
    "        claim_carrier_categories(X_train, X_val)\n",
    "\n",
    "        impute_missing_values(X_val, \"Carrier Claim Category\", cat_inputing_algorithm)\n",
    "\n",
    "\n",
    "        # Categorical Prop Encoding\n",
    "        for cat_feature in cat_features:\n",
    "            categorical_prop_encode(X_train, X_val, cat_feature)\n",
    "            \n",
    "        \n",
    "        # Training the classification model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        \n",
    "        # Making the predictions for the training and validation data\n",
    "        pred_train = model.predict(X_train)\n",
    "        pred_val = model.predict(X_val)\n",
    "    \n",
    "        \n",
    "        # Calculating and storing the scores\n",
    "        precision_scores_train.append(precision_score(y_train, pred_train, average='macro'))\n",
    "        precision_scores_val.append(precision_score(y_val, pred_val, average='macro'))\n",
    "        recall_scores_train.append(recall_score(y_train, pred_train, average='macro'))\n",
    "        recall_scores_val.append(recall_score(y_val, pred_val, average='macro'))\n",
    "        f1_scores_train.append(f1_score(y_train, pred_train, average='macro'))\n",
    "        f1_scores_val.append(f1_score(y_val, pred_val, average='macro'))\n",
    "\n",
    "    \n",
    "    precision_scores_train.append(mean(precision_scores_train))\n",
    "    precision_scores_val.append(mean(precision_scores_val))\n",
    "    recall_scores_train.append(mean(recall_scores_train))\n",
    "    recall_scores_val.append(mean(recall_scores_val))\n",
    "    f1_scores_train.append(mean(f1_scores_train))\n",
    "    f1_scores_val.append(mean(f1_scores_val))\n",
    "\n",
    "\n",
    "    # Storing the results in a dataframe\n",
    "    model_results = pd.DataFrame(data={\n",
    "        'Train_precision': precision_scores_train,\n",
    "        'Test_precision': precision_scores_val,\n",
    "        'Train_recall': recall_scores_train,\n",
    "        'Test_recall': recall_scores_val,\n",
    "        'Train_f1_score': f1_scores_train,\n",
    "        'Test_f1_score': f1_scores_val,\n",
    "    }, index=index)\n",
    "    \n",
    "    return model_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb306304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(model, X, y, num_features, cat_features, data_test, \n",
    "                    num_inputing_algorithm=XGBRegressor(), \n",
    "                    cat_inputing_algorithm=XGBClassifier(), \n",
    "                    scaler=MinMaxScaler()):\n",
    "\n",
    "    # Etapa 1: Imputação inicial de valores ausentes\n",
    "    for column in num_features:\n",
    "        impute_missing_values_with_fallback(X, column, num_inputing_algorithm)\n",
    "        impute_missing_values_with_fallback(data_test, column, num_inputing_algorithm)\n",
    "    impute_missing_values_with_fallback(X, \"Alternative Dispute Resolution\", cat_inputing_algorithm)\n",
    "    impute_missing_values_with_fallback(data_test, \"Alternative Dispute Resolution\", cat_inputing_algorithm)\n",
    "\n",
    "    # Etapa 2: Remoção de inconsistências\n",
    "    inconsistent = X[(X['Age at Injury'] > 80) | (X[\"Age at Injury\"] < 16)].index\n",
    "    X.drop(inconsistent, inplace=True)\n",
    "    y.drop(inconsistent, inplace=True)\n",
    "\n",
    "    # Etapa 3: Tratamento de outliers\n",
    "    for column in num_features:\n",
    "        handle_outliers(X, column)\n",
    "        handle_outliers(data_test, column)\n",
    "\n",
    "    # Etapa 4: Reimputação Após Outliers\n",
    "    for column in num_features:\n",
    "        impute_missing_values_with_fallback(X, column, num_inputing_algorithm)\n",
    "        impute_missing_values_with_fallback(data_test, column, num_inputing_algorithm)\n",
    "\n",
    "    # Etapa 5: Escalonamento\n",
    "    for column in num_features:\n",
    "        # Diagnóstico antes do escalonamento\n",
    "        if X[column].isnull().sum() > 0:\n",
    "            impute_missing_values_with_fallback(X, column, num_inputing_algorithm)\n",
    "        scale_numerical(column, X, data_test, scaler)\n",
    "\n",
    "    for cat_feature in cat_features:\n",
    "        categorical_prop_encode(X, data_test, cat_feature)\n",
    "\n",
    "    # Diagnóstico final\n",
    "    check_missing_values(X, \"Antes do Treinamento Final\")\n",
    "\n",
    "    model.fit(X, y)\n",
    "    pred_test = model.predict(data_test)\n",
    "    pred_test = le.inverse_transform(pred_test)\n",
    "\n",
    "    submission_df = pd.DataFrame({\n",
    "        \"Claim Injury Type\": pred_test\n",
    "    }, index=data_test.index)\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fc3195-03a0-4757-bb36-5bffec585a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e76c9ed5",
   "metadata": {},
   "source": [
    "Temos de rever as num features e cat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49cc0c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = ['Age at Injury', 'Average Weekly Wage', 'Birth Year', 'IME-4 Count', 'Number of Dependents',\n",
    "                \"Accident Year\",\"Accident Month\",\"Accident Day\",\"Accident DayOfWeek\",\"Assembly Date DSA\",\n",
    "                \"C-2 Date DSA\",\"C-3 Date DSA\",\"First Hearing Date DSA\" ,\"Accident Year\",\n",
    "    \"Accident Month\",\n",
    "    \"Accident Day\",\n",
    "    \"Accident DayOfWeek\",\"Accident Date\",\"C-3 Date\",\"First Hearing Date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bb15763",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\n",
    "    \"Alternative Dispute Resolution\",\n",
    "    \"Carrier Name\",\n",
    "    \"Carrier Type\",\n",
    "    \"County of Injury\",\n",
    "    \"District Name\",\n",
    "    \"Gender\",\n",
    "    \"Industry Code\",\n",
    "    \"Medical Fee Region\",\n",
    "    \"WCIO Cause of Injury Code\",\n",
    "    \"WCIO Nature of Injury Code\",\n",
    "    \"WCIO Part Of Body Code\",\n",
    "    \"Age at Injury Category\",\n",
    "    \"Carrier Claim Category\",\n",
    "    \"Body Section\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea71eec",
   "metadata": {},
   "source": [
    "temos de dropar as colunas e por no pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f92e4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping redundant variables that carry almost the same information (are extremely correlated (|0.8|))\n",
    "# We believe it was better to keep Age at Injury than birth year since it should be more related to the injury claim type (it will be tested later)\n",
    "# The same logic was applied to dropping the other two dates and two DSA variables since we believe Accident date to be more important\n",
    "# Para `data`\n",
    "data = data.loc[:, ~data.columns.isin(['Birth Year', 'Assembly Date', 'C-2 Date', 'Assembly Date DSA', 'First Hearing Date DSA'])]\n",
    "\n",
    "# Para `data_test`\n",
    "data_test = data_test.loc[:, ~data_test.columns.isin(['Birth Year', 'Assembly Date', 'C-2 Date', 'Assembly Date DSA', 'First Hearing Date DSA'])]\n",
    "\n",
    "\n",
    "for col in ['Birth Year',\"Assembly Date DSA\", \"First Hearing Date DSA\"]:\n",
    "    num_features.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61c9780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the codes always seem to provide the same or more information than the descriptions (have more categories),\n",
    "#and the codes are consistent (always only having 1 description for code, while descriptions may have multiple codes)\n",
    "#we will drop the description columns.\n",
    "data.drop(['Industry Code Description','WCIO Cause of Injury Description','WCIO Nature of Injury Description','WCIO Part Of Body Description'], axis = 1,inplace = True)\n",
    "data_test.drop(['Industry Code Description','WCIO Cause of Injury Description','WCIO Nature of Injury Description','WCIO Part Of Body Description'], axis = 1,inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0b67b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Zip Code for reason meantion in pre-processement\n",
    "data.drop(['Zip Code'], axis=1, inplace = True)\n",
    "data_test.drop(['Zip Code'], axis=1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f0380ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"Claim Injury Type\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "130bc8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"Claim Injury Type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a9c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4cdcba-054b-43fc-8d41-919cf8f0b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70233f94-61da-42da-9986-7d1d15398230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Categorical features\n",
    "selected_cat_features = ['',''] # based on RFE\n",
    "\n",
    "# Select numerical features\n",
    "selected_num_features = ['Age at Injury', 'Average Weekly Wage','IME-4 Count','Number of Dependents',\n",
    "                         'Accident Year','Accident Month', 'Accident Day', 'Accident DayOfWeek',\n",
    "                         'C-2 Date DSA', 'C-3 Date DSA'] # based on RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "f430371d-2ed7-4955-ba0d-7d061b421201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imputando valores para coluna: Age at Injury\n",
      "Linhas disponíveis para treino: 455208\n",
      "Linhas com valores ausentes: 4012\n",
      "\n",
      "Imputando valores para coluna: Age at Injury\n",
      "Linhas disponíveis para treino: 113354\n",
      "Linhas com valores ausentes: 1452\n",
      "\n",
      "Imputando valores para coluna: Average Weekly Wage\n",
      "Linhas disponíveis para treino: 168515\n",
      "Linhas com valores ausentes: 290705\n",
      "\n",
      "Imputando valores para coluna: Average Weekly Wage\n",
      "Linhas disponíveis para treino: 41410\n",
      "Linhas com valores ausentes: 73396\n",
      "\n",
      "Imputando valores para coluna: IME-4 Count\n",
      "Linhas disponíveis para treino: 107670\n",
      "Linhas com valores ausentes: 351550\n",
      "\n",
      "Imputando valores para coluna: IME-4 Count\n",
      "Linhas disponíveis para treino: 25133\n",
      "Linhas com valores ausentes: 89673\n",
      "\n",
      "Imputando valores para coluna: Number of Dependents\n",
      "Linhas disponíveis para treino: 459220\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Number of Dependents\n",
      "\n",
      "Imputando valores para coluna: Number of Dependents\n",
      "Linhas disponíveis para treino: 114806\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Number of Dependents\n",
      "\n",
      "Imputando valores para coluna: Accident Year\n",
      "Linhas disponíveis para treino: 456463\n",
      "Linhas com valores ausentes: 2757\n",
      "\n",
      "Imputando valores para coluna: Accident Year\n",
      "Linhas disponíveis para treino: 113874\n",
      "Linhas com valores ausentes: 932\n",
      "\n",
      "Imputando valores para coluna: Accident Month\n",
      "Linhas disponíveis para treino: 456463\n",
      "Linhas com valores ausentes: 2757\n",
      "\n",
      "Imputando valores para coluna: Accident Month\n",
      "Linhas disponíveis para treino: 113874\n",
      "Linhas com valores ausentes: 932\n",
      "\n",
      "Imputando valores para coluna: Accident Day\n",
      "Linhas disponíveis para treino: 456463\n",
      "Linhas com valores ausentes: 2757\n",
      "\n",
      "Imputando valores para coluna: Accident Day\n",
      "Linhas disponíveis para treino: 113874\n",
      "Linhas com valores ausentes: 932\n",
      "\n",
      "Imputando valores para coluna: Accident DayOfWeek\n",
      "Linhas disponíveis para treino: 456463\n",
      "Linhas com valores ausentes: 2757\n",
      "\n",
      "Imputando valores para coluna: Accident DayOfWeek\n",
      "Linhas disponíveis para treino: 113874\n",
      "Linhas com valores ausentes: 932\n",
      "\n",
      "Imputando valores para coluna: C-2 Date DSA\n",
      "Linhas disponíveis para treino: 445836\n",
      "Linhas com valores ausentes: 13384\n",
      "\n",
      "Imputando valores para coluna: C-2 Date DSA\n",
      "Linhas disponíveis para treino: 110528\n",
      "Linhas com valores ausentes: 4278\n",
      "\n",
      "Imputando valores para coluna: C-3 Date DSA\n",
      "Linhas disponíveis para treino: 86349\n",
      "Linhas com valores ausentes: 372871\n",
      "\n",
      "Imputando valores para coluna: C-3 Date DSA\n",
      "Linhas disponíveis para treino: 23578\n",
      "Linhas com valores ausentes: 91228\n",
      "\n",
      "Imputando valores para coluna: Accident Year\n",
      "Linhas disponíveis para treino: 459220\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident Year\n",
      "\n",
      "Imputando valores para coluna: Accident Year\n",
      "Linhas disponíveis para treino: 114806\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident Year\n",
      "\n",
      "Imputando valores para coluna: Accident Month\n",
      "Linhas disponíveis para treino: 459220\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident Month\n",
      "\n",
      "Imputando valores para coluna: Accident Month\n",
      "Linhas disponíveis para treino: 114806\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident Month\n",
      "\n",
      "Imputando valores para coluna: Accident Day\n",
      "Linhas disponíveis para treino: 459220\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident Day\n",
      "\n",
      "Imputando valores para coluna: Accident Day\n",
      "Linhas disponíveis para treino: 114806\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident Day\n",
      "\n",
      "Imputando valores para coluna: Accident DayOfWeek\n",
      "Linhas disponíveis para treino: 459220\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident DayOfWeek\n",
      "\n",
      "Imputando valores para coluna: Accident DayOfWeek\n",
      "Linhas disponíveis para treino: 114806\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident DayOfWeek\n",
      "\n",
      "Imputando valores para coluna: Accident Date\n",
      "Linhas disponíveis para treino: 456463\n",
      "Linhas com valores ausentes: 2757\n",
      "\n",
      "Imputando valores para coluna: Accident Date\n",
      "Linhas disponíveis para treino: 113874\n",
      "Linhas com valores ausentes: 932\n",
      "\n",
      "Imputando valores para coluna: C-3 Date\n",
      "Linhas disponíveis para treino: 87366\n",
      "Linhas com valores ausentes: 371854\n",
      "\n",
      "Imputando valores para coluna: C-3 Date\n",
      "Linhas disponíveis para treino: 23968\n",
      "Linhas com valores ausentes: 90838\n",
      "\n",
      "Imputando valores para coluna: First Hearing Date\n",
      "Linhas disponíveis para treino: 113699\n",
      "Linhas com valores ausentes: 345521\n",
      "\n",
      "Imputando valores para coluna: First Hearing Date\n",
      "Linhas disponíveis para treino: 29523\n",
      "Linhas com valores ausentes: 85283\n",
      "\n",
      "Imputando valores para coluna: Alternative Dispute Resolution\n",
      "Linhas disponíveis para treino: 459215\n",
      "Linhas com valores ausentes: 5\n",
      "\n",
      "Imputando valores para coluna: Alternative Dispute Resolution\n",
      "Linhas disponíveis para treino: 114806\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Alternative Dispute Resolution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_8339/1444211760.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val[feature] = X_val[feature].map(proportion) # Do the same for the valid dataset\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_8339/1444211760.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val[feature] = X_val[feature].fillna(0)  # Handle categories in X_val not seen in X_train with 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imputando valores para coluna: Age at Injury\n",
      "Linhas disponíveis para treino: 455036\n",
      "Linhas com valores ausentes: 4185\n",
      "\n",
      "Imputando valores para coluna: Age at Injury\n",
      "Linhas disponíveis para treino: 113526\n",
      "Linhas com valores ausentes: 1279\n",
      "\n",
      "Imputando valores para coluna: Average Weekly Wage\n",
      "Linhas disponíveis para treino: 168185\n",
      "Linhas com valores ausentes: 291036\n",
      "\n",
      "Imputando valores para coluna: Average Weekly Wage\n",
      "Linhas disponíveis para treino: 41740\n",
      "Linhas com valores ausentes: 73065\n",
      "\n",
      "Imputando valores para coluna: IME-4 Count\n",
      "Linhas disponíveis para treino: 106606\n",
      "Linhas com valores ausentes: 352615\n",
      "\n",
      "Imputando valores para coluna: IME-4 Count\n",
      "Linhas disponíveis para treino: 26197\n",
      "Linhas com valores ausentes: 88608\n",
      "\n",
      "Imputando valores para coluna: Number of Dependents\n",
      "Linhas disponíveis para treino: 459221\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Number of Dependents\n",
      "\n",
      "Imputando valores para coluna: Number of Dependents\n",
      "Linhas disponíveis para treino: 114805\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Number of Dependents\n",
      "\n",
      "Imputando valores para coluna: Accident Year\n",
      "Linhas disponíveis para treino: 456408\n",
      "Linhas com valores ausentes: 2813\n",
      "\n",
      "Imputando valores para coluna: Accident Year\n",
      "Linhas disponíveis para treino: 113929\n",
      "Linhas com valores ausentes: 876\n",
      "\n",
      "Imputando valores para coluna: Accident Month\n",
      "Linhas disponíveis para treino: 456408\n",
      "Linhas com valores ausentes: 2813\n",
      "\n",
      "Imputando valores para coluna: Accident Month\n",
      "Linhas disponíveis para treino: 113929\n",
      "Linhas com valores ausentes: 876\n",
      "\n",
      "Imputando valores para coluna: Accident Day\n",
      "Linhas disponíveis para treino: 456408\n",
      "Linhas com valores ausentes: 2813\n",
      "\n",
      "Imputando valores para coluna: Accident Day\n",
      "Linhas disponíveis para treino: 113929\n",
      "Linhas com valores ausentes: 876\n",
      "\n",
      "Imputando valores para coluna: Accident DayOfWeek\n",
      "Linhas disponíveis para treino: 456408\n",
      "Linhas com valores ausentes: 2813\n",
      "\n",
      "Imputando valores para coluna: Accident DayOfWeek\n",
      "Linhas disponíveis para treino: 113929\n",
      "Linhas com valores ausentes: 876\n",
      "\n",
      "Imputando valores para coluna: C-2 Date DSA\n",
      "Linhas disponíveis para treino: 445362\n",
      "Linhas com valores ausentes: 13859\n",
      "\n",
      "Imputando valores para coluna: C-2 Date DSA\n",
      "Linhas disponíveis para treino: 111002\n",
      "Linhas com valores ausentes: 3803\n",
      "\n",
      "Imputando valores para coluna: C-3 Date DSA\n",
      "Linhas disponíveis para treino: 87206\n",
      "Linhas com valores ausentes: 372015\n",
      "\n",
      "Imputando valores para coluna: C-3 Date DSA\n",
      "Linhas disponíveis para treino: 22721\n",
      "Linhas com valores ausentes: 92084\n",
      "\n",
      "Imputando valores para coluna: Accident Year\n",
      "Linhas disponíveis para treino: 459221\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident Year\n",
      "\n",
      "Imputando valores para coluna: Accident Year\n",
      "Linhas disponíveis para treino: 114805\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident Year\n",
      "\n",
      "Imputando valores para coluna: Accident Month\n",
      "Linhas disponíveis para treino: 459221\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident Month\n",
      "\n",
      "Imputando valores para coluna: Accident Month\n",
      "Linhas disponíveis para treino: 114805\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident Month\n",
      "\n",
      "Imputando valores para coluna: Accident Day\n",
      "Linhas disponíveis para treino: 459221\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident Day\n",
      "\n",
      "Imputando valores para coluna: Accident Day\n",
      "Linhas disponíveis para treino: 114805\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident Day\n",
      "\n",
      "Imputando valores para coluna: Accident DayOfWeek\n",
      "Linhas disponíveis para treino: 459221\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident DayOfWeek\n",
      "\n",
      "Imputando valores para coluna: Accident DayOfWeek\n",
      "Linhas disponíveis para treino: 114805\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident DayOfWeek\n",
      "\n",
      "Imputando valores para coluna: Accident Date\n",
      "Linhas disponíveis para treino: 456408\n",
      "Linhas com valores ausentes: 2813\n",
      "\n",
      "Imputando valores para coluna: Accident Date\n",
      "Linhas disponíveis para treino: 113929\n",
      "Linhas com valores ausentes: 876\n",
      "\n",
      "Imputando valores para coluna: C-3 Date\n",
      "Linhas disponíveis para treino: 88281\n",
      "Linhas com valores ausentes: 370940\n",
      "\n",
      "Imputando valores para coluna: C-3 Date\n",
      "Linhas disponíveis para treino: 23053\n",
      "Linhas com valores ausentes: 91752\n",
      "\n",
      "Imputando valores para coluna: First Hearing Date\n",
      "Linhas disponíveis para treino: 113536\n",
      "Linhas com valores ausentes: 345685\n",
      "\n",
      "Imputando valores para coluna: First Hearing Date\n",
      "Linhas disponíveis para treino: 29686\n",
      "Linhas com valores ausentes: 85119\n",
      "\n",
      "Imputando valores para coluna: Alternative Dispute Resolution\n",
      "Linhas disponíveis para treino: 459217\n",
      "Linhas com valores ausentes: 4\n",
      "\n",
      "Imputando valores para coluna: Alternative Dispute Resolution\n",
      "Linhas disponíveis para treino: 114804\n",
      "Linhas com valores ausentes: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_8339/1444211760.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val[feature] = X_val[feature].map(proportion) # Do the same for the valid dataset\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_8339/1444211760.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val[feature] = X_val[feature].fillna(0)  # Handle categories in X_val not seen in X_train with 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imputando valores para coluna: Age at Injury\n",
      "Linhas disponíveis para treino: 454785\n",
      "Linhas com valores ausentes: 4436\n",
      "\n",
      "Imputando valores para coluna: Age at Injury\n",
      "Linhas disponíveis para treino: 113777\n",
      "Linhas com valores ausentes: 1028\n",
      "\n",
      "Imputando valores para coluna: Average Weekly Wage\n",
      "Linhas disponíveis para treino: 167921\n",
      "Linhas com valores ausentes: 291300\n",
      "\n",
      "Imputando valores para coluna: Average Weekly Wage\n",
      "Linhas disponíveis para treino: 42004\n",
      "Linhas com valores ausentes: 72801\n",
      "\n",
      "Imputando valores para coluna: IME-4 Count\n",
      "Linhas disponíveis para treino: 105844\n",
      "Linhas com valores ausentes: 353377\n",
      "\n",
      "Imputando valores para coluna: IME-4 Count\n",
      "Linhas disponíveis para treino: 26959\n",
      "Linhas com valores ausentes: 87846\n",
      "\n",
      "Imputando valores para coluna: Number of Dependents\n",
      "Linhas disponíveis para treino: 459221\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Number of Dependents\n",
      "\n",
      "Imputando valores para coluna: Number of Dependents\n",
      "Linhas disponíveis para treino: 114805\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Number of Dependents\n",
      "\n",
      "Imputando valores para coluna: Accident Year\n",
      "Linhas disponíveis para treino: 456192\n",
      "Linhas com valores ausentes: 3029\n",
      "\n",
      "Imputando valores para coluna: Accident Year\n",
      "Linhas disponíveis para treino: 114145\n",
      "Linhas com valores ausentes: 660\n",
      "\n",
      "Imputando valores para coluna: Accident Month\n",
      "Linhas disponíveis para treino: 456192\n",
      "Linhas com valores ausentes: 3029\n",
      "\n",
      "Imputando valores para coluna: Accident Month\n",
      "Linhas disponíveis para treino: 114145\n",
      "Linhas com valores ausentes: 660\n",
      "\n",
      "Imputando valores para coluna: Accident Day\n",
      "Linhas disponíveis para treino: 456192\n",
      "Linhas com valores ausentes: 3029\n",
      "\n",
      "Imputando valores para coluna: Accident Day\n",
      "Linhas disponíveis para treino: 114145\n",
      "Linhas com valores ausentes: 660\n",
      "\n",
      "Imputando valores para coluna: Accident DayOfWeek\n",
      "Linhas disponíveis para treino: 456192\n",
      "Linhas com valores ausentes: 3029\n",
      "\n",
      "Imputando valores para coluna: Accident DayOfWeek\n",
      "Linhas disponíveis para treino: 114145\n",
      "Linhas com valores ausentes: 660\n",
      "\n",
      "Imputando valores para coluna: C-2 Date DSA\n",
      "Linhas disponíveis para treino: 444953\n",
      "Linhas com valores ausentes: 14268\n",
      "\n",
      "Imputando valores para coluna: C-2 Date DSA\n",
      "Linhas disponíveis para treino: 111411\n",
      "Linhas com valores ausentes: 3394\n",
      "\n",
      "Imputando valores para coluna: C-3 Date DSA\n",
      "Linhas disponíveis para treino: 88144\n",
      "Linhas com valores ausentes: 371077\n",
      "\n",
      "Imputando valores para coluna: C-3 Date DSA\n",
      "Linhas disponíveis para treino: 21783\n",
      "Linhas com valores ausentes: 93022\n",
      "\n",
      "Imputando valores para coluna: Accident Year\n",
      "Linhas disponíveis para treino: 459221\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident Year\n",
      "\n",
      "Imputando valores para coluna: Accident Year\n",
      "Linhas disponíveis para treino: 114805\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident Year\n",
      "\n",
      "Imputando valores para coluna: Accident Month\n",
      "Linhas disponíveis para treino: 459221\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident Month\n",
      "\n",
      "Imputando valores para coluna: Accident Month\n",
      "Linhas disponíveis para treino: 114805\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident Month\n",
      "\n",
      "Imputando valores para coluna: Accident Day\n",
      "Linhas disponíveis para treino: 459221\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident Day\n",
      "\n",
      "Imputando valores para coluna: Accident Day\n",
      "Linhas disponíveis para treino: 114805\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident Day\n",
      "\n",
      "Imputando valores para coluna: Accident DayOfWeek\n",
      "Linhas disponíveis para treino: 459221\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident DayOfWeek\n",
      "\n",
      "Imputando valores para coluna: Accident DayOfWeek\n",
      "Linhas disponíveis para treino: 114805\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident DayOfWeek\n",
      "\n",
      "Imputando valores para coluna: Accident Date\n",
      "Linhas disponíveis para treino: 456192\n",
      "Linhas com valores ausentes: 3029\n",
      "\n",
      "Imputando valores para coluna: Accident Date\n",
      "Linhas disponíveis para treino: 114145\n",
      "Linhas com valores ausentes: 660\n",
      "\n",
      "Imputando valores para coluna: C-3 Date\n",
      "Linhas disponíveis para treino: 89312\n",
      "Linhas com valores ausentes: 369909\n",
      "\n",
      "Imputando valores para coluna: C-3 Date\n",
      "Linhas disponíveis para treino: 22022\n",
      "Linhas com valores ausentes: 92783\n",
      "\n",
      "Imputando valores para coluna: First Hearing Date\n",
      "Linhas disponíveis para treino: 115023\n",
      "Linhas com valores ausentes: 344198\n",
      "\n",
      "Imputando valores para coluna: First Hearing Date\n",
      "Linhas disponíveis para treino: 28199\n",
      "Linhas com valores ausentes: 86606\n",
      "\n",
      "Imputando valores para coluna: Alternative Dispute Resolution\n",
      "Linhas disponíveis para treino: 459216\n",
      "Linhas com valores ausentes: 5\n",
      "\n",
      "Imputando valores para coluna: Alternative Dispute Resolution\n",
      "Linhas disponíveis para treino: 114805\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Alternative Dispute Resolution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_8339/1444211760.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val[feature] = X_val[feature].map(proportion) # Do the same for the valid dataset\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_8339/1444211760.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val[feature] = X_val[feature].fillna(0)  # Handle categories in X_val not seen in X_train with 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imputando valores para coluna: Age at Injury\n",
      "Linhas disponíveis para treino: 454689\n",
      "Linhas com valores ausentes: 4532\n",
      "\n",
      "Imputando valores para coluna: Age at Injury\n",
      "Linhas disponíveis para treino: 113873\n",
      "Linhas com valores ausentes: 932\n",
      "\n",
      "Imputando valores para coluna: Average Weekly Wage\n",
      "Linhas disponíveis para treino: 167705\n",
      "Linhas com valores ausentes: 291516\n",
      "\n",
      "Imputando valores para coluna: Average Weekly Wage\n",
      "Linhas disponíveis para treino: 42220\n",
      "Linhas com valores ausentes: 72585\n",
      "\n",
      "Imputando valores para coluna: IME-4 Count\n",
      "Linhas disponíveis para treino: 105589\n",
      "Linhas com valores ausentes: 353632\n",
      "\n",
      "Imputando valores para coluna: IME-4 Count\n",
      "Linhas disponíveis para treino: 27214\n",
      "Linhas com valores ausentes: 87591\n",
      "\n",
      "Imputando valores para coluna: Number of Dependents\n",
      "Linhas disponíveis para treino: 459221\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Number of Dependents\n",
      "\n",
      "Imputando valores para coluna: Number of Dependents\n",
      "Linhas disponíveis para treino: 114805\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Number of Dependents\n",
      "\n",
      "Imputando valores para coluna: Accident Year\n",
      "Linhas disponíveis para treino: 456174\n",
      "Linhas com valores ausentes: 3047\n",
      "\n",
      "Imputando valores para coluna: Accident Year\n",
      "Linhas disponíveis para treino: 114163\n",
      "Linhas com valores ausentes: 642\n",
      "\n",
      "Imputando valores para coluna: Accident Month\n",
      "Linhas disponíveis para treino: 456174\n",
      "Linhas com valores ausentes: 3047\n",
      "\n",
      "Imputando valores para coluna: Accident Month\n",
      "Linhas disponíveis para treino: 114163\n",
      "Linhas com valores ausentes: 642\n",
      "\n",
      "Imputando valores para coluna: Accident Day\n",
      "Linhas disponíveis para treino: 456174\n",
      "Linhas com valores ausentes: 3047\n",
      "\n",
      "Imputando valores para coluna: Accident Day\n",
      "Linhas disponíveis para treino: 114163\n",
      "Linhas com valores ausentes: 642\n",
      "\n",
      "Imputando valores para coluna: Accident DayOfWeek\n",
      "Linhas disponíveis para treino: 456174\n",
      "Linhas com valores ausentes: 3047\n",
      "\n",
      "Imputando valores para coluna: Accident DayOfWeek\n",
      "Linhas disponíveis para treino: 114163\n",
      "Linhas com valores ausentes: 642\n",
      "\n",
      "Imputando valores para coluna: C-2 Date DSA\n",
      "Linhas disponíveis para treino: 444859\n",
      "Linhas com valores ausentes: 14362\n",
      "\n",
      "Imputando valores para coluna: C-2 Date DSA\n",
      "Linhas disponíveis para treino: 111505\n",
      "Linhas com valores ausentes: 3300\n",
      "\n",
      "Imputando valores para coluna: C-3 Date DSA\n",
      "Linhas disponíveis para treino: 88600\n",
      "Linhas com valores ausentes: 370621\n",
      "\n",
      "Imputando valores para coluna: C-3 Date DSA\n",
      "Linhas disponíveis para treino: 21327\n",
      "Linhas com valores ausentes: 93478\n",
      "\n",
      "Imputando valores para coluna: Accident Year\n",
      "Linhas disponíveis para treino: 459221\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident Year\n",
      "\n",
      "Imputando valores para coluna: Accident Year\n",
      "Linhas disponíveis para treino: 114805\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident Year\n",
      "\n",
      "Imputando valores para coluna: Accident Month\n",
      "Linhas disponíveis para treino: 459221\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident Month\n",
      "\n",
      "Imputando valores para coluna: Accident Month\n",
      "Linhas disponíveis para treino: 114805\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident Month\n",
      "\n",
      "Imputando valores para coluna: Accident Day\n",
      "Linhas disponíveis para treino: 459221\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident Day\n",
      "\n",
      "Imputando valores para coluna: Accident Day\n",
      "Linhas disponíveis para treino: 114805\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident Day\n",
      "\n",
      "Imputando valores para coluna: Accident DayOfWeek\n",
      "Linhas disponíveis para treino: 459221\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident DayOfWeek\n",
      "\n",
      "Imputando valores para coluna: Accident DayOfWeek\n",
      "Linhas disponíveis para treino: 114805\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Accident DayOfWeek\n",
      "\n",
      "Imputando valores para coluna: Accident Date\n",
      "Linhas disponíveis para treino: 456174\n",
      "Linhas com valores ausentes: 3047\n",
      "\n",
      "Imputando valores para coluna: Accident Date\n",
      "Linhas disponíveis para treino: 114163\n",
      "Linhas com valores ausentes: 642\n",
      "\n",
      "Imputando valores para coluna: C-3 Date\n",
      "Linhas disponíveis para treino: 89763\n",
      "Linhas com valores ausentes: 369458\n",
      "\n",
      "Imputando valores para coluna: C-3 Date\n",
      "Linhas disponíveis para treino: 21571\n",
      "Linhas com valores ausentes: 93234\n",
      "\n",
      "Imputando valores para coluna: First Hearing Date\n",
      "Linhas disponíveis para treino: 115013\n",
      "Linhas com valores ausentes: 344208\n",
      "\n",
      "Imputando valores para coluna: First Hearing Date\n",
      "Linhas disponíveis para treino: 28209\n",
      "Linhas com valores ausentes: 86596\n",
      "\n",
      "Imputando valores para coluna: Alternative Dispute Resolution\n",
      "Linhas disponíveis para treino: 459216\n",
      "Linhas com valores ausentes: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_8339/1444211760.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val[feature] = X_val[feature].map(proportion) # Do the same for the valid dataset\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_8339/1444211760.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val[feature] = X_val[feature].fillna(0)  # Handle categories in X_val not seen in X_train with 0\n",
      "Exception ignored on calling ctypes callback function <bound method DataIter._next_wrapper of <xgboost.data.SingleBatchInternalIter object at 0x13ae74470>>:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/core.py\", line 582, in _next_wrapper\n",
      "    def _next_wrapper(self, this: None) -> int:  # pylint: disable=unused-argument\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imputando valores para coluna: Alternative Dispute Resolution\n",
      "Linhas disponíveis para treino: 114805\n",
      "Linhas com valores ausentes: 0\n",
      "Sem dados suficientes para imputar valores na coluna Alternative Dispute Resolution\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[16:30:48] /Users/runner/work/xgboost/xgboost/src/common/quantile.h:770: Check failed: count <= total_entries (458159 vs. 0) : \nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x000000012db58454 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x000000012dc646b0 std::__1::vector<unsigned int, std::__1::allocator<unsigned int>> xgboost::common::LoadBalance<xgboost::data::ColumnarAdapterBatch, xgboost::data::IsValidFunctor&>(xgboost::data::ColumnarAdapterBatch const&, unsigned long, unsigned int, unsigned long, xgboost::data::IsValidFunctor&) + 432\n  [bt] (2) 3   libxgboost.dylib                    0x000000012dc643e0 void xgboost::common::SketchContainerImpl<xgboost::common::WQuantileSketch<float, float>>::PushRowPageImpl<xgboost::data::ColumnarAdapterBatch, xgboost::data::IsValidFunctor>(xgboost::data::ColumnarAdapterBatch const&, unsigned long, xgboost::common::OptionalWeights, unsigned long, unsigned long, bool, xgboost::data::IsValidFunctor) + 152\n  [bt] (3) 4   libxgboost.dylib                    0x000000012dc64160 void xgboost::common::HostSketchContainer::PushAdapterBatch<xgboost::data::ColumnarAdapterBatch>(xgboost::data::ColumnarAdapterBatch const&, unsigned long, xgboost::MetaInfo const&, float) + 248\n  [bt] (4) 5   libxgboost.dylib                    0x000000012dcea30c xgboost::data::IterativeDMatrix::InitFromCPU(xgboost::Context const*, xgboost::BatchParam const&, void*, float, std::__1::shared_ptr<xgboost::DMatrix>) + 8560\n  [bt] (5) 6   libxgboost.dylib                    0x000000012dce7db0 xgboost::data::IterativeDMatrix::IterativeDMatrix(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 840\n  [bt] (6) 7   libxgboost.dylib                    0x000000012dca42d8 xgboost::DMatrix* xgboost::DMatrix::Create<void*, void*, void (void*), int (void*)>(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 140\n  [bt] (7) 8   libxgboost.dylib                    0x000000012db61628 XGQuantileDMatrixCreateFromCallback + 496\n  [bt] (8) 9   libffi.dylib                        0x000000019fb63050 ffi_call_SYSV + 80\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[554], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m xgb_results \u001b[38;5;241m=\u001b[39m \u001b[43mcv_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXGBClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscalling_outlier\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# look at results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[545], line 55\u001b[0m, in \u001b[0;36mcv_scores\u001b[0;34m(model, X, y, num_features, cat_features, num_imputing_algorithm, cat_imputing_algorithm, scalling_outlier, scaler)\u001b[0m\n\u001b[1;32m     51\u001b[0m     categorical_prop_encode(X_train, X_val, cat_feature)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Training the classification model\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Making the predictions for the training and validation data\u001b[39;00m\n\u001b[1;32m     59\u001b[0m pred_train \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/sklearn.py:1512\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_class\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_\n\u001b[1;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[0;32m-> 1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_evaluation_matrices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1515\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_qid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_dmatrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[1;32m   1532\u001b[0m     params,\n\u001b[1;32m   1533\u001b[0m     train_dmatrix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1542\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[1;32m   1543\u001b[0m )\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/sklearn.py:596\u001b[0m, in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_evaluation_matrices\u001b[39m(\n\u001b[1;32m    577\u001b[0m     missing: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    578\u001b[0m     X: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[1;32m    593\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, List[Tuple[Any, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;124;03m    way.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 596\u001b[0m     train_dmatrix \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dmatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m     n_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_set \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eval_set)\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_or_none\u001b[39m(meta: Optional[Sequence], name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/sklearn.py:1003\u001b[0m, in \u001b[0;36mXGBModel._create_dmatrix\u001b[0;34m(self, ref, **kwargs)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _can_use_qdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_method) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbooster \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1003\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQuantileDMatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnthread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_bin\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/core.py:1573\u001b[0m, in \u001b[0;36mQuantileDMatrix.__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, data_split_mode)\u001b[0m\n\u001b[1;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   1554\u001b[0m         info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1555\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m         )\n\u001b[1;32m   1567\u001b[0m     ):\n\u001b[1;32m   1568\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1569\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf data iterator is used as input, data like label should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1570\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified as batch argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1571\u001b[0m         )\n\u001b[0;32m-> 1573\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/core.py:1634\u001b[0m, in \u001b[0;36mQuantileDMatrix._init\u001b[0;34m(self, data, ref, enable_categorical, **meta)\u001b[0m\n\u001b[1;32m   1632\u001b[0m it\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;66;03m# delay check_call to throw intermediate exception first\u001b[39;00m\n\u001b[0;32m-> 1634\u001b[0m \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;241m=\u001b[39m handle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/xgboost/core.py:284\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [16:30:48] /Users/runner/work/xgboost/xgboost/src/common/quantile.h:770: Check failed: count <= total_entries (458159 vs. 0) : \nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x000000012db58454 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x000000012dc646b0 std::__1::vector<unsigned int, std::__1::allocator<unsigned int>> xgboost::common::LoadBalance<xgboost::data::ColumnarAdapterBatch, xgboost::data::IsValidFunctor&>(xgboost::data::ColumnarAdapterBatch const&, unsigned long, unsigned int, unsigned long, xgboost::data::IsValidFunctor&) + 432\n  [bt] (2) 3   libxgboost.dylib                    0x000000012dc643e0 void xgboost::common::SketchContainerImpl<xgboost::common::WQuantileSketch<float, float>>::PushRowPageImpl<xgboost::data::ColumnarAdapterBatch, xgboost::data::IsValidFunctor>(xgboost::data::ColumnarAdapterBatch const&, unsigned long, xgboost::common::OptionalWeights, unsigned long, unsigned long, bool, xgboost::data::IsValidFunctor) + 152\n  [bt] (3) 4   libxgboost.dylib                    0x000000012dc64160 void xgboost::common::HostSketchContainer::PushAdapterBatch<xgboost::data::ColumnarAdapterBatch>(xgboost::data::ColumnarAdapterBatch const&, unsigned long, xgboost::MetaInfo const&, float) + 248\n  [bt] (4) 5   libxgboost.dylib                    0x000000012dcea30c xgboost::data::IterativeDMatrix::InitFromCPU(xgboost::Context const*, xgboost::BatchParam const&, void*, float, std::__1::shared_ptr<xgboost::DMatrix>) + 8560\n  [bt] (5) 6   libxgboost.dylib                    0x000000012dce7db0 xgboost::data::IterativeDMatrix::IterativeDMatrix(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 840\n  [bt] (6) 7   libxgboost.dylib                    0x000000012dca42d8 xgboost::DMatrix* xgboost::DMatrix::Create<void*, void*, void (void*), int (void*)>(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int) + 140\n  [bt] (7) 8   libxgboost.dylib                    0x000000012db61628 XGQuantileDMatrixCreateFromCallback + 496\n  [bt] (8) 9   libffi.dylib                        0x000000019fb63050 ffi_call_SYSV + 80\n\n"
     ]
    }
   ],
   "source": [
    "xgb_results = cv_scores(XGBClassifier(), X, y,num_features,cat_features,scalling_outlier =False)\n",
    "# look at results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ffce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "923c21bd-9847-47e1-91ff-70b453b3b311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sem dados suficientes para imputar a coluna 'Number of Dependents', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Number of Dependents', usando mediana.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sem dados suficientes para imputar a coluna 'Accident Year', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Accident Year', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Accident Month', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Accident Month', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Accident Day', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Accident Day', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Accident DayOfWeek', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Accident DayOfWeek', usando mediana.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sem dados suficientes para imputar a coluna 'Age at Injury', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Age at Injury', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Average Weekly Wage', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Average Weekly Wage', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'IME-4 Count', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'IME-4 Count', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Number of Dependents', usando mediana.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sem dados suficientes para imputar a coluna 'Number of Dependents', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Accident Year', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Accident Year', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Accident Month', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Accident Month', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Accident Day', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Accident Day', usando mediana.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sem dados suficientes para imputar a coluna 'Accident DayOfWeek', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Accident DayOfWeek', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'C-2 Date DSA', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'C-2 Date DSA', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'C-3 Date DSA', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'C-3 Date DSA', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Accident Year', usando mediana.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sem dados suficientes para imputar a coluna 'Accident Year', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Accident Month', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Accident Month', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Accident Day', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Accident Day', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Accident DayOfWeek', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Accident DayOfWeek', usando mediana.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sem dados suficientes para imputar a coluna 'Accident Date', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'Accident Date', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'C-3 Date', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'C-3 Date', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'First Hearing Date', usando mediana.\n",
      "Sem dados suficientes para imputar a coluna 'First Hearing Date', usando mediana.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n",
      "/var/folders/fb/mypr4jd11nj8y_wzms74rj2w0000gn/T/ipykernel_11253/1695158437.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[target_column].fillna(data[target_column].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Antes do Treinamento Final: Valores ausentes restantes:\n",
      "Series([], dtype: int64)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claim Injury Type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claim Identifier</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6165911</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166141</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6165907</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166047</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166102</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553137</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553119</th>\n",
       "      <td>1. CANCELLED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553542</th>\n",
       "      <td>1. CANCELLED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553455</th>\n",
       "      <td>2. NON-COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6553594</th>\n",
       "      <td>1. CANCELLED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>387975 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Claim Injury Type\n",
       "Claim Identifier                  \n",
       "6165911                2. NON-COMP\n",
       "6166141                2. NON-COMP\n",
       "6165907                2. NON-COMP\n",
       "6166047                2. NON-COMP\n",
       "6166102                2. NON-COMP\n",
       "...                            ...\n",
       "6553137                2. NON-COMP\n",
       "6553119               1. CANCELLED\n",
       "6553542               1. CANCELLED\n",
       "6553455                2. NON-COMP\n",
       "6553594               1. CANCELLED\n",
       "\n",
       "[387975 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test prediction\n",
    "    # Choose best model from KFold above\n",
    "    # Train model on whole train and predict test data\n",
    "submission = test_prediction(XGBClassifier(),X,y,num_features,cat_features,data_test)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e26d1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Claim Injury Type    7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.nunique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
